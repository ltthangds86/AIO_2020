{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Flowers_InceptionV3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jznFLPIRsFZ_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5uuU44rvz3R",
        "outputId": "86cd59a9-3838-4c0e-bb60-8d686330e973"
      },
      "source": [
        "# !gdown --id 11Buzytn4vIh4x_Oqz8MY29JMMdIqSzj-\n",
        "!gdown https://drive.google.com/u/0/uc?id=11Buzytn4vIh4x_Oqz8MY29JMMdIqSzj-"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=11Buzytn4vIh4x_Oqz8MY29JMMdIqSzj-\n",
            "To: /content/flower_photos.zip\n",
            "230MB [00:05, 41.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8to4c4j2v6kd"
      },
      "source": [
        "!unzip -qq flower_photos.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uSZjGcNsKIn",
        "outputId": "7a26cc12-6f63-46ed-e8bd-812ebf68c5c0"
      },
      "source": [
        "PATH = 'flower_photos/'\n",
        "\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "IMG_SIZE = (160, 160)\n",
        "BUFFER_SIZE = BATCH_SIZE*5\n",
        "\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                                              PATH,\n",
        "                                              validation_split=0.2,\n",
        "                                              subset=\"training\",\n",
        "                                              seed=123,\n",
        "                                              image_size=IMG_SIZE,\n",
        "                                              batch_size=BATCH_SIZE)\n",
        "\n",
        "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "                                                  PATH,\n",
        "                                                  validation_split=0.2,\n",
        "                                                  subset=\"validation\",\n",
        "                                                  seed=123,\n",
        "                                                  image_size=IMG_SIZE,\n",
        "                                                  batch_size=BATCH_SIZE)\n",
        "\n",
        "train_dataset = train_dataset.cache().prefetch(buffer_size=BUFFER_SIZE)\n",
        "validation_dataset = validation_dataset.cache().prefetch(buffer_size=BUFFER_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3670 files belonging to 5 classes.\n",
            "Using 2936 files for training.\n",
            "Found 3670 files belonging to 5 classes.\n",
            "Using 734 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1FH3F6qDsFaF",
        "outputId": "4dc7aa8a-f621-43d7-b9c7-674432b0d480"
      },
      "source": [
        "### from scratch\n",
        "\n",
        "# Create the base model \n",
        "base_model = tf.keras.applications.InceptionV3(input_shape=(160,160,3),\n",
        "                                               include_top=False, weights=None)\n",
        "# process data\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
        "])\n",
        "\n",
        "# flattening\n",
        "flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "# final layer\n",
        "prediction_layer = tf.keras.layers.Dense(5)\n",
        "\n",
        "# construct a new network\n",
        "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = base_model(x)\n",
        "x = flatten(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# train\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer = tf.keras.optimizers.Adam(lr=0.00001), \n",
        "              metrics=['accuracy'])\n",
        "history_fine = model.fit(train_dataset, epochs=250,\n",
        "                         validation_data=validation_dataset)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(history_fine.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_fine.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(history_fine.history['loss'], label='Training Loss')\n",
        "plt.plot(history_fine.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "23/23 [==============================] - 31s 695ms/step - loss: 1.8793 - accuracy: 0.2113 - val_loss: 1.6064 - val_accuracy: 0.2398\n",
            "Epoch 2/250\n",
            "23/23 [==============================] - 10s 427ms/step - loss: 1.6126 - accuracy: 0.3339 - val_loss: 1.6078 - val_accuracy: 0.2398\n",
            "Epoch 3/250\n",
            "23/23 [==============================] - 10s 431ms/step - loss: 1.5260 - accuracy: 0.3618 - val_loss: 1.6112 - val_accuracy: 0.2398\n",
            "Epoch 4/250\n",
            "23/23 [==============================] - 10s 434ms/step - loss: 1.4278 - accuracy: 0.4207 - val_loss: 1.6156 - val_accuracy: 0.2398\n",
            "Epoch 5/250\n",
            "23/23 [==============================] - 10s 438ms/step - loss: 1.3772 - accuracy: 0.4530 - val_loss: 1.6202 - val_accuracy: 0.2398\n",
            "Epoch 6/250\n",
            "23/23 [==============================] - 10s 441ms/step - loss: 1.3489 - accuracy: 0.4654 - val_loss: 1.6231 - val_accuracy: 0.2425\n",
            "Epoch 7/250\n",
            "23/23 [==============================] - 10s 446ms/step - loss: 1.2881 - accuracy: 0.4847 - val_loss: 1.6403 - val_accuracy: 0.2480\n",
            "Epoch 8/250\n",
            "23/23 [==============================] - 10s 451ms/step - loss: 1.2874 - accuracy: 0.4923 - val_loss: 1.6739 - val_accuracy: 0.2425\n",
            "Epoch 9/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 1.2142 - accuracy: 0.5137 - val_loss: 1.7595 - val_accuracy: 0.2398\n",
            "Epoch 10/250\n",
            "23/23 [==============================] - 11s 460ms/step - loss: 1.2008 - accuracy: 0.5204 - val_loss: 1.8440 - val_accuracy: 0.2398\n",
            "Epoch 11/250\n",
            "23/23 [==============================] - 11s 462ms/step - loss: 1.1758 - accuracy: 0.5261 - val_loss: 1.9040 - val_accuracy: 0.2398\n",
            "Epoch 12/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 1.1597 - accuracy: 0.5488 - val_loss: 1.9277 - val_accuracy: 0.2452\n",
            "Epoch 13/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 1.1724 - accuracy: 0.5421 - val_loss: 1.9239 - val_accuracy: 0.2629\n",
            "Epoch 14/250\n",
            "23/23 [==============================] - 10s 453ms/step - loss: 1.1347 - accuracy: 0.5632 - val_loss: 1.8188 - val_accuracy: 0.3025\n",
            "Epoch 15/250\n",
            "23/23 [==============================] - 10s 454ms/step - loss: 1.1355 - accuracy: 0.5510 - val_loss: 1.6941 - val_accuracy: 0.3569\n",
            "Epoch 16/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 1.1264 - accuracy: 0.5598 - val_loss: 1.5510 - val_accuracy: 0.3747\n",
            "Epoch 17/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 1.0944 - accuracy: 0.5779 - val_loss: 1.4531 - val_accuracy: 0.4183\n",
            "Epoch 18/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 1.0939 - accuracy: 0.5671 - val_loss: 1.3544 - val_accuracy: 0.4537\n",
            "Epoch 19/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 1.1025 - accuracy: 0.5704 - val_loss: 1.2498 - val_accuracy: 0.5041\n",
            "Epoch 20/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 1.1077 - accuracy: 0.5580 - val_loss: 1.1906 - val_accuracy: 0.5218\n",
            "Epoch 21/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 1.0530 - accuracy: 0.5922 - val_loss: 1.1567 - val_accuracy: 0.5354\n",
            "Epoch 22/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 1.0640 - accuracy: 0.5831 - val_loss: 1.1189 - val_accuracy: 0.5409\n",
            "Epoch 23/250\n",
            "23/23 [==============================] - 10s 454ms/step - loss: 1.0585 - accuracy: 0.5924 - val_loss: 1.1065 - val_accuracy: 0.5586\n",
            "Epoch 24/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 1.0501 - accuracy: 0.5835 - val_loss: 1.0827 - val_accuracy: 0.5722\n",
            "Epoch 25/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 1.0093 - accuracy: 0.6097 - val_loss: 1.0836 - val_accuracy: 0.5845\n",
            "Epoch 26/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 1.0469 - accuracy: 0.5963 - val_loss: 1.0770 - val_accuracy: 0.5858\n",
            "Epoch 27/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.9981 - accuracy: 0.6102 - val_loss: 1.0888 - val_accuracy: 0.5845\n",
            "Epoch 28/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 1.0054 - accuracy: 0.6059 - val_loss: 1.1556 - val_accuracy: 0.5722\n",
            "Epoch 29/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.9838 - accuracy: 0.5983 - val_loss: 1.1385 - val_accuracy: 0.5926\n",
            "Epoch 30/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 1.0072 - accuracy: 0.6008 - val_loss: 1.1072 - val_accuracy: 0.5926\n",
            "Epoch 31/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 1.0054 - accuracy: 0.6024 - val_loss: 1.1133 - val_accuracy: 0.5926\n",
            "Epoch 32/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.9788 - accuracy: 0.6218 - val_loss: 1.1230 - val_accuracy: 0.5926\n",
            "Epoch 33/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.9902 - accuracy: 0.6282 - val_loss: 1.1480 - val_accuracy: 0.5804\n",
            "Epoch 34/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.9408 - accuracy: 0.6309 - val_loss: 1.1703 - val_accuracy: 0.5886\n",
            "Epoch 35/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.9547 - accuracy: 0.6099 - val_loss: 1.1877 - val_accuracy: 0.5981\n",
            "Epoch 36/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.9424 - accuracy: 0.6290 - val_loss: 1.1217 - val_accuracy: 0.6090\n",
            "Epoch 37/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.9349 - accuracy: 0.6433 - val_loss: 1.1129 - val_accuracy: 0.5981\n",
            "Epoch 38/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.9531 - accuracy: 0.6294 - val_loss: 1.1189 - val_accuracy: 0.5845\n",
            "Epoch 39/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.9646 - accuracy: 0.6269 - val_loss: 1.1037 - val_accuracy: 0.5790\n",
            "Epoch 40/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.9313 - accuracy: 0.6317 - val_loss: 1.1241 - val_accuracy: 0.5926\n",
            "Epoch 41/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.9488 - accuracy: 0.6269 - val_loss: 1.1160 - val_accuracy: 0.6049\n",
            "Epoch 42/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.9066 - accuracy: 0.6493 - val_loss: 1.1246 - val_accuracy: 0.5777\n",
            "Epoch 43/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8761 - accuracy: 0.6604 - val_loss: 1.1430 - val_accuracy: 0.5845\n",
            "Epoch 44/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8968 - accuracy: 0.6457 - val_loss: 1.1911 - val_accuracy: 0.5954\n",
            "Epoch 45/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.9040 - accuracy: 0.6568 - val_loss: 1.1639 - val_accuracy: 0.5913\n",
            "Epoch 46/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.9144 - accuracy: 0.6351 - val_loss: 1.3229 - val_accuracy: 0.5668\n",
            "Epoch 47/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.8992 - accuracy: 0.6542 - val_loss: 1.1255 - val_accuracy: 0.5899\n",
            "Epoch 48/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8567 - accuracy: 0.6662 - val_loss: 1.1827 - val_accuracy: 0.5763\n",
            "Epoch 49/250\n",
            "23/23 [==============================] - 10s 454ms/step - loss: 0.8688 - accuracy: 0.6543 - val_loss: 1.1622 - val_accuracy: 0.5804\n",
            "Epoch 50/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8819 - accuracy: 0.6427 - val_loss: 1.1421 - val_accuracy: 0.5940\n",
            "Epoch 51/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8696 - accuracy: 0.6490 - val_loss: 1.0946 - val_accuracy: 0.6117\n",
            "Epoch 52/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.8563 - accuracy: 0.6621 - val_loss: 1.1148 - val_accuracy: 0.5940\n",
            "Epoch 53/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8782 - accuracy: 0.6710 - val_loss: 1.2316 - val_accuracy: 0.6022\n",
            "Epoch 54/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8315 - accuracy: 0.6696 - val_loss: 1.1280 - val_accuracy: 0.5954\n",
            "Epoch 55/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8478 - accuracy: 0.6615 - val_loss: 1.1409 - val_accuracy: 0.5913\n",
            "Epoch 56/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8190 - accuracy: 0.6905 - val_loss: 1.2983 - val_accuracy: 0.5926\n",
            "Epoch 57/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8515 - accuracy: 0.6679 - val_loss: 1.1651 - val_accuracy: 0.5899\n",
            "Epoch 58/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8547 - accuracy: 0.6830 - val_loss: 1.1753 - val_accuracy: 0.5995\n",
            "Epoch 59/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8535 - accuracy: 0.6634 - val_loss: 1.2084 - val_accuracy: 0.5763\n",
            "Epoch 60/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8316 - accuracy: 0.6613 - val_loss: 1.1403 - val_accuracy: 0.5967\n",
            "Epoch 61/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8464 - accuracy: 0.6682 - val_loss: 1.1177 - val_accuracy: 0.6035\n",
            "Epoch 62/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8408 - accuracy: 0.6802 - val_loss: 1.1250 - val_accuracy: 0.5831\n",
            "Epoch 63/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.8184 - accuracy: 0.6874 - val_loss: 1.2513 - val_accuracy: 0.6185\n",
            "Epoch 64/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.8249 - accuracy: 0.6768 - val_loss: 1.1117 - val_accuracy: 0.6076\n",
            "Epoch 65/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.8145 - accuracy: 0.6758 - val_loss: 1.1558 - val_accuracy: 0.6035\n",
            "Epoch 66/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7945 - accuracy: 0.6933 - val_loss: 1.1291 - val_accuracy: 0.6035\n",
            "Epoch 67/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7825 - accuracy: 0.7030 - val_loss: 1.1201 - val_accuracy: 0.5899\n",
            "Epoch 68/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.8013 - accuracy: 0.7010 - val_loss: 1.0977 - val_accuracy: 0.5967\n",
            "Epoch 69/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.8072 - accuracy: 0.6824 - val_loss: 1.0907 - val_accuracy: 0.6008\n",
            "Epoch 70/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7784 - accuracy: 0.6910 - val_loss: 1.1235 - val_accuracy: 0.6022\n",
            "Epoch 71/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7743 - accuracy: 0.7088 - val_loss: 1.1138 - val_accuracy: 0.5913\n",
            "Epoch 72/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.8164 - accuracy: 0.6957 - val_loss: 1.1321 - val_accuracy: 0.5940\n",
            "Epoch 73/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.8089 - accuracy: 0.6920 - val_loss: 1.2581 - val_accuracy: 0.5831\n",
            "Epoch 74/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.8112 - accuracy: 0.6855 - val_loss: 1.1587 - val_accuracy: 0.5967\n",
            "Epoch 75/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7894 - accuracy: 0.6891 - val_loss: 1.1175 - val_accuracy: 0.5777\n",
            "Epoch 76/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7673 - accuracy: 0.7101 - val_loss: 1.0869 - val_accuracy: 0.5858\n",
            "Epoch 77/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.7881 - accuracy: 0.6970 - val_loss: 1.0698 - val_accuracy: 0.5995\n",
            "Epoch 78/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7650 - accuracy: 0.6970 - val_loss: 1.0931 - val_accuracy: 0.5995\n",
            "Epoch 79/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.7836 - accuracy: 0.6929 - val_loss: 1.1258 - val_accuracy: 0.5913\n",
            "Epoch 80/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7726 - accuracy: 0.6980 - val_loss: 1.1380 - val_accuracy: 0.5736\n",
            "Epoch 81/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7441 - accuracy: 0.7101 - val_loss: 1.1135 - val_accuracy: 0.5899\n",
            "Epoch 82/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.7719 - accuracy: 0.6899 - val_loss: 1.0931 - val_accuracy: 0.6035\n",
            "Epoch 83/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.7309 - accuracy: 0.7101 - val_loss: 1.1358 - val_accuracy: 0.5886\n",
            "Epoch 84/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7463 - accuracy: 0.7038 - val_loss: 1.1075 - val_accuracy: 0.6008\n",
            "Epoch 85/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.7586 - accuracy: 0.7082 - val_loss: 1.1069 - val_accuracy: 0.5981\n",
            "Epoch 86/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7664 - accuracy: 0.6945 - val_loss: 1.1584 - val_accuracy: 0.6172\n",
            "Epoch 87/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7343 - accuracy: 0.7215 - val_loss: 1.1124 - val_accuracy: 0.6308\n",
            "Epoch 88/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7360 - accuracy: 0.7147 - val_loss: 1.1399 - val_accuracy: 0.6076\n",
            "Epoch 89/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7232 - accuracy: 0.7089 - val_loss: 1.2712 - val_accuracy: 0.6104\n",
            "Epoch 90/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7024 - accuracy: 0.7302 - val_loss: 1.1215 - val_accuracy: 0.6076\n",
            "Epoch 91/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.7073 - accuracy: 0.7221 - val_loss: 1.1807 - val_accuracy: 0.6022\n",
            "Epoch 92/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7140 - accuracy: 0.7141 - val_loss: 1.1634 - val_accuracy: 0.6022\n",
            "Epoch 93/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.7137 - accuracy: 0.7244 - val_loss: 1.2724 - val_accuracy: 0.6063\n",
            "Epoch 94/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6966 - accuracy: 0.7309 - val_loss: 1.1613 - val_accuracy: 0.6076\n",
            "Epoch 95/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6980 - accuracy: 0.7348 - val_loss: 1.2051 - val_accuracy: 0.5967\n",
            "Epoch 96/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.6825 - accuracy: 0.7340 - val_loss: 1.1590 - val_accuracy: 0.6035\n",
            "Epoch 97/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6905 - accuracy: 0.7210 - val_loss: 1.1329 - val_accuracy: 0.6063\n",
            "Epoch 98/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.6837 - accuracy: 0.7356 - val_loss: 1.1985 - val_accuracy: 0.6035\n",
            "Epoch 99/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6684 - accuracy: 0.7358 - val_loss: 1.2251 - val_accuracy: 0.6049\n",
            "Epoch 100/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6902 - accuracy: 0.7279 - val_loss: 1.1451 - val_accuracy: 0.6308\n",
            "Epoch 101/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6807 - accuracy: 0.7367 - val_loss: 1.1140 - val_accuracy: 0.6158\n",
            "Epoch 102/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6812 - accuracy: 0.7329 - val_loss: 1.1175 - val_accuracy: 0.5995\n",
            "Epoch 103/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6846 - accuracy: 0.7392 - val_loss: 1.2223 - val_accuracy: 0.5940\n",
            "Epoch 104/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6764 - accuracy: 0.7359 - val_loss: 1.1492 - val_accuracy: 0.5995\n",
            "Epoch 105/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6530 - accuracy: 0.7477 - val_loss: 1.1943 - val_accuracy: 0.5954\n",
            "Epoch 106/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6773 - accuracy: 0.7418 - val_loss: 1.1597 - val_accuracy: 0.6185\n",
            "Epoch 107/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6824 - accuracy: 0.7317 - val_loss: 1.1589 - val_accuracy: 0.6117\n",
            "Epoch 108/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6569 - accuracy: 0.7429 - val_loss: 1.2710 - val_accuracy: 0.6049\n",
            "Epoch 109/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.6634 - accuracy: 0.7488 - val_loss: 1.1905 - val_accuracy: 0.6172\n",
            "Epoch 110/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6684 - accuracy: 0.7406 - val_loss: 1.3648 - val_accuracy: 0.6104\n",
            "Epoch 111/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6558 - accuracy: 0.7460 - val_loss: 1.1675 - val_accuracy: 0.6213\n",
            "Epoch 112/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6696 - accuracy: 0.7311 - val_loss: 1.2206 - val_accuracy: 0.6226\n",
            "Epoch 113/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6532 - accuracy: 0.7569 - val_loss: 1.3350 - val_accuracy: 0.6185\n",
            "Epoch 114/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6430 - accuracy: 0.7641 - val_loss: 1.1417 - val_accuracy: 0.6308\n",
            "Epoch 115/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.6332 - accuracy: 0.7501 - val_loss: 1.1970 - val_accuracy: 0.6076\n",
            "Epoch 116/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6326 - accuracy: 0.7564 - val_loss: 1.2367 - val_accuracy: 0.6267\n",
            "Epoch 117/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.6348 - accuracy: 0.7503 - val_loss: 1.1309 - val_accuracy: 0.6185\n",
            "Epoch 118/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.6451 - accuracy: 0.7519 - val_loss: 1.2443 - val_accuracy: 0.6076\n",
            "Epoch 119/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6401 - accuracy: 0.7478 - val_loss: 1.2010 - val_accuracy: 0.6144\n",
            "Epoch 120/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.6400 - accuracy: 0.7516 - val_loss: 1.3010 - val_accuracy: 0.6090\n",
            "Epoch 121/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.6157 - accuracy: 0.7709 - val_loss: 1.1813 - val_accuracy: 0.6199\n",
            "Epoch 122/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6199 - accuracy: 0.7691 - val_loss: 1.1658 - val_accuracy: 0.6063\n",
            "Epoch 123/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6050 - accuracy: 0.7793 - val_loss: 1.2046 - val_accuracy: 0.6226\n",
            "Epoch 124/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6076 - accuracy: 0.7727 - val_loss: 1.1284 - val_accuracy: 0.6172\n",
            "Epoch 125/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6189 - accuracy: 0.7595 - val_loss: 1.1513 - val_accuracy: 0.6158\n",
            "Epoch 126/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6181 - accuracy: 0.7575 - val_loss: 1.2053 - val_accuracy: 0.6199\n",
            "Epoch 127/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.6234 - accuracy: 0.7598 - val_loss: 1.1897 - val_accuracy: 0.6144\n",
            "Epoch 128/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6041 - accuracy: 0.7638 - val_loss: 1.1762 - val_accuracy: 0.6213\n",
            "Epoch 129/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5820 - accuracy: 0.7808 - val_loss: 1.1345 - val_accuracy: 0.6185\n",
            "Epoch 130/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5927 - accuracy: 0.7794 - val_loss: 1.1437 - val_accuracy: 0.6226\n",
            "Epoch 131/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6219 - accuracy: 0.7651 - val_loss: 1.1839 - val_accuracy: 0.6090\n",
            "Epoch 132/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6106 - accuracy: 0.7677 - val_loss: 1.2205 - val_accuracy: 0.6172\n",
            "Epoch 133/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5727 - accuracy: 0.7786 - val_loss: 1.2252 - val_accuracy: 0.6308\n",
            "Epoch 134/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5890 - accuracy: 0.7687 - val_loss: 1.1389 - val_accuracy: 0.6172\n",
            "Epoch 135/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.6039 - accuracy: 0.7723 - val_loss: 1.1749 - val_accuracy: 0.6158\n",
            "Epoch 136/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5618 - accuracy: 0.7831 - val_loss: 1.1744 - val_accuracy: 0.6335\n",
            "Epoch 137/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5578 - accuracy: 0.7856 - val_loss: 1.1599 - val_accuracy: 0.6199\n",
            "Epoch 138/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.5400 - accuracy: 0.7898 - val_loss: 1.1332 - val_accuracy: 0.6213\n",
            "Epoch 139/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.5706 - accuracy: 0.7843 - val_loss: 1.2075 - val_accuracy: 0.6213\n",
            "Epoch 140/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5608 - accuracy: 0.7811 - val_loss: 1.3340 - val_accuracy: 0.6308\n",
            "Epoch 141/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5951 - accuracy: 0.7621 - val_loss: 1.3566 - val_accuracy: 0.6090\n",
            "Epoch 142/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5638 - accuracy: 0.7799 - val_loss: 1.1469 - val_accuracy: 0.6294\n",
            "Epoch 143/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5773 - accuracy: 0.7789 - val_loss: 1.2004 - val_accuracy: 0.6226\n",
            "Epoch 144/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5409 - accuracy: 0.7882 - val_loss: 1.2011 - val_accuracy: 0.6240\n",
            "Epoch 145/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.5531 - accuracy: 0.7913 - val_loss: 1.2248 - val_accuracy: 0.6199\n",
            "Epoch 146/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5554 - accuracy: 0.7829 - val_loss: 1.2313 - val_accuracy: 0.6185\n",
            "Epoch 147/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5276 - accuracy: 0.8006 - val_loss: 1.1695 - val_accuracy: 0.6267\n",
            "Epoch 148/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5636 - accuracy: 0.7859 - val_loss: 1.2397 - val_accuracy: 0.6322\n",
            "Epoch 149/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5379 - accuracy: 0.7922 - val_loss: 1.1640 - val_accuracy: 0.6322\n",
            "Epoch 150/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5306 - accuracy: 0.7926 - val_loss: 1.1846 - val_accuracy: 0.6294\n",
            "Epoch 151/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5589 - accuracy: 0.7881 - val_loss: 1.1894 - val_accuracy: 0.6213\n",
            "Epoch 152/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5180 - accuracy: 0.8032 - val_loss: 1.2361 - val_accuracy: 0.6158\n",
            "Epoch 153/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5196 - accuracy: 0.8064 - val_loss: 1.1799 - val_accuracy: 0.6213\n",
            "Epoch 154/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5099 - accuracy: 0.8042 - val_loss: 1.3214 - val_accuracy: 0.6131\n",
            "Epoch 155/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5322 - accuracy: 0.7936 - val_loss: 1.2222 - val_accuracy: 0.6213\n",
            "Epoch 156/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5146 - accuracy: 0.7979 - val_loss: 1.1703 - val_accuracy: 0.6322\n",
            "Epoch 157/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5144 - accuracy: 0.8038 - val_loss: 1.2202 - val_accuracy: 0.6444\n",
            "Epoch 158/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5175 - accuracy: 0.8008 - val_loss: 1.2457 - val_accuracy: 0.6294\n",
            "Epoch 159/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5112 - accuracy: 0.8100 - val_loss: 1.1238 - val_accuracy: 0.6471\n",
            "Epoch 160/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.5109 - accuracy: 0.8066 - val_loss: 1.2024 - val_accuracy: 0.6335\n",
            "Epoch 161/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4971 - accuracy: 0.8070 - val_loss: 1.1416 - val_accuracy: 0.6403\n",
            "Epoch 162/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4936 - accuracy: 0.8120 - val_loss: 1.1238 - val_accuracy: 0.6485\n",
            "Epoch 163/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.4806 - accuracy: 0.8136 - val_loss: 1.1767 - val_accuracy: 0.6390\n",
            "Epoch 164/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4944 - accuracy: 0.8056 - val_loss: 1.1658 - val_accuracy: 0.6294\n",
            "Epoch 165/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4871 - accuracy: 0.8149 - val_loss: 1.2160 - val_accuracy: 0.6226\n",
            "Epoch 166/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4956 - accuracy: 0.8183 - val_loss: 1.2042 - val_accuracy: 0.6403\n",
            "Epoch 167/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.4914 - accuracy: 0.8088 - val_loss: 1.1429 - val_accuracy: 0.6417\n",
            "Epoch 168/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.5036 - accuracy: 0.8090 - val_loss: 1.2154 - val_accuracy: 0.6335\n",
            "Epoch 169/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.4648 - accuracy: 0.8219 - val_loss: 1.4632 - val_accuracy: 0.6253\n",
            "Epoch 170/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4833 - accuracy: 0.8156 - val_loss: 1.2166 - val_accuracy: 0.6403\n",
            "Epoch 171/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.4496 - accuracy: 0.8313 - val_loss: 1.3286 - val_accuracy: 0.6185\n",
            "Epoch 172/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4841 - accuracy: 0.8121 - val_loss: 1.1625 - val_accuracy: 0.6431\n",
            "Epoch 173/250\n",
            "23/23 [==============================] - 10s 454ms/step - loss: 0.4793 - accuracy: 0.8215 - val_loss: 1.2117 - val_accuracy: 0.6444\n",
            "Epoch 174/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4867 - accuracy: 0.8121 - val_loss: 1.2514 - val_accuracy: 0.6199\n",
            "Epoch 175/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4508 - accuracy: 0.8249 - val_loss: 1.3200 - val_accuracy: 0.6240\n",
            "Epoch 176/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4727 - accuracy: 0.8141 - val_loss: 1.2135 - val_accuracy: 0.6362\n",
            "Epoch 177/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4608 - accuracy: 0.8206 - val_loss: 1.2246 - val_accuracy: 0.6226\n",
            "Epoch 178/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4252 - accuracy: 0.8371 - val_loss: 1.1980 - val_accuracy: 0.6322\n",
            "Epoch 179/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4535 - accuracy: 0.8192 - val_loss: 1.3036 - val_accuracy: 0.6144\n",
            "Epoch 180/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4437 - accuracy: 0.8343 - val_loss: 1.1791 - val_accuracy: 0.6335\n",
            "Epoch 181/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4289 - accuracy: 0.8334 - val_loss: 1.2023 - val_accuracy: 0.6390\n",
            "Epoch 182/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.4302 - accuracy: 0.8288 - val_loss: 1.1948 - val_accuracy: 0.6417\n",
            "Epoch 183/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4561 - accuracy: 0.8242 - val_loss: 1.3639 - val_accuracy: 0.6267\n",
            "Epoch 184/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4650 - accuracy: 0.8256 - val_loss: 1.1959 - val_accuracy: 0.6444\n",
            "Epoch 185/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.4226 - accuracy: 0.8410 - val_loss: 1.2297 - val_accuracy: 0.6349\n",
            "Epoch 186/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.4318 - accuracy: 0.8418 - val_loss: 1.2279 - val_accuracy: 0.6294\n",
            "Epoch 187/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4126 - accuracy: 0.8291 - val_loss: 1.2867 - val_accuracy: 0.6417\n",
            "Epoch 188/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4194 - accuracy: 0.8361 - val_loss: 1.1950 - val_accuracy: 0.6417\n",
            "Epoch 189/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4370 - accuracy: 0.8302 - val_loss: 1.2064 - val_accuracy: 0.6553\n",
            "Epoch 190/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4256 - accuracy: 0.8311 - val_loss: 1.2841 - val_accuracy: 0.6403\n",
            "Epoch 191/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4175 - accuracy: 0.8447 - val_loss: 1.2346 - val_accuracy: 0.6485\n",
            "Epoch 192/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4309 - accuracy: 0.8415 - val_loss: 1.2305 - val_accuracy: 0.6608\n",
            "Epoch 193/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4138 - accuracy: 0.8473 - val_loss: 1.1949 - val_accuracy: 0.6499\n",
            "Epoch 194/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.3997 - accuracy: 0.8500 - val_loss: 1.3819 - val_accuracy: 0.6458\n",
            "Epoch 195/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4165 - accuracy: 0.8248 - val_loss: 1.2370 - val_accuracy: 0.6458\n",
            "Epoch 196/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4361 - accuracy: 0.8328 - val_loss: 1.2111 - val_accuracy: 0.6431\n",
            "Epoch 197/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3985 - accuracy: 0.8478 - val_loss: 1.3133 - val_accuracy: 0.6376\n",
            "Epoch 198/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3956 - accuracy: 0.8497 - val_loss: 1.3255 - val_accuracy: 0.6349\n",
            "Epoch 199/250\n",
            "23/23 [==============================] - 10s 458ms/step - loss: 0.3999 - accuracy: 0.8498 - val_loss: 1.2247 - val_accuracy: 0.6499\n",
            "Epoch 200/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4451 - accuracy: 0.8268 - val_loss: 1.2575 - val_accuracy: 0.6485\n",
            "Epoch 201/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3806 - accuracy: 0.8553 - val_loss: 1.2933 - val_accuracy: 0.6349\n",
            "Epoch 202/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.3806 - accuracy: 0.8583 - val_loss: 1.2746 - val_accuracy: 0.6376\n",
            "Epoch 203/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3862 - accuracy: 0.8484 - val_loss: 1.2390 - val_accuracy: 0.6390\n",
            "Epoch 204/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.4070 - accuracy: 0.8428 - val_loss: 1.3261 - val_accuracy: 0.6308\n",
            "Epoch 205/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3731 - accuracy: 0.8599 - val_loss: 1.2629 - val_accuracy: 0.6390\n",
            "Epoch 206/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3654 - accuracy: 0.8557 - val_loss: 1.2421 - val_accuracy: 0.6362\n",
            "Epoch 207/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.3677 - accuracy: 0.8656 - val_loss: 1.2196 - val_accuracy: 0.6349\n",
            "Epoch 208/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3726 - accuracy: 0.8641 - val_loss: 1.2110 - val_accuracy: 0.6526\n",
            "Epoch 209/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.3774 - accuracy: 0.8533 - val_loss: 1.2044 - val_accuracy: 0.6485\n",
            "Epoch 210/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3653 - accuracy: 0.8572 - val_loss: 1.2147 - val_accuracy: 0.6567\n",
            "Epoch 211/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3746 - accuracy: 0.8610 - val_loss: 1.2580 - val_accuracy: 0.6376\n",
            "Epoch 212/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3295 - accuracy: 0.8855 - val_loss: 1.2598 - val_accuracy: 0.6458\n",
            "Epoch 213/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3514 - accuracy: 0.8688 - val_loss: 1.2529 - val_accuracy: 0.6580\n",
            "Epoch 214/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3691 - accuracy: 0.8594 - val_loss: 1.2828 - val_accuracy: 0.6403\n",
            "Epoch 215/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.3587 - accuracy: 0.8675 - val_loss: 1.2833 - val_accuracy: 0.6458\n",
            "Epoch 216/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3817 - accuracy: 0.8557 - val_loss: 1.3668 - val_accuracy: 0.6403\n",
            "Epoch 217/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3668 - accuracy: 0.8655 - val_loss: 1.2684 - val_accuracy: 0.6553\n",
            "Epoch 218/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3677 - accuracy: 0.8596 - val_loss: 1.3697 - val_accuracy: 0.6471\n",
            "Epoch 219/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3493 - accuracy: 0.8672 - val_loss: 1.2474 - val_accuracy: 0.6444\n",
            "Epoch 220/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3089 - accuracy: 0.8832 - val_loss: 1.1849 - val_accuracy: 0.6512\n",
            "Epoch 221/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3608 - accuracy: 0.8711 - val_loss: 1.1587 - val_accuracy: 0.6580\n",
            "Epoch 222/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3491 - accuracy: 0.8666 - val_loss: 1.2966 - val_accuracy: 0.6431\n",
            "Epoch 223/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.3468 - accuracy: 0.8713 - val_loss: 1.2861 - val_accuracy: 0.6403\n",
            "Epoch 224/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.3170 - accuracy: 0.8766 - val_loss: 1.2431 - val_accuracy: 0.6376\n",
            "Epoch 225/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.3443 - accuracy: 0.8637 - val_loss: 1.2297 - val_accuracy: 0.6512\n",
            "Epoch 226/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3245 - accuracy: 0.8628 - val_loss: 1.2682 - val_accuracy: 0.6444\n",
            "Epoch 227/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.3264 - accuracy: 0.8744 - val_loss: 1.2875 - val_accuracy: 0.6240\n",
            "Epoch 228/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3315 - accuracy: 0.8772 - val_loss: 1.3381 - val_accuracy: 0.6567\n",
            "Epoch 229/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.3218 - accuracy: 0.8835 - val_loss: 1.3355 - val_accuracy: 0.6485\n",
            "Epoch 230/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.3403 - accuracy: 0.8704 - val_loss: 1.2362 - val_accuracy: 0.6444\n",
            "Epoch 231/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3335 - accuracy: 0.8738 - val_loss: 1.2506 - val_accuracy: 0.6458\n",
            "Epoch 232/250\n",
            "23/23 [==============================] - 10s 458ms/step - loss: 0.3200 - accuracy: 0.8722 - val_loss: 1.2516 - val_accuracy: 0.6403\n",
            "Epoch 233/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.3005 - accuracy: 0.8828 - val_loss: 1.2435 - val_accuracy: 0.6403\n",
            "Epoch 234/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3205 - accuracy: 0.8760 - val_loss: 1.2461 - val_accuracy: 0.6403\n",
            "Epoch 235/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3000 - accuracy: 0.8902 - val_loss: 1.2389 - val_accuracy: 0.6308\n",
            "Epoch 236/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.2933 - accuracy: 0.8851 - val_loss: 1.2477 - val_accuracy: 0.6485\n",
            "Epoch 237/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.2965 - accuracy: 0.8868 - val_loss: 1.3261 - val_accuracy: 0.6417\n",
            "Epoch 238/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3036 - accuracy: 0.8882 - val_loss: 1.2708 - val_accuracy: 0.6362\n",
            "Epoch 239/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3097 - accuracy: 0.8876 - val_loss: 1.2888 - val_accuracy: 0.6417\n",
            "Epoch 240/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3016 - accuracy: 0.8813 - val_loss: 1.3214 - val_accuracy: 0.6485\n",
            "Epoch 241/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.3021 - accuracy: 0.8838 - val_loss: 1.2912 - val_accuracy: 0.6471\n",
            "Epoch 242/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.2842 - accuracy: 0.8900 - val_loss: 1.2848 - val_accuracy: 0.6512\n",
            "Epoch 243/250\n",
            "23/23 [==============================] - 10s 457ms/step - loss: 0.3180 - accuracy: 0.8818 - val_loss: 1.3387 - val_accuracy: 0.6308\n",
            "Epoch 244/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.2944 - accuracy: 0.8923 - val_loss: 1.2983 - val_accuracy: 0.6403\n",
            "Epoch 245/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.2962 - accuracy: 0.8896 - val_loss: 1.2979 - val_accuracy: 0.6431\n",
            "Epoch 246/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.2896 - accuracy: 0.8923 - val_loss: 1.3091 - val_accuracy: 0.6390\n",
            "Epoch 247/250\n",
            "23/23 [==============================] - 10s 455ms/step - loss: 0.2913 - accuracy: 0.8893 - val_loss: 1.3106 - val_accuracy: 0.6512\n",
            "Epoch 248/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.2839 - accuracy: 0.8888 - val_loss: 1.3928 - val_accuracy: 0.6308\n",
            "Epoch 249/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.2666 - accuracy: 0.8959 - val_loss: 1.3680 - val_accuracy: 0.6390\n",
            "Epoch 250/250\n",
            "23/23 [==============================] - 10s 456ms/step - loss: 0.2851 - accuracy: 0.8955 - val_loss: 1.3293 - val_accuracy: 0.6512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OJPTr4fksFaH",
        "outputId": "6df8c306-aa38-4627-e017-b2cf29159c6a"
      },
      "source": [
        "# transfer learning\n",
        "\n",
        "# Create the base model \n",
        "base_model = tf.keras.applications.InceptionV3(input_shape=(160,160,3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.summary()\n",
        "\n",
        "# freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# process data\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
        "])\n",
        "\n",
        "# flattening\n",
        "flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "# final layer\n",
        "prediction_layer = tf.keras.layers.Dense(5)\n",
        "\n",
        "# construct a new network\n",
        "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = base_model(x)\n",
        "x = flatten(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "print(len(base_model.trainable_variables))\n",
        "print(len(model.trainable_variables))\n",
        "\n",
        "base_learning_rate = 0.0001\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate/10),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(train_dataset,\n",
        "                         epochs=250,\n",
        "                         validation_data=validation_dataset)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(history_fine.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_fine.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(history_fine.history['loss'], label='Training Loss')\n",
        "plt.plot(history_fine.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 79, 79, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 79, 79, 32)   96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 79, 79, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 77, 77, 32)   9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 77, 77, 32)   96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 77, 77, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 77, 77, 64)   18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 77, 77, 64)   192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 77, 77, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 38, 38, 64)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 38, 38, 80)   5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 38, 38, 80)   240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 38, 38, 80)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 36, 36, 192)  138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 36, 36, 192)  576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 36, 36, 192)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 17, 17, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 17, 17, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 17, 17, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 17, 17, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 17, 17, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 17, 17, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 17, 17, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 17, 17, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 17, 17, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 17, 17, 96)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 17, 17, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 17, 17, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 17, 17, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 17, 17, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 17, 17, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 17, 17, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 17, 17, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 17, 17, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 17, 17, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 17, 17, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 17, 17, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 17, 17, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 17, 17, 32)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 17, 17, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 17, 17, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 17, 17, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 17, 17, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 17, 17, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 17, 17, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 17, 17, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 17, 17, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 17, 17, 48)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 17, 17, 96)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 17, 17, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 17, 17, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 17, 17, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 17, 17, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 17, 17, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 17, 17, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 17, 17, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 17, 17, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 17, 17, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 17, 17, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 17, 17, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 17, 17, 96)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 17, 17, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 17, 17, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 17, 17, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 17, 17, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 17, 17, 64)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 17, 17, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 17, 17, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 17, 17, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 17, 17, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 17, 17, 48)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 17, 17, 96)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 17, 17, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 17, 17, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 17, 17, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 17, 17, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 17, 17, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 17, 17, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 17, 17, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 17, 17, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 17, 17, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 17, 17, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 17, 17, 64)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 17, 17, 96)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 17, 17, 64)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 17, 17, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 17, 17, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 17, 17, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 17, 17, 64)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 17, 17, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 17, 17, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 17, 17, 96)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 384)    995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 96)     82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 384)    1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 96)     288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 384)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 96)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 288)    0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 8, 8, 768)    0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 128)    384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 128)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 128)    114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 128)    384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 128)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 128)    114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 128)    384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 128)    384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 128)    0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 128)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 128)    114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 128)    114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 128)    384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 128)    384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 128)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 128)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 8, 8, 768)    0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 192)    147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 192)    172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 192)    172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 192)    147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 192)    576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 192)    576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 192)    576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 192)    576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 192)    0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 192)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 192)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 192)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 8, 8, 768)    0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 8, 8, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 8, 8, 160)    480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 8, 8, 160)    0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 8, 8, 160)    179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 8, 8, 160)    480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 8, 8, 160)    0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 8, 8, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 8, 8, 160)    179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 8, 8, 160)    480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 8, 8, 160)    480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 8, 160)    0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 8, 8, 160)    0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 8, 8, 160)    179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 8, 8, 160)    179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 8, 8, 160)    480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 8, 8, 160)    480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 8, 8, 160)    0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 8, 160)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 8, 8, 768)    0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 192)    147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 8, 8, 192)    215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 8, 8, 192)    215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 8, 8, 192)    147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 8, 8, 192)    576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 8, 8, 192)    576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 8, 8, 192)    576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 192)    576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 8, 8, 192)    0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 8, 8, 192)    0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 8, 8, 192)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 192)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 8, 8, 768)    0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 8, 8, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 160)    480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 160)    0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 8, 8, 160)    179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 8, 8, 160)    480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 8, 8, 160)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 8, 8, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 160)    179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 8, 8, 160)    480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 8, 8, 160)    480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 8, 8, 160)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 8, 8, 160)    0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 8, 8, 160)    179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 8, 8, 160)    179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 8, 8, 160)    480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 8, 8, 160)    480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 8, 8, 160)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 8, 8, 160)    0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 8, 8, 768)    0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 8, 8, 192)    147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 8, 8, 192)    215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 8, 8, 192)    215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 8, 8, 192)    147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 8, 8, 192)    576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 8, 8, 192)    576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 8, 8, 192)    576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 8, 8, 192)    576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 8, 8, 192)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 8, 8, 192)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 8, 8, 192)    0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 8, 8, 192)    0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 8, 8, 768)    0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 8, 8, 192)    576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 8, 8, 192)    0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 8, 8, 192)    258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 8, 8, 192)    576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 8, 8, 192)    0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 8, 8, 192)    258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 8, 8, 192)    576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 8, 8, 192)    576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 8, 8, 192)    0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 8, 8, 192)    0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 8, 8, 192)    258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 8, 8, 192)    258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 8, 8, 192)    576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 8, 8, 192)    576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 8, 8, 192)    0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 8, 8, 192)    0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 8, 8, 768)    0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 8, 8, 192)    258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 8, 8, 192)    258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 8, 8, 192)    147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 8, 8, 192)    576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 8, 8, 192)    576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 8, 8, 192)    576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 8, 8, 192)    576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 8, 8, 192)    0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 8, 8, 192)    0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 8, 8, 192)    0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 8, 8, 192)    0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 8, 8, 768)    0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 8, 8, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 8, 8, 192)    576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 8, 8, 192)    0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 8, 8, 192)    258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 8, 8, 192)    576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 8, 8, 192)    0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 8, 8, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 8, 8, 192)    258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 8, 8, 192)    576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 8, 8, 192)    576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 8, 8, 192)    0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 8, 8, 192)    0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 3, 3, 320)    552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 3, 3, 192)    331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 3, 3, 320)    960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 3, 3, 192)    576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 3, 3, 320)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 3, 3, 192)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 3, 3, 448)    1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 3, 3, 448)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 3, 3, 384)    1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 3, 3, 384)    1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 3, 3, 384)    1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 3, 3, 384)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 3, 3, 384)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 3, 3, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 3, 3, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 3, 3, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 3, 3, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 3, 3, 384)    1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 3, 3, 384)    1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 3, 3, 384)    1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 3, 3, 384)    1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 3, 3, 192)    245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 3, 3, 320)    960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 3, 3, 384)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 3, 3, 384)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 3, 3, 384)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 3, 3, 384)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 3, 3, 192)    576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 3, 3, 320)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3, 3, 768)    0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 3, 3, 192)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 3, 3, 448)    1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 3, 3, 448)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 3, 3, 384)    1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 3, 3, 384)    1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 3, 3, 384)    1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 3, 3, 384)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 3, 3, 384)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 3, 3, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 3, 3, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 3, 3, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 3, 3, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 3, 3, 384)    1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 3, 3, 384)    1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 3, 3, 384)    1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 3, 3, 384)    1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 3, 3, 192)    393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 3, 3, 320)    960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 3, 3, 384)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 3, 3, 384)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 3, 3, 384)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 3, 3, 384)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 3, 3, 192)    576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 3, 3, 320)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3, 3, 768)    0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 3, 3, 192)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 21,768,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n",
            "0\n",
            "2\n",
            "Epoch 1/250\n",
            "23/23 [==============================] - 40s 260ms/step - loss: 2.5073 - accuracy: 0.2478 - val_loss: 1.8247 - val_accuracy: 0.3610\n",
            "Epoch 2/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 2.1109 - accuracy: 0.3198 - val_loss: 1.5354 - val_accuracy: 0.4510\n",
            "Epoch 3/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 1.7445 - accuracy: 0.4252 - val_loss: 1.3347 - val_accuracy: 0.5109\n",
            "Epoch 4/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 1.6209 - accuracy: 0.4643 - val_loss: 1.2024 - val_accuracy: 0.5654\n",
            "Epoch 5/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 1.3994 - accuracy: 0.5374 - val_loss: 1.1070 - val_accuracy: 0.5995\n",
            "Epoch 6/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 1.3858 - accuracy: 0.5506 - val_loss: 1.0360 - val_accuracy: 0.6226\n",
            "Epoch 7/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 1.2745 - accuracy: 0.5797 - val_loss: 0.9865 - val_accuracy: 0.6444\n",
            "Epoch 8/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 1.2684 - accuracy: 0.5769 - val_loss: 0.9256 - val_accuracy: 0.6458\n",
            "Epoch 9/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 1.1339 - accuracy: 0.6159 - val_loss: 0.8881 - val_accuracy: 0.6703\n",
            "Epoch 10/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 1.1214 - accuracy: 0.6297 - val_loss: 0.8546 - val_accuracy: 0.6771\n",
            "Epoch 11/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 1.1468 - accuracy: 0.6169 - val_loss: 0.8254 - val_accuracy: 0.6962\n",
            "Epoch 12/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 1.0331 - accuracy: 0.6507 - val_loss: 0.8002 - val_accuracy: 0.7030\n",
            "Epoch 13/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 1.0696 - accuracy: 0.6511 - val_loss: 0.7751 - val_accuracy: 0.7153\n",
            "Epoch 14/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 1.0470 - accuracy: 0.6581 - val_loss: 0.7528 - val_accuracy: 0.7221\n",
            "Epoch 15/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.9465 - accuracy: 0.6688 - val_loss: 0.7433 - val_accuracy: 0.7248\n",
            "Epoch 16/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.9826 - accuracy: 0.6808 - val_loss: 0.7170 - val_accuracy: 0.7275\n",
            "Epoch 17/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.9231 - accuracy: 0.6818 - val_loss: 0.7061 - val_accuracy: 0.7343\n",
            "Epoch 18/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.9281 - accuracy: 0.6779 - val_loss: 0.6921 - val_accuracy: 0.7411\n",
            "Epoch 19/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.9105 - accuracy: 0.6957 - val_loss: 0.6779 - val_accuracy: 0.7439\n",
            "Epoch 20/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.9038 - accuracy: 0.6931 - val_loss: 0.6692 - val_accuracy: 0.7507\n",
            "Epoch 21/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.8368 - accuracy: 0.7239 - val_loss: 0.6569 - val_accuracy: 0.7548\n",
            "Epoch 22/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.8611 - accuracy: 0.7202 - val_loss: 0.6475 - val_accuracy: 0.7548\n",
            "Epoch 23/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.8540 - accuracy: 0.7255 - val_loss: 0.6381 - val_accuracy: 0.7602\n",
            "Epoch 24/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.8498 - accuracy: 0.7131 - val_loss: 0.6314 - val_accuracy: 0.7602\n",
            "Epoch 25/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.7972 - accuracy: 0.7254 - val_loss: 0.6211 - val_accuracy: 0.7643\n",
            "Epoch 26/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.7769 - accuracy: 0.7294 - val_loss: 0.6159 - val_accuracy: 0.7670\n",
            "Epoch 27/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.8218 - accuracy: 0.7338 - val_loss: 0.6090 - val_accuracy: 0.7738\n",
            "Epoch 28/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.7892 - accuracy: 0.7294 - val_loss: 0.6017 - val_accuracy: 0.7698\n",
            "Epoch 29/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.7913 - accuracy: 0.7400 - val_loss: 0.5957 - val_accuracy: 0.7738\n",
            "Epoch 30/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.7472 - accuracy: 0.7505 - val_loss: 0.5952 - val_accuracy: 0.7752\n",
            "Epoch 31/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.7571 - accuracy: 0.7430 - val_loss: 0.5880 - val_accuracy: 0.7752\n",
            "Epoch 32/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.7423 - accuracy: 0.7429 - val_loss: 0.5813 - val_accuracy: 0.7766\n",
            "Epoch 33/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.7408 - accuracy: 0.7397 - val_loss: 0.5818 - val_accuracy: 0.7820\n",
            "Epoch 34/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.7483 - accuracy: 0.7526 - val_loss: 0.5723 - val_accuracy: 0.7875\n",
            "Epoch 35/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.6823 - accuracy: 0.7608 - val_loss: 0.5714 - val_accuracy: 0.7847\n",
            "Epoch 36/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.7050 - accuracy: 0.7673 - val_loss: 0.5614 - val_accuracy: 0.7970\n",
            "Epoch 37/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.7240 - accuracy: 0.7632 - val_loss: 0.5620 - val_accuracy: 0.7888\n",
            "Epoch 38/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.7038 - accuracy: 0.7571 - val_loss: 0.5589 - val_accuracy: 0.7929\n",
            "Epoch 39/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.6795 - accuracy: 0.7640 - val_loss: 0.5519 - val_accuracy: 0.7984\n",
            "Epoch 40/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.7365 - accuracy: 0.7577 - val_loss: 0.5507 - val_accuracy: 0.7956\n",
            "Epoch 41/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.7257 - accuracy: 0.7531 - val_loss: 0.5508 - val_accuracy: 0.7956\n",
            "Epoch 42/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.6774 - accuracy: 0.7641 - val_loss: 0.5415 - val_accuracy: 0.8025\n",
            "Epoch 43/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.6987 - accuracy: 0.7701 - val_loss: 0.5438 - val_accuracy: 0.8025\n",
            "Epoch 44/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.7053 - accuracy: 0.7649 - val_loss: 0.5398 - val_accuracy: 0.8052\n",
            "Epoch 45/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.6779 - accuracy: 0.7843 - val_loss: 0.5332 - val_accuracy: 0.8052\n",
            "Epoch 46/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.6543 - accuracy: 0.7718 - val_loss: 0.5367 - val_accuracy: 0.8079\n",
            "Epoch 47/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.7353 - accuracy: 0.7643 - val_loss: 0.5322 - val_accuracy: 0.8052\n",
            "Epoch 48/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.6778 - accuracy: 0.7641 - val_loss: 0.5290 - val_accuracy: 0.8038\n",
            "Epoch 49/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.6491 - accuracy: 0.7780 - val_loss: 0.5270 - val_accuracy: 0.8093\n",
            "Epoch 50/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.6494 - accuracy: 0.7748 - val_loss: 0.5277 - val_accuracy: 0.8038\n",
            "Epoch 51/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.6581 - accuracy: 0.7752 - val_loss: 0.5216 - val_accuracy: 0.8106\n",
            "Epoch 52/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.6035 - accuracy: 0.7832 - val_loss: 0.5230 - val_accuracy: 0.8106\n",
            "Epoch 53/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.6373 - accuracy: 0.7806 - val_loss: 0.5177 - val_accuracy: 0.8065\n",
            "Epoch 54/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.6204 - accuracy: 0.7923 - val_loss: 0.5179 - val_accuracy: 0.8093\n",
            "Epoch 55/250\n",
            "23/23 [==============================] - 2s 102ms/step - loss: 0.6843 - accuracy: 0.7646 - val_loss: 0.5141 - val_accuracy: 0.8106\n",
            "Epoch 56/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.6258 - accuracy: 0.7860 - val_loss: 0.5165 - val_accuracy: 0.8120\n",
            "Epoch 57/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.6073 - accuracy: 0.7979 - val_loss: 0.5079 - val_accuracy: 0.8093\n",
            "Epoch 58/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.6213 - accuracy: 0.7805 - val_loss: 0.5079 - val_accuracy: 0.8174\n",
            "Epoch 59/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.5970 - accuracy: 0.7903 - val_loss: 0.5056 - val_accuracy: 0.8174\n",
            "Epoch 60/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.6184 - accuracy: 0.7812 - val_loss: 0.5034 - val_accuracy: 0.8188\n",
            "Epoch 61/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5903 - accuracy: 0.7993 - val_loss: 0.5022 - val_accuracy: 0.8134\n",
            "Epoch 62/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.5967 - accuracy: 0.7992 - val_loss: 0.5013 - val_accuracy: 0.8202\n",
            "Epoch 63/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.6116 - accuracy: 0.7909 - val_loss: 0.5019 - val_accuracy: 0.8188\n",
            "Epoch 64/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.5711 - accuracy: 0.7964 - val_loss: 0.4971 - val_accuracy: 0.8188\n",
            "Epoch 65/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.5458 - accuracy: 0.8110 - val_loss: 0.4964 - val_accuracy: 0.8229\n",
            "Epoch 66/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.5949 - accuracy: 0.8108 - val_loss: 0.4974 - val_accuracy: 0.8243\n",
            "Epoch 67/250\n",
            "23/23 [==============================] - 2s 102ms/step - loss: 0.5467 - accuracy: 0.8133 - val_loss: 0.4974 - val_accuracy: 0.8243\n",
            "Epoch 68/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.6154 - accuracy: 0.8062 - val_loss: 0.4910 - val_accuracy: 0.8174\n",
            "Epoch 69/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.6001 - accuracy: 0.8012 - val_loss: 0.4949 - val_accuracy: 0.8188\n",
            "Epoch 70/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.5845 - accuracy: 0.8069 - val_loss: 0.4946 - val_accuracy: 0.8215\n",
            "Epoch 71/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5829 - accuracy: 0.8048 - val_loss: 0.4912 - val_accuracy: 0.8215\n",
            "Epoch 72/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5702 - accuracy: 0.8084 - val_loss: 0.4917 - val_accuracy: 0.8229\n",
            "Epoch 73/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.5867 - accuracy: 0.7954 - val_loss: 0.4897 - val_accuracy: 0.8256\n",
            "Epoch 74/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.5399 - accuracy: 0.7997 - val_loss: 0.4919 - val_accuracy: 0.8283\n",
            "Epoch 75/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5708 - accuracy: 0.8059 - val_loss: 0.4900 - val_accuracy: 0.8256\n",
            "Epoch 76/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.5854 - accuracy: 0.7918 - val_loss: 0.4851 - val_accuracy: 0.8283\n",
            "Epoch 77/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5868 - accuracy: 0.7954 - val_loss: 0.4885 - val_accuracy: 0.8270\n",
            "Epoch 78/250\n",
            "23/23 [==============================] - 2s 102ms/step - loss: 0.5654 - accuracy: 0.8135 - val_loss: 0.4817 - val_accuracy: 0.8243\n",
            "Epoch 79/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5597 - accuracy: 0.7995 - val_loss: 0.4840 - val_accuracy: 0.8297\n",
            "Epoch 80/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5775 - accuracy: 0.7917 - val_loss: 0.4819 - val_accuracy: 0.8311\n",
            "Epoch 81/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.5557 - accuracy: 0.8134 - val_loss: 0.4838 - val_accuracy: 0.8243\n",
            "Epoch 82/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5427 - accuracy: 0.8186 - val_loss: 0.4813 - val_accuracy: 0.8283\n",
            "Epoch 83/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5584 - accuracy: 0.8089 - val_loss: 0.4784 - val_accuracy: 0.8256\n",
            "Epoch 84/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5370 - accuracy: 0.8171 - val_loss: 0.4797 - val_accuracy: 0.8283\n",
            "Epoch 85/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5465 - accuracy: 0.8062 - val_loss: 0.4768 - val_accuracy: 0.8256\n",
            "Epoch 86/250\n",
            "23/23 [==============================] - 2s 106ms/step - loss: 0.4926 - accuracy: 0.8274 - val_loss: 0.4770 - val_accuracy: 0.8256\n",
            "Epoch 87/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5329 - accuracy: 0.8221 - val_loss: 0.4767 - val_accuracy: 0.8270\n",
            "Epoch 88/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5476 - accuracy: 0.8091 - val_loss: 0.4741 - val_accuracy: 0.8270\n",
            "Epoch 89/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5322 - accuracy: 0.8251 - val_loss: 0.4739 - val_accuracy: 0.8297\n",
            "Epoch 90/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5176 - accuracy: 0.8326 - val_loss: 0.4747 - val_accuracy: 0.8324\n",
            "Epoch 91/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5194 - accuracy: 0.8257 - val_loss: 0.4745 - val_accuracy: 0.8283\n",
            "Epoch 92/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5231 - accuracy: 0.8219 - val_loss: 0.4698 - val_accuracy: 0.8270\n",
            "Epoch 93/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5304 - accuracy: 0.8251 - val_loss: 0.4720 - val_accuracy: 0.8297\n",
            "Epoch 94/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5438 - accuracy: 0.8121 - val_loss: 0.4696 - val_accuracy: 0.8311\n",
            "Epoch 95/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4918 - accuracy: 0.8289 - val_loss: 0.4706 - val_accuracy: 0.8297\n",
            "Epoch 96/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4919 - accuracy: 0.8213 - val_loss: 0.4683 - val_accuracy: 0.8297\n",
            "Epoch 97/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5087 - accuracy: 0.8344 - val_loss: 0.4692 - val_accuracy: 0.8324\n",
            "Epoch 98/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4948 - accuracy: 0.8306 - val_loss: 0.4704 - val_accuracy: 0.8324\n",
            "Epoch 99/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5047 - accuracy: 0.8217 - val_loss: 0.4664 - val_accuracy: 0.8338\n",
            "Epoch 100/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4869 - accuracy: 0.8263 - val_loss: 0.4666 - val_accuracy: 0.8379\n",
            "Epoch 101/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5130 - accuracy: 0.8211 - val_loss: 0.4668 - val_accuracy: 0.8392\n",
            "Epoch 102/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4658 - accuracy: 0.8276 - val_loss: 0.4652 - val_accuracy: 0.8338\n",
            "Epoch 103/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.4676 - accuracy: 0.8425 - val_loss: 0.4622 - val_accuracy: 0.8351\n",
            "Epoch 104/250\n",
            "23/23 [==============================] - 2s 106ms/step - loss: 0.5102 - accuracy: 0.8290 - val_loss: 0.4652 - val_accuracy: 0.8392\n",
            "Epoch 105/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.4936 - accuracy: 0.8359 - val_loss: 0.4610 - val_accuracy: 0.8351\n",
            "Epoch 106/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5173 - accuracy: 0.8199 - val_loss: 0.4644 - val_accuracy: 0.8351\n",
            "Epoch 107/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5149 - accuracy: 0.8194 - val_loss: 0.4605 - val_accuracy: 0.8379\n",
            "Epoch 108/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5050 - accuracy: 0.8278 - val_loss: 0.4604 - val_accuracy: 0.8365\n",
            "Epoch 109/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5026 - accuracy: 0.8237 - val_loss: 0.4647 - val_accuracy: 0.8420\n",
            "Epoch 110/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4706 - accuracy: 0.8297 - val_loss: 0.4600 - val_accuracy: 0.8338\n",
            "Epoch 111/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4999 - accuracy: 0.8179 - val_loss: 0.4610 - val_accuracy: 0.8351\n",
            "Epoch 112/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4659 - accuracy: 0.8361 - val_loss: 0.4601 - val_accuracy: 0.8406\n",
            "Epoch 113/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5074 - accuracy: 0.8177 - val_loss: 0.4580 - val_accuracy: 0.8351\n",
            "Epoch 114/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4678 - accuracy: 0.8361 - val_loss: 0.4611 - val_accuracy: 0.8392\n",
            "Epoch 115/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4788 - accuracy: 0.8326 - val_loss: 0.4605 - val_accuracy: 0.8338\n",
            "Epoch 116/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4902 - accuracy: 0.8262 - val_loss: 0.4578 - val_accuracy: 0.8433\n",
            "Epoch 117/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4776 - accuracy: 0.8338 - val_loss: 0.4580 - val_accuracy: 0.8392\n",
            "Epoch 118/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.4736 - accuracy: 0.8269 - val_loss: 0.4568 - val_accuracy: 0.8406\n",
            "Epoch 119/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.5033 - accuracy: 0.8170 - val_loss: 0.4589 - val_accuracy: 0.8351\n",
            "Epoch 120/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4819 - accuracy: 0.8354 - val_loss: 0.4552 - val_accuracy: 0.8392\n",
            "Epoch 121/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.4829 - accuracy: 0.8330 - val_loss: 0.4550 - val_accuracy: 0.8379\n",
            "Epoch 122/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5007 - accuracy: 0.8341 - val_loss: 0.4566 - val_accuracy: 0.8406\n",
            "Epoch 123/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4514 - accuracy: 0.8382 - val_loss: 0.4537 - val_accuracy: 0.8460\n",
            "Epoch 124/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4527 - accuracy: 0.8362 - val_loss: 0.4563 - val_accuracy: 0.8406\n",
            "Epoch 125/250\n",
            "23/23 [==============================] - 2s 106ms/step - loss: 0.4612 - accuracy: 0.8390 - val_loss: 0.4532 - val_accuracy: 0.8420\n",
            "Epoch 126/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4706 - accuracy: 0.8312 - val_loss: 0.4567 - val_accuracy: 0.8433\n",
            "Epoch 127/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.5048 - accuracy: 0.8378 - val_loss: 0.4567 - val_accuracy: 0.8379\n",
            "Epoch 128/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4651 - accuracy: 0.8379 - val_loss: 0.4541 - val_accuracy: 0.8379\n",
            "Epoch 129/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4588 - accuracy: 0.8421 - val_loss: 0.4539 - val_accuracy: 0.8433\n",
            "Epoch 130/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4331 - accuracy: 0.8563 - val_loss: 0.4542 - val_accuracy: 0.8406\n",
            "Epoch 131/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4681 - accuracy: 0.8486 - val_loss: 0.4524 - val_accuracy: 0.8447\n",
            "Epoch 132/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4361 - accuracy: 0.8494 - val_loss: 0.4531 - val_accuracy: 0.8406\n",
            "Epoch 133/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4546 - accuracy: 0.8388 - val_loss: 0.4525 - val_accuracy: 0.8433\n",
            "Epoch 134/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4527 - accuracy: 0.8410 - val_loss: 0.4516 - val_accuracy: 0.8433\n",
            "Epoch 135/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4678 - accuracy: 0.8354 - val_loss: 0.4521 - val_accuracy: 0.8392\n",
            "Epoch 136/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4610 - accuracy: 0.8405 - val_loss: 0.4556 - val_accuracy: 0.8406\n",
            "Epoch 137/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4601 - accuracy: 0.8342 - val_loss: 0.4511 - val_accuracy: 0.8392\n",
            "Epoch 138/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4472 - accuracy: 0.8352 - val_loss: 0.4513 - val_accuracy: 0.8460\n",
            "Epoch 139/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4437 - accuracy: 0.8459 - val_loss: 0.4489 - val_accuracy: 0.8447\n",
            "Epoch 140/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4596 - accuracy: 0.8445 - val_loss: 0.4468 - val_accuracy: 0.8460\n",
            "Epoch 141/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4329 - accuracy: 0.8441 - val_loss: 0.4519 - val_accuracy: 0.8420\n",
            "Epoch 142/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4426 - accuracy: 0.8408 - val_loss: 0.4455 - val_accuracy: 0.8447\n",
            "Epoch 143/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4341 - accuracy: 0.8392 - val_loss: 0.4504 - val_accuracy: 0.8420\n",
            "Epoch 144/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4314 - accuracy: 0.8485 - val_loss: 0.4471 - val_accuracy: 0.8460\n",
            "Epoch 145/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4418 - accuracy: 0.8477 - val_loss: 0.4489 - val_accuracy: 0.8515\n",
            "Epoch 146/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4237 - accuracy: 0.8563 - val_loss: 0.4469 - val_accuracy: 0.8460\n",
            "Epoch 147/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4369 - accuracy: 0.8531 - val_loss: 0.4471 - val_accuracy: 0.8433\n",
            "Epoch 148/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4265 - accuracy: 0.8396 - val_loss: 0.4476 - val_accuracy: 0.8447\n",
            "Epoch 149/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4320 - accuracy: 0.8525 - val_loss: 0.4547 - val_accuracy: 0.8406\n",
            "Epoch 150/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4338 - accuracy: 0.8486 - val_loss: 0.4446 - val_accuracy: 0.8501\n",
            "Epoch 151/250\n",
            "23/23 [==============================] - 2s 106ms/step - loss: 0.4823 - accuracy: 0.8364 - val_loss: 0.4494 - val_accuracy: 0.8406\n",
            "Epoch 152/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4226 - accuracy: 0.8537 - val_loss: 0.4488 - val_accuracy: 0.8406\n",
            "Epoch 153/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4473 - accuracy: 0.8449 - val_loss: 0.4469 - val_accuracy: 0.8406\n",
            "Epoch 154/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4240 - accuracy: 0.8554 - val_loss: 0.4500 - val_accuracy: 0.8460\n",
            "Epoch 155/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4297 - accuracy: 0.8412 - val_loss: 0.4468 - val_accuracy: 0.8420\n",
            "Epoch 156/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4470 - accuracy: 0.8449 - val_loss: 0.4461 - val_accuracy: 0.8515\n",
            "Epoch 157/250\n",
            "23/23 [==============================] - 2s 106ms/step - loss: 0.4517 - accuracy: 0.8418 - val_loss: 0.4491 - val_accuracy: 0.8406\n",
            "Epoch 158/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4295 - accuracy: 0.8410 - val_loss: 0.4466 - val_accuracy: 0.8447\n",
            "Epoch 159/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3945 - accuracy: 0.8573 - val_loss: 0.4462 - val_accuracy: 0.8529\n",
            "Epoch 160/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4266 - accuracy: 0.8496 - val_loss: 0.4449 - val_accuracy: 0.8447\n",
            "Epoch 161/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4207 - accuracy: 0.8555 - val_loss: 0.4477 - val_accuracy: 0.8447\n",
            "Epoch 162/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3918 - accuracy: 0.8650 - val_loss: 0.4449 - val_accuracy: 0.8433\n",
            "Epoch 163/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4116 - accuracy: 0.8494 - val_loss: 0.4463 - val_accuracy: 0.8433\n",
            "Epoch 164/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3865 - accuracy: 0.8655 - val_loss: 0.4439 - val_accuracy: 0.8460\n",
            "Epoch 165/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3907 - accuracy: 0.8633 - val_loss: 0.4420 - val_accuracy: 0.8474\n",
            "Epoch 166/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4360 - accuracy: 0.8438 - val_loss: 0.4437 - val_accuracy: 0.8474\n",
            "Epoch 167/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4053 - accuracy: 0.8510 - val_loss: 0.4423 - val_accuracy: 0.8488\n",
            "Epoch 168/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.4117 - accuracy: 0.8565 - val_loss: 0.4432 - val_accuracy: 0.8474\n",
            "Epoch 169/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3969 - accuracy: 0.8550 - val_loss: 0.4449 - val_accuracy: 0.8447\n",
            "Epoch 170/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4120 - accuracy: 0.8490 - val_loss: 0.4408 - val_accuracy: 0.8488\n",
            "Epoch 171/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4305 - accuracy: 0.8423 - val_loss: 0.4441 - val_accuracy: 0.8488\n",
            "Epoch 172/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3879 - accuracy: 0.8622 - val_loss: 0.4436 - val_accuracy: 0.8488\n",
            "Epoch 173/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4217 - accuracy: 0.8573 - val_loss: 0.4448 - val_accuracy: 0.8447\n",
            "Epoch 174/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4321 - accuracy: 0.8413 - val_loss: 0.4395 - val_accuracy: 0.8474\n",
            "Epoch 175/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3764 - accuracy: 0.8492 - val_loss: 0.4425 - val_accuracy: 0.8447\n",
            "Epoch 176/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3830 - accuracy: 0.8617 - val_loss: 0.4423 - val_accuracy: 0.8474\n",
            "Epoch 177/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4078 - accuracy: 0.8501 - val_loss: 0.4421 - val_accuracy: 0.8460\n",
            "Epoch 178/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3708 - accuracy: 0.8733 - val_loss: 0.4405 - val_accuracy: 0.8447\n",
            "Epoch 179/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.4394 - accuracy: 0.8434 - val_loss: 0.4414 - val_accuracy: 0.8488\n",
            "Epoch 180/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3810 - accuracy: 0.8656 - val_loss: 0.4393 - val_accuracy: 0.8488\n",
            "Epoch 181/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3833 - accuracy: 0.8621 - val_loss: 0.4423 - val_accuracy: 0.8488\n",
            "Epoch 182/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3831 - accuracy: 0.8635 - val_loss: 0.4393 - val_accuracy: 0.8488\n",
            "Epoch 183/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3866 - accuracy: 0.8634 - val_loss: 0.4416 - val_accuracy: 0.8488\n",
            "Epoch 184/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3932 - accuracy: 0.8647 - val_loss: 0.4440 - val_accuracy: 0.8447\n",
            "Epoch 185/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.4079 - accuracy: 0.8566 - val_loss: 0.4435 - val_accuracy: 0.8474\n",
            "Epoch 186/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3691 - accuracy: 0.8666 - val_loss: 0.4434 - val_accuracy: 0.8460\n",
            "Epoch 187/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3967 - accuracy: 0.8592 - val_loss: 0.4462 - val_accuracy: 0.8474\n",
            "Epoch 188/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3920 - accuracy: 0.8606 - val_loss: 0.4436 - val_accuracy: 0.8488\n",
            "Epoch 189/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3774 - accuracy: 0.8583 - val_loss: 0.4476 - val_accuracy: 0.8447\n",
            "Epoch 190/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4065 - accuracy: 0.8691 - val_loss: 0.4449 - val_accuracy: 0.8488\n",
            "Epoch 191/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3952 - accuracy: 0.8538 - val_loss: 0.4440 - val_accuracy: 0.8529\n",
            "Epoch 192/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3934 - accuracy: 0.8602 - val_loss: 0.4450 - val_accuracy: 0.8488\n",
            "Epoch 193/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3783 - accuracy: 0.8665 - val_loss: 0.4441 - val_accuracy: 0.8501\n",
            "Epoch 194/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3854 - accuracy: 0.8568 - val_loss: 0.4406 - val_accuracy: 0.8515\n",
            "Epoch 195/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3791 - accuracy: 0.8668 - val_loss: 0.4423 - val_accuracy: 0.8501\n",
            "Epoch 196/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3525 - accuracy: 0.8754 - val_loss: 0.4409 - val_accuracy: 0.8501\n",
            "Epoch 197/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3937 - accuracy: 0.8625 - val_loss: 0.4417 - val_accuracy: 0.8542\n",
            "Epoch 198/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3750 - accuracy: 0.8639 - val_loss: 0.4435 - val_accuracy: 0.8447\n",
            "Epoch 199/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3642 - accuracy: 0.8637 - val_loss: 0.4424 - val_accuracy: 0.8515\n",
            "Epoch 200/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.4008 - accuracy: 0.8461 - val_loss: 0.4482 - val_accuracy: 0.8420\n",
            "Epoch 201/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3946 - accuracy: 0.8613 - val_loss: 0.4432 - val_accuracy: 0.8501\n",
            "Epoch 202/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3818 - accuracy: 0.8566 - val_loss: 0.4417 - val_accuracy: 0.8488\n",
            "Epoch 203/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3627 - accuracy: 0.8702 - val_loss: 0.4396 - val_accuracy: 0.8529\n",
            "Epoch 204/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3555 - accuracy: 0.8698 - val_loss: 0.4410 - val_accuracy: 0.8501\n",
            "Epoch 205/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3841 - accuracy: 0.8684 - val_loss: 0.4423 - val_accuracy: 0.8515\n",
            "Epoch 206/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3762 - accuracy: 0.8688 - val_loss: 0.4396 - val_accuracy: 0.8501\n",
            "Epoch 207/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3980 - accuracy: 0.8533 - val_loss: 0.4419 - val_accuracy: 0.8542\n",
            "Epoch 208/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3446 - accuracy: 0.8775 - val_loss: 0.4426 - val_accuracy: 0.8460\n",
            "Epoch 209/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3442 - accuracy: 0.8778 - val_loss: 0.4406 - val_accuracy: 0.8529\n",
            "Epoch 210/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3670 - accuracy: 0.8600 - val_loss: 0.4388 - val_accuracy: 0.8515\n",
            "Epoch 211/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3848 - accuracy: 0.8713 - val_loss: 0.4386 - val_accuracy: 0.8542\n",
            "Epoch 212/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3649 - accuracy: 0.8743 - val_loss: 0.4437 - val_accuracy: 0.8529\n",
            "Epoch 213/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3669 - accuracy: 0.8730 - val_loss: 0.4397 - val_accuracy: 0.8515\n",
            "Epoch 214/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3434 - accuracy: 0.8703 - val_loss: 0.4392 - val_accuracy: 0.8529\n",
            "Epoch 215/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3534 - accuracy: 0.8726 - val_loss: 0.4413 - val_accuracy: 0.8529\n",
            "Epoch 216/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3403 - accuracy: 0.8854 - val_loss: 0.4401 - val_accuracy: 0.8515\n",
            "Epoch 217/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3794 - accuracy: 0.8657 - val_loss: 0.4403 - val_accuracy: 0.8556\n",
            "Epoch 218/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3751 - accuracy: 0.8637 - val_loss: 0.4403 - val_accuracy: 0.8474\n",
            "Epoch 219/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3638 - accuracy: 0.8688 - val_loss: 0.4407 - val_accuracy: 0.8529\n",
            "Epoch 220/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3787 - accuracy: 0.8758 - val_loss: 0.4406 - val_accuracy: 0.8515\n",
            "Epoch 221/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3666 - accuracy: 0.8648 - val_loss: 0.4413 - val_accuracy: 0.8597\n",
            "Epoch 222/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3379 - accuracy: 0.8786 - val_loss: 0.4399 - val_accuracy: 0.8515\n",
            "Epoch 223/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3619 - accuracy: 0.8736 - val_loss: 0.4429 - val_accuracy: 0.8556\n",
            "Epoch 224/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3547 - accuracy: 0.8762 - val_loss: 0.4404 - val_accuracy: 0.8529\n",
            "Epoch 225/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3511 - accuracy: 0.8763 - val_loss: 0.4386 - val_accuracy: 0.8569\n",
            "Epoch 226/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3533 - accuracy: 0.8722 - val_loss: 0.4392 - val_accuracy: 0.8597\n",
            "Epoch 227/250\n",
            "23/23 [==============================] - 2s 103ms/step - loss: 0.3454 - accuracy: 0.8754 - val_loss: 0.4425 - val_accuracy: 0.8597\n",
            "Epoch 228/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3585 - accuracy: 0.8735 - val_loss: 0.4391 - val_accuracy: 0.8583\n",
            "Epoch 229/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3364 - accuracy: 0.8789 - val_loss: 0.4385 - val_accuracy: 0.8569\n",
            "Epoch 230/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3456 - accuracy: 0.8717 - val_loss: 0.4370 - val_accuracy: 0.8583\n",
            "Epoch 231/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3269 - accuracy: 0.8824 - val_loss: 0.4358 - val_accuracy: 0.8597\n",
            "Epoch 232/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3521 - accuracy: 0.8702 - val_loss: 0.4352 - val_accuracy: 0.8583\n",
            "Epoch 233/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3356 - accuracy: 0.8744 - val_loss: 0.4364 - val_accuracy: 0.8583\n",
            "Epoch 234/250\n",
            "23/23 [==============================] - 2s 106ms/step - loss: 0.3576 - accuracy: 0.8648 - val_loss: 0.4361 - val_accuracy: 0.8597\n",
            "Epoch 235/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3080 - accuracy: 0.8864 - val_loss: 0.4360 - val_accuracy: 0.8624\n",
            "Epoch 236/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3367 - accuracy: 0.8788 - val_loss: 0.4357 - val_accuracy: 0.8583\n",
            "Epoch 237/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3405 - accuracy: 0.8778 - val_loss: 0.4345 - val_accuracy: 0.8610\n",
            "Epoch 238/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3619 - accuracy: 0.8723 - val_loss: 0.4358 - val_accuracy: 0.8583\n",
            "Epoch 239/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3304 - accuracy: 0.8729 - val_loss: 0.4355 - val_accuracy: 0.8583\n",
            "Epoch 240/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3407 - accuracy: 0.8758 - val_loss: 0.4355 - val_accuracy: 0.8583\n",
            "Epoch 241/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3709 - accuracy: 0.8752 - val_loss: 0.4379 - val_accuracy: 0.8610\n",
            "Epoch 242/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3467 - accuracy: 0.8756 - val_loss: 0.4349 - val_accuracy: 0.8569\n",
            "Epoch 243/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3510 - accuracy: 0.8764 - val_loss: 0.4337 - val_accuracy: 0.8583\n",
            "Epoch 244/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3464 - accuracy: 0.8796 - val_loss: 0.4342 - val_accuracy: 0.8583\n",
            "Epoch 245/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3313 - accuracy: 0.8768 - val_loss: 0.4342 - val_accuracy: 0.8583\n",
            "Epoch 246/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3706 - accuracy: 0.8633 - val_loss: 0.4314 - val_accuracy: 0.8624\n",
            "Epoch 247/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3396 - accuracy: 0.8827 - val_loss: 0.4324 - val_accuracy: 0.8583\n",
            "Epoch 248/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3343 - accuracy: 0.8812 - val_loss: 0.4331 - val_accuracy: 0.8583\n",
            "Epoch 249/250\n",
            "23/23 [==============================] - 2s 105ms/step - loss: 0.3432 - accuracy: 0.8800 - val_loss: 0.4302 - val_accuracy: 0.8624\n",
            "Epoch 250/250\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 0.3290 - accuracy: 0.8743 - val_loss: 0.4312 - val_accuracy: 0.8638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nyvMpuJ2sFaI",
        "outputId": "dbc69409-1a79-4d87-d4ee-d7a4abd381eb"
      },
      "source": [
        "# fine tuning\n",
        "\n",
        "# Create the base model \n",
        "base_model = tf.keras.applications.InceptionV3(input_shape=(160,160,3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "#base_model.summary()\n",
        "\n",
        "# Freeze some first the layers\n",
        "fine_tune_at = 150\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable =  False\n",
        "\n",
        "print(len(base_model.trainable_variables))\n",
        "\n",
        "# process data\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
        "])\n",
        "\n",
        "# flattening\n",
        "flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "# final layer\n",
        "prediction_layer = tf.keras.layers.Dense(5)\n",
        "\n",
        "# construct a new network\n",
        "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = base_model(x)\n",
        "x = flatten(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "print(len(model.trainable_variables))\n",
        "\n",
        "base_learning_rate = 0.0001\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate/10),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(train_dataset,\n",
        "                         epochs=250,\n",
        "                         validation_data=validation_dataset)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(history_fine.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_fine.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(history_fine.history['loss'], label='Training Loss')\n",
        "plt.plot(history_fine.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96\n",
            "98\n",
            "Epoch 1/250\n",
            "23/23 [==============================] - 10s 227ms/step - loss: 1.8350 - accuracy: 0.2326 - val_loss: 1.8133 - val_accuracy: 0.3542\n",
            "Epoch 2/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 1.4145 - accuracy: 0.4265 - val_loss: 1.3346 - val_accuracy: 0.5313\n",
            "Epoch 3/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 1.1661 - accuracy: 0.5446 - val_loss: 1.0617 - val_accuracy: 0.6226\n",
            "Epoch 4/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.9981 - accuracy: 0.6241 - val_loss: 0.9110 - val_accuracy: 0.6853\n",
            "Epoch 5/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.9264 - accuracy: 0.6448 - val_loss: 0.8066 - val_accuracy: 0.7221\n",
            "Epoch 6/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.8122 - accuracy: 0.6956 - val_loss: 0.7177 - val_accuracy: 0.7507\n",
            "Epoch 7/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.7258 - accuracy: 0.7517 - val_loss: 0.6680 - val_accuracy: 0.7752\n",
            "Epoch 8/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.7088 - accuracy: 0.7486 - val_loss: 0.6307 - val_accuracy: 0.7793\n",
            "Epoch 9/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.6365 - accuracy: 0.7744 - val_loss: 0.6034 - val_accuracy: 0.7875\n",
            "Epoch 10/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.5969 - accuracy: 0.7803 - val_loss: 0.5818 - val_accuracy: 0.7916\n",
            "Epoch 11/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.5452 - accuracy: 0.8182 - val_loss: 0.5713 - val_accuracy: 0.7956\n",
            "Epoch 12/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.5163 - accuracy: 0.8172 - val_loss: 0.5539 - val_accuracy: 0.7984\n",
            "Epoch 13/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.4893 - accuracy: 0.8261 - val_loss: 0.5451 - val_accuracy: 0.8079\n",
            "Epoch 14/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.4619 - accuracy: 0.8370 - val_loss: 0.5331 - val_accuracy: 0.8106\n",
            "Epoch 15/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.4477 - accuracy: 0.8484 - val_loss: 0.5252 - val_accuracy: 0.8134\n",
            "Epoch 16/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.4151 - accuracy: 0.8539 - val_loss: 0.5156 - val_accuracy: 0.8270\n",
            "Epoch 17/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.3892 - accuracy: 0.8661 - val_loss: 0.5046 - val_accuracy: 0.8311\n",
            "Epoch 18/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.3803 - accuracy: 0.8601 - val_loss: 0.4950 - val_accuracy: 0.8351\n",
            "Epoch 19/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.3739 - accuracy: 0.8653 - val_loss: 0.4843 - val_accuracy: 0.8365\n",
            "Epoch 20/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.3392 - accuracy: 0.8855 - val_loss: 0.4773 - val_accuracy: 0.8406\n",
            "Epoch 21/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.3292 - accuracy: 0.8859 - val_loss: 0.4647 - val_accuracy: 0.8392\n",
            "Epoch 22/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.3193 - accuracy: 0.8905 - val_loss: 0.4572 - val_accuracy: 0.8406\n",
            "Epoch 23/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.2796 - accuracy: 0.9083 - val_loss: 0.4524 - val_accuracy: 0.8447\n",
            "Epoch 24/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.3000 - accuracy: 0.8976 - val_loss: 0.4436 - val_accuracy: 0.8488\n",
            "Epoch 25/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.2691 - accuracy: 0.9082 - val_loss: 0.4366 - val_accuracy: 0.8556\n",
            "Epoch 26/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.2619 - accuracy: 0.9036 - val_loss: 0.4312 - val_accuracy: 0.8542\n",
            "Epoch 27/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.2463 - accuracy: 0.9259 - val_loss: 0.4281 - val_accuracy: 0.8556\n",
            "Epoch 28/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.2409 - accuracy: 0.9262 - val_loss: 0.4245 - val_accuracy: 0.8556\n",
            "Epoch 29/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.2379 - accuracy: 0.9180 - val_loss: 0.4206 - val_accuracy: 0.8542\n",
            "Epoch 30/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.2273 - accuracy: 0.9241 - val_loss: 0.4175 - val_accuracy: 0.8624\n",
            "Epoch 31/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.2203 - accuracy: 0.9287 - val_loss: 0.4116 - val_accuracy: 0.8665\n",
            "Epoch 32/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1999 - accuracy: 0.9308 - val_loss: 0.4083 - val_accuracy: 0.8624\n",
            "Epoch 33/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1877 - accuracy: 0.9426 - val_loss: 0.4067 - val_accuracy: 0.8678\n",
            "Epoch 34/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1932 - accuracy: 0.9364 - val_loss: 0.4046 - val_accuracy: 0.8706\n",
            "Epoch 35/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1667 - accuracy: 0.9472 - val_loss: 0.4032 - val_accuracy: 0.8706\n",
            "Epoch 36/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1738 - accuracy: 0.9435 - val_loss: 0.4013 - val_accuracy: 0.8733\n",
            "Epoch 37/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1701 - accuracy: 0.9458 - val_loss: 0.4041 - val_accuracy: 0.8719\n",
            "Epoch 38/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1453 - accuracy: 0.9603 - val_loss: 0.4009 - val_accuracy: 0.8706\n",
            "Epoch 39/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1481 - accuracy: 0.9566 - val_loss: 0.3984 - val_accuracy: 0.8747\n",
            "Epoch 40/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1504 - accuracy: 0.9528 - val_loss: 0.3971 - val_accuracy: 0.8747\n",
            "Epoch 41/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.1301 - accuracy: 0.9650 - val_loss: 0.3938 - val_accuracy: 0.8828\n",
            "Epoch 42/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.1288 - accuracy: 0.9624 - val_loss: 0.3922 - val_accuracy: 0.8801\n",
            "Epoch 43/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.1227 - accuracy: 0.9631 - val_loss: 0.3924 - val_accuracy: 0.8828\n",
            "Epoch 44/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1277 - accuracy: 0.9600 - val_loss: 0.3958 - val_accuracy: 0.8787\n",
            "Epoch 45/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1022 - accuracy: 0.9696 - val_loss: 0.3957 - val_accuracy: 0.8774\n",
            "Epoch 46/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0990 - accuracy: 0.9762 - val_loss: 0.3953 - val_accuracy: 0.8787\n",
            "Epoch 47/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1047 - accuracy: 0.9709 - val_loss: 0.3909 - val_accuracy: 0.8842\n",
            "Epoch 48/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0953 - accuracy: 0.9724 - val_loss: 0.3873 - val_accuracy: 0.8856\n",
            "Epoch 49/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.1021 - accuracy: 0.9703 - val_loss: 0.3890 - val_accuracy: 0.8869\n",
            "Epoch 50/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0922 - accuracy: 0.9750 - val_loss: 0.3921 - val_accuracy: 0.8842\n",
            "Epoch 51/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0932 - accuracy: 0.9772 - val_loss: 0.3898 - val_accuracy: 0.8869\n",
            "Epoch 52/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0916 - accuracy: 0.9761 - val_loss: 0.3885 - val_accuracy: 0.8869\n",
            "Epoch 53/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0880 - accuracy: 0.9746 - val_loss: 0.3902 - val_accuracy: 0.8883\n",
            "Epoch 54/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0852 - accuracy: 0.9727 - val_loss: 0.3897 - val_accuracy: 0.8896\n",
            "Epoch 55/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0812 - accuracy: 0.9800 - val_loss: 0.3900 - val_accuracy: 0.8883\n",
            "Epoch 56/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0703 - accuracy: 0.9800 - val_loss: 0.3877 - val_accuracy: 0.8910\n",
            "Epoch 57/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0658 - accuracy: 0.9825 - val_loss: 0.3853 - val_accuracy: 0.8883\n",
            "Epoch 58/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0644 - accuracy: 0.9862 - val_loss: 0.3861 - val_accuracy: 0.8883\n",
            "Epoch 59/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0577 - accuracy: 0.9887 - val_loss: 0.3846 - val_accuracy: 0.8869\n",
            "Epoch 60/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0543 - accuracy: 0.9872 - val_loss: 0.3842 - val_accuracy: 0.8869\n",
            "Epoch 61/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0544 - accuracy: 0.9871 - val_loss: 0.3853 - val_accuracy: 0.8869\n",
            "Epoch 62/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0562 - accuracy: 0.9865 - val_loss: 0.3858 - val_accuracy: 0.8883\n",
            "Epoch 63/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0627 - accuracy: 0.9825 - val_loss: 0.3821 - val_accuracy: 0.8896\n",
            "Epoch 64/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0572 - accuracy: 0.9854 - val_loss: 0.3830 - val_accuracy: 0.8910\n",
            "Epoch 65/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0592 - accuracy: 0.9855 - val_loss: 0.3829 - val_accuracy: 0.8924\n",
            "Epoch 66/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0508 - accuracy: 0.9885 - val_loss: 0.3824 - val_accuracy: 0.8951\n",
            "Epoch 67/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0524 - accuracy: 0.9856 - val_loss: 0.3839 - val_accuracy: 0.8924\n",
            "Epoch 68/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0471 - accuracy: 0.9890 - val_loss: 0.3851 - val_accuracy: 0.8937\n",
            "Epoch 69/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0430 - accuracy: 0.9917 - val_loss: 0.3808 - val_accuracy: 0.8965\n",
            "Epoch 70/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0443 - accuracy: 0.9896 - val_loss: 0.3840 - val_accuracy: 0.8937\n",
            "Epoch 71/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0421 - accuracy: 0.9894 - val_loss: 0.3847 - val_accuracy: 0.8951\n",
            "Epoch 72/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0401 - accuracy: 0.9879 - val_loss: 0.3833 - val_accuracy: 0.8965\n",
            "Epoch 73/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0385 - accuracy: 0.9915 - val_loss: 0.3845 - val_accuracy: 0.8951\n",
            "Epoch 74/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0342 - accuracy: 0.9927 - val_loss: 0.3860 - val_accuracy: 0.8937\n",
            "Epoch 75/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0307 - accuracy: 0.9957 - val_loss: 0.3912 - val_accuracy: 0.8937\n",
            "Epoch 76/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0325 - accuracy: 0.9918 - val_loss: 0.3919 - val_accuracy: 0.8965\n",
            "Epoch 77/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0333 - accuracy: 0.9918 - val_loss: 0.3968 - val_accuracy: 0.8951\n",
            "Epoch 78/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0359 - accuracy: 0.9914 - val_loss: 0.3959 - val_accuracy: 0.8937\n",
            "Epoch 79/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0282 - accuracy: 0.9926 - val_loss: 0.3933 - val_accuracy: 0.8937\n",
            "Epoch 80/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0255 - accuracy: 0.9952 - val_loss: 0.3915 - val_accuracy: 0.8978\n",
            "Epoch 81/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0233 - accuracy: 0.9967 - val_loss: 0.3911 - val_accuracy: 0.8965\n",
            "Epoch 82/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0245 - accuracy: 0.9947 - val_loss: 0.3912 - val_accuracy: 0.8951\n",
            "Epoch 83/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0249 - accuracy: 0.9950 - val_loss: 0.3926 - val_accuracy: 0.8992\n",
            "Epoch 84/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0194 - accuracy: 0.9980 - val_loss: 0.3922 - val_accuracy: 0.8951\n",
            "Epoch 85/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0272 - accuracy: 0.9951 - val_loss: 0.3991 - val_accuracy: 0.8992\n",
            "Epoch 86/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0206 - accuracy: 0.9978 - val_loss: 0.3969 - val_accuracy: 0.8937\n",
            "Epoch 87/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0201 - accuracy: 0.9988 - val_loss: 0.3987 - val_accuracy: 0.8978\n",
            "Epoch 88/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0202 - accuracy: 0.9966 - val_loss: 0.3968 - val_accuracy: 0.8992\n",
            "Epoch 89/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0178 - accuracy: 0.9986 - val_loss: 0.3957 - val_accuracy: 0.9033\n",
            "Epoch 90/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0179 - accuracy: 0.9980 - val_loss: 0.3964 - val_accuracy: 0.9046\n",
            "Epoch 91/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0145 - accuracy: 0.9987 - val_loss: 0.3964 - val_accuracy: 0.9033\n",
            "Epoch 92/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0252 - accuracy: 0.9919 - val_loss: 0.3989 - val_accuracy: 0.9033\n",
            "Epoch 93/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0209 - accuracy: 0.9953 - val_loss: 0.3999 - val_accuracy: 0.9005\n",
            "Epoch 94/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0172 - accuracy: 0.9967 - val_loss: 0.3979 - val_accuracy: 0.9005\n",
            "Epoch 95/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0140 - accuracy: 0.9975 - val_loss: 0.3997 - val_accuracy: 0.9019\n",
            "Epoch 96/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0164 - accuracy: 0.9962 - val_loss: 0.4036 - val_accuracy: 0.8978\n",
            "Epoch 97/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0121 - accuracy: 0.9989 - val_loss: 0.4036 - val_accuracy: 0.9033\n",
            "Epoch 98/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0160 - accuracy: 0.9960 - val_loss: 0.4044 - val_accuracy: 0.9046\n",
            "Epoch 99/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 0.4047 - val_accuracy: 0.9033\n",
            "Epoch 100/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0118 - accuracy: 0.9994 - val_loss: 0.4052 - val_accuracy: 0.9046\n",
            "Epoch 101/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.4082 - val_accuracy: 0.9046\n",
            "Epoch 102/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0121 - accuracy: 0.9984 - val_loss: 0.4068 - val_accuracy: 0.9046\n",
            "Epoch 103/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0116 - accuracy: 0.9986 - val_loss: 0.4058 - val_accuracy: 0.9046\n",
            "Epoch 104/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0147 - accuracy: 0.9965 - val_loss: 0.4018 - val_accuracy: 0.9046\n",
            "Epoch 105/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0111 - accuracy: 0.9990 - val_loss: 0.4034 - val_accuracy: 0.9033\n",
            "Epoch 106/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0134 - accuracy: 0.9973 - val_loss: 0.4028 - val_accuracy: 0.8992\n",
            "Epoch 107/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0118 - accuracy: 0.9974 - val_loss: 0.4040 - val_accuracy: 0.8992\n",
            "Epoch 108/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.4045 - val_accuracy: 0.8978\n",
            "Epoch 109/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0096 - accuracy: 0.9999 - val_loss: 0.4011 - val_accuracy: 0.9046\n",
            "Epoch 110/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0106 - accuracy: 0.9998 - val_loss: 0.4045 - val_accuracy: 0.9033\n",
            "Epoch 111/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0092 - accuracy: 0.9994 - val_loss: 0.4074 - val_accuracy: 0.9033\n",
            "Epoch 112/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0090 - accuracy: 0.9993 - val_loss: 0.4065 - val_accuracy: 0.9033\n",
            "Epoch 113/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0089 - accuracy: 0.9983 - val_loss: 0.4106 - val_accuracy: 0.9046\n",
            "Epoch 114/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0088 - accuracy: 0.9988 - val_loss: 0.4110 - val_accuracy: 0.9087\n",
            "Epoch 115/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0096 - accuracy: 0.9983 - val_loss: 0.4183 - val_accuracy: 0.9046\n",
            "Epoch 116/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0108 - accuracy: 0.9970 - val_loss: 0.4146 - val_accuracy: 0.9033\n",
            "Epoch 117/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0073 - accuracy: 0.9998 - val_loss: 0.4139 - val_accuracy: 0.9046\n",
            "Epoch 118/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0103 - accuracy: 0.9976 - val_loss: 0.4113 - val_accuracy: 0.9074\n",
            "Epoch 119/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0075 - accuracy: 0.9986 - val_loss: 0.4097 - val_accuracy: 0.9046\n",
            "Epoch 120/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0069 - accuracy: 0.9989 - val_loss: 0.4112 - val_accuracy: 0.9060\n",
            "Epoch 121/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0081 - accuracy: 0.9995 - val_loss: 0.4103 - val_accuracy: 0.9060\n",
            "Epoch 122/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.4061 - val_accuracy: 0.9019\n",
            "Epoch 123/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0072 - accuracy: 0.9990 - val_loss: 0.4086 - val_accuracy: 0.9019\n",
            "Epoch 124/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0073 - accuracy: 0.9997 - val_loss: 0.4149 - val_accuracy: 0.9019\n",
            "Epoch 125/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0070 - accuracy: 0.9986 - val_loss: 0.4220 - val_accuracy: 0.9033\n",
            "Epoch 126/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.4234 - val_accuracy: 0.9033\n",
            "Epoch 127/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.4247 - val_accuracy: 0.9019\n",
            "Epoch 128/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0065 - accuracy: 0.9989 - val_loss: 0.4220 - val_accuracy: 0.9019\n",
            "Epoch 129/250\n",
            "23/23 [==============================] - 4s 158ms/step - loss: 0.0056 - accuracy: 0.9996 - val_loss: 0.4178 - val_accuracy: 0.9033\n",
            "Epoch 130/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0051 - accuracy: 0.9998 - val_loss: 0.4177 - val_accuracy: 0.9046\n",
            "Epoch 131/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.4217 - val_accuracy: 0.9033\n",
            "Epoch 132/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0065 - accuracy: 0.9986 - val_loss: 0.4235 - val_accuracy: 0.9060\n",
            "Epoch 133/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.4261 - val_accuracy: 0.8992\n",
            "Epoch 134/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.4244 - val_accuracy: 0.9033\n",
            "Epoch 135/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.4232 - val_accuracy: 0.9033\n",
            "Epoch 136/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0060 - accuracy: 0.9997 - val_loss: 0.4276 - val_accuracy: 0.8992\n",
            "Epoch 137/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0052 - accuracy: 0.9997 - val_loss: 0.4312 - val_accuracy: 0.8978\n",
            "Epoch 138/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0055 - accuracy: 0.9997 - val_loss: 0.4299 - val_accuracy: 0.8978\n",
            "Epoch 139/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.4335 - val_accuracy: 0.8965\n",
            "Epoch 140/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.4292 - val_accuracy: 0.8965\n",
            "Epoch 141/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0056 - accuracy: 0.9992 - val_loss: 0.4315 - val_accuracy: 0.9019\n",
            "Epoch 142/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0036 - accuracy: 0.9999 - val_loss: 0.4331 - val_accuracy: 0.9019\n",
            "Epoch 143/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0057 - accuracy: 0.9992 - val_loss: 0.4352 - val_accuracy: 0.8992\n",
            "Epoch 144/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 0.4358 - val_accuracy: 0.9005\n",
            "Epoch 145/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.4401 - val_accuracy: 0.8978\n",
            "Epoch 146/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 0.4415 - val_accuracy: 0.8978\n",
            "Epoch 147/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4403 - val_accuracy: 0.8992\n",
            "Epoch 148/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0030 - accuracy: 0.9999 - val_loss: 0.4393 - val_accuracy: 0.8978\n",
            "Epoch 149/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4383 - val_accuracy: 0.9005\n",
            "Epoch 150/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.4419 - val_accuracy: 0.8992\n",
            "Epoch 151/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0039 - accuracy: 0.9996 - val_loss: 0.4429 - val_accuracy: 0.9019\n",
            "Epoch 152/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0031 - accuracy: 0.9998 - val_loss: 0.4401 - val_accuracy: 0.9019\n",
            "Epoch 153/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0082 - accuracy: 0.9965 - val_loss: 0.4348 - val_accuracy: 0.9005\n",
            "Epoch 154/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0034 - accuracy: 0.9998 - val_loss: 0.4333 - val_accuracy: 0.9046\n",
            "Epoch 155/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0032 - accuracy: 0.9999 - val_loss: 0.4327 - val_accuracy: 0.9046\n",
            "Epoch 156/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0033 - accuracy: 0.9999 - val_loss: 0.4349 - val_accuracy: 0.9046\n",
            "Epoch 157/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.4382 - val_accuracy: 0.9046\n",
            "Epoch 158/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4398 - val_accuracy: 0.9033\n",
            "Epoch 159/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.4417 - val_accuracy: 0.9046\n",
            "Epoch 160/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0026 - accuracy: 0.9999 - val_loss: 0.4402 - val_accuracy: 0.9074\n",
            "Epoch 161/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4410 - val_accuracy: 0.9019\n",
            "Epoch 162/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4417 - val_accuracy: 0.9019\n",
            "Epoch 163/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.4461 - val_accuracy: 0.9033\n",
            "Epoch 164/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4435 - val_accuracy: 0.9005\n",
            "Epoch 165/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4379 - val_accuracy: 0.9033\n",
            "Epoch 166/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4385 - val_accuracy: 0.9060\n",
            "Epoch 167/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.4396 - val_accuracy: 0.9074\n",
            "Epoch 168/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4415 - val_accuracy: 0.9033\n",
            "Epoch 169/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.4378 - val_accuracy: 0.9005\n",
            "Epoch 170/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.4343 - val_accuracy: 0.9046\n",
            "Epoch 171/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4338 - val_accuracy: 0.9019\n",
            "Epoch 172/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0050 - accuracy: 0.9992 - val_loss: 0.4278 - val_accuracy: 0.9074\n",
            "Epoch 173/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4305 - val_accuracy: 0.9074\n",
            "Epoch 174/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4329 - val_accuracy: 0.9074\n",
            "Epoch 175/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4359 - val_accuracy: 0.9101\n",
            "Epoch 176/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.4415 - val_accuracy: 0.9046\n",
            "Epoch 177/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.4415 - val_accuracy: 0.9033\n",
            "Epoch 178/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.4375 - val_accuracy: 0.9114\n",
            "Epoch 179/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4344 - val_accuracy: 0.9074\n",
            "Epoch 180/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4387 - val_accuracy: 0.9033\n",
            "Epoch 181/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.4373 - val_accuracy: 0.9074\n",
            "Epoch 182/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.4376 - val_accuracy: 0.9087\n",
            "Epoch 183/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4397 - val_accuracy: 0.9060\n",
            "Epoch 184/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4411 - val_accuracy: 0.9087\n",
            "Epoch 185/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4449 - val_accuracy: 0.9074\n",
            "Epoch 186/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4497 - val_accuracy: 0.9046\n",
            "Epoch 187/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4534 - val_accuracy: 0.9033\n",
            "Epoch 188/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.4577 - val_accuracy: 0.9033\n",
            "Epoch 189/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4552 - val_accuracy: 0.9033\n",
            "Epoch 190/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4510 - val_accuracy: 0.9019\n",
            "Epoch 191/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4558 - val_accuracy: 0.9019\n",
            "Epoch 192/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4573 - val_accuracy: 0.9005\n",
            "Epoch 193/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0030 - accuracy: 0.9985 - val_loss: 0.4641 - val_accuracy: 0.9033\n",
            "Epoch 194/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4627 - val_accuracy: 0.9019\n",
            "Epoch 195/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4605 - val_accuracy: 0.9060\n",
            "Epoch 196/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.4572 - val_accuracy: 0.9060\n",
            "Epoch 197/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4580 - val_accuracy: 0.9060\n",
            "Epoch 198/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4566 - val_accuracy: 0.9060\n",
            "Epoch 199/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.4587 - val_accuracy: 0.9074\n",
            "Epoch 200/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4611 - val_accuracy: 0.9074\n",
            "Epoch 201/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4691 - val_accuracy: 0.9087\n",
            "Epoch 202/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4691 - val_accuracy: 0.9060\n",
            "Epoch 203/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 9.6740e-04 - accuracy: 1.0000 - val_loss: 0.4734 - val_accuracy: 0.9060\n",
            "Epoch 204/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4789 - val_accuracy: 0.9019\n",
            "Epoch 205/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 9.3726e-04 - accuracy: 1.0000 - val_loss: 0.4773 - val_accuracy: 0.9046\n",
            "Epoch 206/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4780 - val_accuracy: 0.9046\n",
            "Epoch 207/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 9.6483e-04 - accuracy: 1.0000 - val_loss: 0.4701 - val_accuracy: 0.9060\n",
            "Epoch 208/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4698 - val_accuracy: 0.9033\n",
            "Epoch 209/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4689 - val_accuracy: 0.9033\n",
            "Epoch 210/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4737 - val_accuracy: 0.9033\n",
            "Epoch 211/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4889 - val_accuracy: 0.9019\n",
            "Epoch 212/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.4871 - val_accuracy: 0.9005\n",
            "Epoch 213/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4850 - val_accuracy: 0.9005\n",
            "Epoch 214/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 7.9660e-04 - accuracy: 1.0000 - val_loss: 0.4807 - val_accuracy: 0.9019\n",
            "Epoch 215/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 7.0346e-04 - accuracy: 1.0000 - val_loss: 0.4786 - val_accuracy: 0.9019\n",
            "Epoch 216/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 8.8223e-04 - accuracy: 1.0000 - val_loss: 0.4761 - val_accuracy: 0.9019\n",
            "Epoch 217/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 8.5314e-04 - accuracy: 1.0000 - val_loss: 0.4780 - val_accuracy: 0.9060\n",
            "Epoch 218/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 9.3225e-04 - accuracy: 1.0000 - val_loss: 0.4779 - val_accuracy: 0.9060\n",
            "Epoch 219/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4815 - val_accuracy: 0.9060\n",
            "Epoch 220/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 8.0069e-04 - accuracy: 1.0000 - val_loss: 0.4800 - val_accuracy: 0.9060\n",
            "Epoch 221/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 9.5919e-04 - accuracy: 1.0000 - val_loss: 0.4790 - val_accuracy: 0.9060\n",
            "Epoch 222/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 6.9937e-04 - accuracy: 1.0000 - val_loss: 0.4775 - val_accuracy: 0.9033\n",
            "Epoch 223/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 9.0885e-04 - accuracy: 1.0000 - val_loss: 0.4811 - val_accuracy: 0.9033\n",
            "Epoch 224/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 6.7555e-04 - accuracy: 1.0000 - val_loss: 0.4829 - val_accuracy: 0.9019\n",
            "Epoch 225/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 9.3340e-04 - accuracy: 1.0000 - val_loss: 0.4808 - val_accuracy: 0.9046\n",
            "Epoch 226/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 6.8581e-04 - accuracy: 1.0000 - val_loss: 0.4806 - val_accuracy: 0.9046\n",
            "Epoch 227/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 6.9865e-04 - accuracy: 1.0000 - val_loss: 0.4830 - val_accuracy: 0.9033\n",
            "Epoch 228/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 7.1247e-04 - accuracy: 1.0000 - val_loss: 0.4825 - val_accuracy: 0.9033\n",
            "Epoch 229/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 5.5269e-04 - accuracy: 1.0000 - val_loss: 0.4808 - val_accuracy: 0.9046\n",
            "Epoch 230/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4783 - val_accuracy: 0.9046\n",
            "Epoch 231/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 9.1774e-04 - accuracy: 1.0000 - val_loss: 0.4808 - val_accuracy: 0.9060\n",
            "Epoch 232/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 6.3160e-04 - accuracy: 1.0000 - val_loss: 0.4814 - val_accuracy: 0.9046\n",
            "Epoch 233/250\n",
            "23/23 [==============================] - 4s 161ms/step - loss: 9.4358e-04 - accuracy: 0.9997 - val_loss: 0.4856 - val_accuracy: 0.9046\n",
            "Epoch 234/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 7.9617e-04 - accuracy: 1.0000 - val_loss: 0.4861 - val_accuracy: 0.9046\n",
            "Epoch 235/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 6.7777e-04 - accuracy: 1.0000 - val_loss: 0.4895 - val_accuracy: 0.9033\n",
            "Epoch 236/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 8.4262e-04 - accuracy: 0.9998 - val_loss: 0.4911 - val_accuracy: 0.9046\n",
            "Epoch 237/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 6.4067e-04 - accuracy: 1.0000 - val_loss: 0.4909 - val_accuracy: 0.9046\n",
            "Epoch 238/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 7.7499e-04 - accuracy: 1.0000 - val_loss: 0.4930 - val_accuracy: 0.9019\n",
            "Epoch 239/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 7.8642e-04 - accuracy: 1.0000 - val_loss: 0.4938 - val_accuracy: 0.9005\n",
            "Epoch 240/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 5.1422e-04 - accuracy: 1.0000 - val_loss: 0.4913 - val_accuracy: 0.9005\n",
            "Epoch 241/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 8.8178e-04 - accuracy: 0.9999 - val_loss: 0.4910 - val_accuracy: 0.9019\n",
            "Epoch 242/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 5.5140e-04 - accuracy: 1.0000 - val_loss: 0.4924 - val_accuracy: 0.9033\n",
            "Epoch 243/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 6.6750e-04 - accuracy: 1.0000 - val_loss: 0.4920 - val_accuracy: 0.9046\n",
            "Epoch 244/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 5.4323e-04 - accuracy: 1.0000 - val_loss: 0.4913 - val_accuracy: 0.9074\n",
            "Epoch 245/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 6.4919e-04 - accuracy: 1.0000 - val_loss: 0.4902 - val_accuracy: 0.9019\n",
            "Epoch 246/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 5.9781e-04 - accuracy: 1.0000 - val_loss: 0.4926 - val_accuracy: 0.9005\n",
            "Epoch 247/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 8.5064e-04 - accuracy: 0.9997 - val_loss: 0.4974 - val_accuracy: 0.9019\n",
            "Epoch 248/250\n",
            "23/23 [==============================] - 4s 159ms/step - loss: 6.8163e-04 - accuracy: 1.0000 - val_loss: 0.5016 - val_accuracy: 0.9046\n",
            "Epoch 249/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 5.8466e-04 - accuracy: 1.0000 - val_loss: 0.4971 - val_accuracy: 0.9033\n",
            "Epoch 250/250\n",
            "23/23 [==============================] - 4s 160ms/step - loss: 5.6849e-04 - accuracy: 0.9999 - val_loss: 0.4892 - val_accuracy: 0.9101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "871mR8dTsFaJ",
        "outputId": "e59350ca-3cc4-47bb-f26b-f9e70cf90743"
      },
      "source": [
        "# flatten\n",
        "\n",
        "# Create the base model \n",
        "base_model = tf.keras.applications.InceptionV3(input_shape=(160,160,3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.summary()\n",
        "\n",
        "# process data\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
        "])\n",
        "\n",
        "# flattening\n",
        "flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "# final layer\n",
        "prediction_layer = tf.keras.layers.Dense(5)\n",
        "\n",
        "# construct a new network\n",
        "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = base_model(x)\n",
        "x = flatten(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "print(len(base_model.trainable_variables))\n",
        "print(len(model.trainable_variables))\n",
        "\n",
        "base_learning_rate = 0.0001\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate/10),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(train_dataset,\n",
        "                         epochs=250,\n",
        "                         validation_data=validation_dataset)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(history_fine.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_fine.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(history_fine.history['loss'], label='Training Loss')\n",
        "plt.plot(history_fine.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_188 (Conv2D)             (None, 79, 79, 32)   864         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 79, 79, 32)   96          conv2d_188[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 79, 79, 32)   0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 77, 77, 32)   9216        activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 77, 77, 32)   96          conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 77, 77, 32)   0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 77, 77, 64)   18432       activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_190 (BatchN (None, 77, 77, 64)   192         conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 77, 77, 64)   0           batch_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 38, 38, 64)   0           activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 38, 38, 80)   5120        max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_191 (BatchN (None, 38, 38, 80)   240         conv2d_191[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 38, 38, 80)   0           batch_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 36, 36, 192)  138240      activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 36, 36, 192)  576         conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 36, 36, 192)  0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 17, 17, 192)  0           activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 17, 17, 64)   12288       max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 17, 17, 64)   192         conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 17, 17, 64)   0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 17, 17, 48)   9216        max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 17, 17, 96)   55296       activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 17, 17, 48)   144         conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 17, 17, 96)   288         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 17, 17, 48)   0           batch_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 17, 17, 96)   0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_18 (AveragePo (None, 17, 17, 192)  0           max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 17, 17, 64)   12288       max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 17, 17, 64)   76800       activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 17, 17, 96)   82944       activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 17, 17, 32)   6144        average_pooling2d_18[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 17, 17, 64)   192         conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 17, 17, 64)   192         conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 17, 17, 96)   288         conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 17, 17, 32)   96          conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 17, 17, 64)   0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 17, 17, 64)   0           batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 17, 17, 96)   0           batch_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 17, 17, 32)   0           batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 17, 17, 256)  0           activation_193[0][0]             \n",
            "                                                                 activation_195[0][0]             \n",
            "                                                                 activation_198[0][0]             \n",
            "                                                                 activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 17, 17, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 17, 17, 64)   192         conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 17, 17, 64)   0           batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 17, 17, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 17, 17, 96)   55296       activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 17, 17, 48)   144         conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 17, 17, 96)   288         conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 17, 17, 48)   0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 17, 17, 96)   0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_19 (AveragePo (None, 17, 17, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 17, 17, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 17, 17, 64)   76800       activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 17, 17, 96)   82944       activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 17, 17, 64)   16384       average_pooling2d_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 17, 17, 64)   192         conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 17, 17, 64)   192         conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 17, 17, 96)   288         conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 17, 17, 64)   192         conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 17, 17, 64)   0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 17, 17, 64)   0           batch_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 17, 17, 96)   0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 17, 17, 64)   0           batch_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 17, 17, 288)  0           activation_200[0][0]             \n",
            "                                                                 activation_202[0][0]             \n",
            "                                                                 activation_205[0][0]             \n",
            "                                                                 activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 17, 17, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 17, 17, 64)   192         conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 17, 17, 64)   0           batch_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 17, 17, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 17, 17, 96)   55296       activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 17, 17, 48)   144         conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 17, 17, 96)   288         conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 17, 17, 48)   0           batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 17, 17, 96)   0           batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_20 (AveragePo (None, 17, 17, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 17, 17, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 17, 17, 64)   76800       activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 17, 17, 96)   82944       activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 17, 17, 64)   18432       average_pooling2d_20[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 17, 17, 64)   192         conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 17, 17, 64)   192         conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 17, 17, 96)   288         conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 17, 17, 64)   192         conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 17, 17, 64)   0           batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 17, 17, 64)   0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 17, 17, 96)   0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 17, 17, 64)   0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 17, 17, 288)  0           activation_207[0][0]             \n",
            "                                                                 activation_209[0][0]             \n",
            "                                                                 activation_212[0][0]             \n",
            "                                                                 activation_213[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 17, 17, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 17, 17, 64)   192         conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 17, 17, 64)   0           batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 17, 17, 96)   55296       activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 17, 17, 96)   288         conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 17, 17, 96)   0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 8, 8, 384)    995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 8, 8, 96)     82944       activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 8, 8, 384)    1152        conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 8, 8, 96)     288         conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 8, 8, 384)    0           batch_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 8, 8, 96)     0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 8, 8, 288)    0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 8, 8, 768)    0           activation_214[0][0]             \n",
            "                                                                 activation_217[0][0]             \n",
            "                                                                 max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 8, 8, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 8, 8, 128)    384         conv2d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 8, 8, 128)    0           batch_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 8, 8, 128)    114688      activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 8, 8, 128)    384         conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 8, 8, 128)    0           batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 8, 8, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 8, 8, 128)    114688      activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 8, 8, 128)    384         conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 8, 8, 128)    384         conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 8, 8, 128)    0           batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 8, 8, 128)    0           batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 8, 8, 128)    114688      activation_219[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 8, 8, 128)    114688      activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 8, 8, 128)    384         conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 8, 8, 128)    384         conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 8, 8, 128)    0           batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 8, 8, 128)    0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_21 (AveragePo (None, 8, 8, 768)    0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 8, 8, 192)    147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 8, 8, 192)    172032      activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 8, 8, 192)    172032      activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 8, 8, 192)    147456      average_pooling2d_21[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 8, 8, 192)    576         conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 8, 8, 192)    576         conv2d_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 8, 8, 192)    576         conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 8, 8, 192)    576         conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 8, 8, 192)    0           batch_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 8, 8, 192)    0           batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 8, 8, 192)    0           batch_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 8, 8, 192)    0           batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 8, 8, 768)    0           activation_218[0][0]             \n",
            "                                                                 activation_221[0][0]             \n",
            "                                                                 activation_226[0][0]             \n",
            "                                                                 activation_227[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 8, 8, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 8, 8, 160)    480         conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 8, 8, 160)    0           batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 8, 8, 160)    179200      activation_232[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 8, 8, 160)    480         conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 8, 8, 160)    0           batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 8, 8, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 8, 8, 160)    179200      activation_233[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 8, 8, 160)    480         conv2d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 8, 8, 160)    480         conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 8, 8, 160)    0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_234 (Activation)     (None, 8, 8, 160)    0           batch_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 8, 8, 160)    179200      activation_229[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 8, 8, 160)    179200      activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 8, 8, 160)    480         conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 8, 8, 160)    480         conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 8, 8, 160)    0           batch_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_235 (Activation)     (None, 8, 8, 160)    0           batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_22 (AveragePo (None, 8, 8, 768)    0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 8, 8, 192)    147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 8, 8, 192)    215040      activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 8, 8, 192)    215040      activation_235[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 8, 8, 192)    147456      average_pooling2d_22[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 8, 8, 192)    576         conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 8, 8, 192)    576         conv2d_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 8, 8, 192)    576         conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 8, 8, 192)    576         conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 8, 8, 192)    0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 8, 8, 192)    0           batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_236 (Activation)     (None, 8, 8, 192)    0           batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_237 (Activation)     (None, 8, 8, 192)    0           batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 8, 8, 768)    0           activation_228[0][0]             \n",
            "                                                                 activation_231[0][0]             \n",
            "                                                                 activation_236[0][0]             \n",
            "                                                                 activation_237[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 8, 8, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_242 (BatchN (None, 8, 8, 160)    480         conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_242 (Activation)     (None, 8, 8, 160)    0           batch_normalization_242[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_243 (Conv2D)             (None, 8, 8, 160)    179200      activation_242[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_243 (BatchN (None, 8, 8, 160)    480         conv2d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_243 (Activation)     (None, 8, 8, 160)    0           batch_normalization_243[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 8, 8, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_244 (Conv2D)             (None, 8, 8, 160)    179200      activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_239 (BatchN (None, 8, 8, 160)    480         conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_244 (BatchN (None, 8, 8, 160)    480         conv2d_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_239 (Activation)     (None, 8, 8, 160)    0           batch_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_244 (Activation)     (None, 8, 8, 160)    0           batch_normalization_244[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 8, 8, 160)    179200      activation_239[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 8, 8, 160)    179200      activation_244[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_240 (BatchN (None, 8, 8, 160)    480         conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_245 (BatchN (None, 8, 8, 160)    480         conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_240 (Activation)     (None, 8, 8, 160)    0           batch_normalization_240[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_245 (Activation)     (None, 8, 8, 160)    0           batch_normalization_245[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_23 (AveragePo (None, 8, 8, 768)    0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 8, 8, 192)    147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 8, 8, 192)    215040      activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 8, 8, 192)    215040      activation_245[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 8, 8, 192)    147456      average_pooling2d_23[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 8, 8, 192)    576         conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_241 (BatchN (None, 8, 8, 192)    576         conv2d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_246 (BatchN (None, 8, 8, 192)    576         conv2d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_247 (BatchN (None, 8, 8, 192)    576         conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_238 (Activation)     (None, 8, 8, 192)    0           batch_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_241 (Activation)     (None, 8, 8, 192)    0           batch_normalization_241[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_246 (Activation)     (None, 8, 8, 192)    0           batch_normalization_246[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_247 (Activation)     (None, 8, 8, 192)    0           batch_normalization_247[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 8, 8, 768)    0           activation_238[0][0]             \n",
            "                                                                 activation_241[0][0]             \n",
            "                                                                 activation_246[0][0]             \n",
            "                                                                 activation_247[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_252 (BatchN (None, 8, 8, 192)    576         conv2d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_252 (Activation)     (None, 8, 8, 192)    0           batch_normalization_252[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 8, 8, 192)    258048      activation_252[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_253 (BatchN (None, 8, 8, 192)    576         conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_253 (Activation)     (None, 8, 8, 192)    0           batch_normalization_253[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 8, 8, 192)    258048      activation_253[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_249 (BatchN (None, 8, 8, 192)    576         conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_254 (BatchN (None, 8, 8, 192)    576         conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_249 (Activation)     (None, 8, 8, 192)    0           batch_normalization_249[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_254 (Activation)     (None, 8, 8, 192)    0           batch_normalization_254[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 8, 8, 192)    258048      activation_249[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_255 (Conv2D)             (None, 8, 8, 192)    258048      activation_254[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_250 (BatchN (None, 8, 8, 192)    576         conv2d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_255 (BatchN (None, 8, 8, 192)    576         conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_250 (Activation)     (None, 8, 8, 192)    0           batch_normalization_250[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_255 (Activation)     (None, 8, 8, 192)    0           batch_normalization_255[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_24 (AveragePo (None, 8, 8, 768)    0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 8, 8, 192)    258048      activation_250[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_256 (Conv2D)             (None, 8, 8, 192)    258048      activation_255[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_257 (Conv2D)             (None, 8, 8, 192)    147456      average_pooling2d_24[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_248 (BatchN (None, 8, 8, 192)    576         conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_251 (BatchN (None, 8, 8, 192)    576         conv2d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_256 (BatchN (None, 8, 8, 192)    576         conv2d_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_257 (BatchN (None, 8, 8, 192)    576         conv2d_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_248 (Activation)     (None, 8, 8, 192)    0           batch_normalization_248[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_251 (Activation)     (None, 8, 8, 192)    0           batch_normalization_251[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_256 (Activation)     (None, 8, 8, 192)    0           batch_normalization_256[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_257 (Activation)     (None, 8, 8, 192)    0           batch_normalization_257[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 8, 8, 768)    0           activation_248[0][0]             \n",
            "                                                                 activation_251[0][0]             \n",
            "                                                                 activation_256[0][0]             \n",
            "                                                                 activation_257[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_260 (Conv2D)             (None, 8, 8, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_260 (BatchN (None, 8, 8, 192)    576         conv2d_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_260 (Activation)     (None, 8, 8, 192)    0           batch_normalization_260[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_261 (Conv2D)             (None, 8, 8, 192)    258048      activation_260[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_261 (BatchN (None, 8, 8, 192)    576         conv2d_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_261 (Activation)     (None, 8, 8, 192)    0           batch_normalization_261[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_258 (Conv2D)             (None, 8, 8, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_262 (Conv2D)             (None, 8, 8, 192)    258048      activation_261[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_258 (BatchN (None, 8, 8, 192)    576         conv2d_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_262 (BatchN (None, 8, 8, 192)    576         conv2d_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_258 (Activation)     (None, 8, 8, 192)    0           batch_normalization_258[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_262 (Activation)     (None, 8, 8, 192)    0           batch_normalization_262[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_259 (Conv2D)             (None, 3, 3, 320)    552960      activation_258[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_263 (Conv2D)             (None, 3, 3, 192)    331776      activation_262[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_259 (BatchN (None, 3, 3, 320)    960         conv2d_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_263 (BatchN (None, 3, 3, 192)    576         conv2d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_259 (Activation)     (None, 3, 3, 320)    0           batch_normalization_259[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_263 (Activation)     (None, 3, 3, 192)    0           batch_normalization_263[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_259[0][0]             \n",
            "                                                                 activation_263[0][0]             \n",
            "                                                                 max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_268 (Conv2D)             (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_268 (BatchN (None, 3, 3, 448)    1344        conv2d_268[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_268 (Activation)     (None, 3, 3, 448)    0           batch_normalization_268[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_265 (Conv2D)             (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_269 (Conv2D)             (None, 3, 3, 384)    1548288     activation_268[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_265 (BatchN (None, 3, 3, 384)    1152        conv2d_265[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_269 (BatchN (None, 3, 3, 384)    1152        conv2d_269[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_265 (Activation)     (None, 3, 3, 384)    0           batch_normalization_265[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_269 (Activation)     (None, 3, 3, 384)    0           batch_normalization_269[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_266 (Conv2D)             (None, 3, 3, 384)    442368      activation_265[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_267 (Conv2D)             (None, 3, 3, 384)    442368      activation_265[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_270 (Conv2D)             (None, 3, 3, 384)    442368      activation_269[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_271 (Conv2D)             (None, 3, 3, 384)    442368      activation_269[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_25 (AveragePo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_264 (Conv2D)             (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_266 (BatchN (None, 3, 3, 384)    1152        conv2d_266[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_267 (BatchN (None, 3, 3, 384)    1152        conv2d_267[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_270 (BatchN (None, 3, 3, 384)    1152        conv2d_270[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_271 (BatchN (None, 3, 3, 384)    1152        conv2d_271[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_272 (Conv2D)             (None, 3, 3, 192)    245760      average_pooling2d_25[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_264 (BatchN (None, 3, 3, 320)    960         conv2d_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_266 (Activation)     (None, 3, 3, 384)    0           batch_normalization_266[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_267 (Activation)     (None, 3, 3, 384)    0           batch_normalization_267[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_270 (Activation)     (None, 3, 3, 384)    0           batch_normalization_270[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_271 (Activation)     (None, 3, 3, 384)    0           batch_normalization_271[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_272 (BatchN (None, 3, 3, 192)    576         conv2d_272[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_264 (Activation)     (None, 3, 3, 320)    0           batch_normalization_264[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_266[0][0]             \n",
            "                                                                 activation_267[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 3, 3, 768)    0           activation_270[0][0]             \n",
            "                                                                 activation_271[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_272 (Activation)     (None, 3, 3, 192)    0           batch_normalization_272[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_264[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_4[0][0]              \n",
            "                                                                 activation_272[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_277 (Conv2D)             (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_277 (BatchN (None, 3, 3, 448)    1344        conv2d_277[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_277 (Activation)     (None, 3, 3, 448)    0           batch_normalization_277[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_274 (Conv2D)             (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_278 (Conv2D)             (None, 3, 3, 384)    1548288     activation_277[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_274 (BatchN (None, 3, 3, 384)    1152        conv2d_274[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_278 (BatchN (None, 3, 3, 384)    1152        conv2d_278[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_274 (Activation)     (None, 3, 3, 384)    0           batch_normalization_274[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_278 (Activation)     (None, 3, 3, 384)    0           batch_normalization_278[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_275 (Conv2D)             (None, 3, 3, 384)    442368      activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_276 (Conv2D)             (None, 3, 3, 384)    442368      activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_279 (Conv2D)             (None, 3, 3, 384)    442368      activation_278[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_280 (Conv2D)             (None, 3, 3, 384)    442368      activation_278[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_26 (AveragePo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_273 (Conv2D)             (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_275 (BatchN (None, 3, 3, 384)    1152        conv2d_275[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_276 (BatchN (None, 3, 3, 384)    1152        conv2d_276[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_279 (BatchN (None, 3, 3, 384)    1152        conv2d_279[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_280 (BatchN (None, 3, 3, 384)    1152        conv2d_280[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_281 (Conv2D)             (None, 3, 3, 192)    393216      average_pooling2d_26[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_273 (BatchN (None, 3, 3, 320)    960         conv2d_273[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_275 (Activation)     (None, 3, 3, 384)    0           batch_normalization_275[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_276 (Activation)     (None, 3, 3, 384)    0           batch_normalization_276[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_279 (Activation)     (None, 3, 3, 384)    0           batch_normalization_279[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_280 (Activation)     (None, 3, 3, 384)    0           batch_normalization_280[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_281 (BatchN (None, 3, 3, 192)    576         conv2d_281[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_273 (Activation)     (None, 3, 3, 320)    0           batch_normalization_273[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_275[0][0]             \n",
            "                                                                 activation_276[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 3, 3, 768)    0           activation_279[0][0]             \n",
            "                                                                 activation_280[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_281 (Activation)     (None, 3, 3, 192)    0           batch_normalization_281[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_273[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_5[0][0]              \n",
            "                                                                 activation_281[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 21,768,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n",
            "188\n",
            "190\n",
            "Epoch 1/250\n",
            "23/23 [==============================] - 14s 339ms/step - loss: 1.8300 - accuracy: 0.2381 - val_loss: 1.8944 - val_accuracy: 0.3529\n",
            "Epoch 2/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 1.4412 - accuracy: 0.4000 - val_loss: 1.3019 - val_accuracy: 0.5163\n",
            "Epoch 3/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 1.2110 - accuracy: 0.5278 - val_loss: 1.0562 - val_accuracy: 0.6076\n",
            "Epoch 4/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 1.0669 - accuracy: 0.5951 - val_loss: 0.8771 - val_accuracy: 0.6826\n",
            "Epoch 5/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.9305 - accuracy: 0.6576 - val_loss: 0.7656 - val_accuracy: 0.7221\n",
            "Epoch 6/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.8314 - accuracy: 0.7031 - val_loss: 0.6881 - val_accuracy: 0.7507\n",
            "Epoch 7/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.7541 - accuracy: 0.7297 - val_loss: 0.6440 - val_accuracy: 0.7725\n",
            "Epoch 8/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.7168 - accuracy: 0.7485 - val_loss: 0.6014 - val_accuracy: 0.7875\n",
            "Epoch 9/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.6319 - accuracy: 0.7807 - val_loss: 0.5707 - val_accuracy: 0.7902\n",
            "Epoch 10/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.6094 - accuracy: 0.7795 - val_loss: 0.5474 - val_accuracy: 0.7997\n",
            "Epoch 11/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.5640 - accuracy: 0.8068 - val_loss: 0.5268 - val_accuracy: 0.8120\n",
            "Epoch 12/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.5129 - accuracy: 0.8208 - val_loss: 0.5081 - val_accuracy: 0.8174\n",
            "Epoch 13/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.4851 - accuracy: 0.8331 - val_loss: 0.4937 - val_accuracy: 0.8229\n",
            "Epoch 14/250\n",
            "23/23 [==============================] - 6s 266ms/step - loss: 0.4594 - accuracy: 0.8349 - val_loss: 0.4803 - val_accuracy: 0.8270\n",
            "Epoch 15/250\n",
            "23/23 [==============================] - 6s 266ms/step - loss: 0.4208 - accuracy: 0.8574 - val_loss: 0.4656 - val_accuracy: 0.8365\n",
            "Epoch 16/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.4130 - accuracy: 0.8581 - val_loss: 0.4550 - val_accuracy: 0.8379\n",
            "Epoch 17/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.3879 - accuracy: 0.8712 - val_loss: 0.4486 - val_accuracy: 0.8433\n",
            "Epoch 18/250\n",
            "23/23 [==============================] - 6s 266ms/step - loss: 0.3525 - accuracy: 0.8861 - val_loss: 0.4374 - val_accuracy: 0.8460\n",
            "Epoch 19/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.3496 - accuracy: 0.8874 - val_loss: 0.4269 - val_accuracy: 0.8515\n",
            "Epoch 20/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.3371 - accuracy: 0.8771 - val_loss: 0.4182 - val_accuracy: 0.8569\n",
            "Epoch 21/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.2882 - accuracy: 0.9046 - val_loss: 0.4147 - val_accuracy: 0.8569\n",
            "Epoch 22/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.3175 - accuracy: 0.8857 - val_loss: 0.4113 - val_accuracy: 0.8638\n",
            "Epoch 23/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.2665 - accuracy: 0.9196 - val_loss: 0.3992 - val_accuracy: 0.8638\n",
            "Epoch 24/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.2867 - accuracy: 0.9075 - val_loss: 0.3902 - val_accuracy: 0.8678\n",
            "Epoch 25/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.2496 - accuracy: 0.9205 - val_loss: 0.3854 - val_accuracy: 0.8692\n",
            "Epoch 26/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 0.2606 - accuracy: 0.9144 - val_loss: 0.3812 - val_accuracy: 0.8719\n",
            "Epoch 27/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.2246 - accuracy: 0.9351 - val_loss: 0.3773 - val_accuracy: 0.8692\n",
            "Epoch 28/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.2240 - accuracy: 0.9237 - val_loss: 0.3739 - val_accuracy: 0.8692\n",
            "Epoch 29/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.2062 - accuracy: 0.9304 - val_loss: 0.3726 - val_accuracy: 0.8719\n",
            "Epoch 30/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.2202 - accuracy: 0.9270 - val_loss: 0.3746 - val_accuracy: 0.8706\n",
            "Epoch 31/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.1781 - accuracy: 0.9430 - val_loss: 0.3717 - val_accuracy: 0.8719\n",
            "Epoch 32/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.1752 - accuracy: 0.9479 - val_loss: 0.3687 - val_accuracy: 0.8719\n",
            "Epoch 33/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.1621 - accuracy: 0.9523 - val_loss: 0.3672 - val_accuracy: 0.8719\n",
            "Epoch 34/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.1669 - accuracy: 0.9469 - val_loss: 0.3652 - val_accuracy: 0.8733\n",
            "Epoch 35/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.1717 - accuracy: 0.9457 - val_loss: 0.3626 - val_accuracy: 0.8760\n",
            "Epoch 36/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.1639 - accuracy: 0.9519 - val_loss: 0.3586 - val_accuracy: 0.8787\n",
            "Epoch 37/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.1339 - accuracy: 0.9613 - val_loss: 0.3547 - val_accuracy: 0.8842\n",
            "Epoch 38/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 0.1261 - accuracy: 0.9700 - val_loss: 0.3610 - val_accuracy: 0.8787\n",
            "Epoch 39/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.1332 - accuracy: 0.9664 - val_loss: 0.3621 - val_accuracy: 0.8787\n",
            "Epoch 40/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.1255 - accuracy: 0.9697 - val_loss: 0.3604 - val_accuracy: 0.8801\n",
            "Epoch 41/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.1208 - accuracy: 0.9653 - val_loss: 0.3563 - val_accuracy: 0.8842\n",
            "Epoch 42/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.1070 - accuracy: 0.9696 - val_loss: 0.3527 - val_accuracy: 0.8828\n",
            "Epoch 43/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0923 - accuracy: 0.9746 - val_loss: 0.3484 - val_accuracy: 0.8869\n",
            "Epoch 44/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0968 - accuracy: 0.9728 - val_loss: 0.3490 - val_accuracy: 0.8869\n",
            "Epoch 45/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0854 - accuracy: 0.9826 - val_loss: 0.3516 - val_accuracy: 0.8856\n",
            "Epoch 46/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0873 - accuracy: 0.9786 - val_loss: 0.3544 - val_accuracy: 0.8842\n",
            "Epoch 47/250\n",
            "23/23 [==============================] - 6s 266ms/step - loss: 0.0746 - accuracy: 0.9823 - val_loss: 0.3497 - val_accuracy: 0.8883\n",
            "Epoch 48/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0819 - accuracy: 0.9769 - val_loss: 0.3494 - val_accuracy: 0.8883\n",
            "Epoch 49/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0697 - accuracy: 0.9828 - val_loss: 0.3504 - val_accuracy: 0.8869\n",
            "Epoch 50/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0779 - accuracy: 0.9826 - val_loss: 0.3516 - val_accuracy: 0.8883\n",
            "Epoch 51/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0684 - accuracy: 0.9815 - val_loss: 0.3537 - val_accuracy: 0.8883\n",
            "Epoch 52/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0627 - accuracy: 0.9827 - val_loss: 0.3548 - val_accuracy: 0.8856\n",
            "Epoch 53/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0624 - accuracy: 0.9858 - val_loss: 0.3472 - val_accuracy: 0.8896\n",
            "Epoch 54/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0565 - accuracy: 0.9866 - val_loss: 0.3456 - val_accuracy: 0.8896\n",
            "Epoch 55/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0609 - accuracy: 0.9862 - val_loss: 0.3419 - val_accuracy: 0.8951\n",
            "Epoch 56/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0517 - accuracy: 0.9882 - val_loss: 0.3424 - val_accuracy: 0.9005\n",
            "Epoch 57/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0531 - accuracy: 0.9830 - val_loss: 0.3469 - val_accuracy: 0.8965\n",
            "Epoch 58/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0505 - accuracy: 0.9864 - val_loss: 0.3475 - val_accuracy: 0.8924\n",
            "Epoch 59/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0417 - accuracy: 0.9886 - val_loss: 0.3455 - val_accuracy: 0.8978\n",
            "Epoch 60/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 0.0407 - accuracy: 0.9950 - val_loss: 0.3477 - val_accuracy: 0.8965\n",
            "Epoch 61/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0387 - accuracy: 0.9911 - val_loss: 0.3466 - val_accuracy: 0.8910\n",
            "Epoch 62/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0348 - accuracy: 0.9947 - val_loss: 0.3478 - val_accuracy: 0.8896\n",
            "Epoch 63/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0366 - accuracy: 0.9923 - val_loss: 0.3493 - val_accuracy: 0.8883\n",
            "Epoch 64/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0357 - accuracy: 0.9920 - val_loss: 0.3477 - val_accuracy: 0.8883\n",
            "Epoch 65/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0337 - accuracy: 0.9885 - val_loss: 0.3479 - val_accuracy: 0.8910\n",
            "Epoch 66/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0277 - accuracy: 0.9948 - val_loss: 0.3482 - val_accuracy: 0.8951\n",
            "Epoch 67/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0345 - accuracy: 0.9914 - val_loss: 0.3531 - val_accuracy: 0.8951\n",
            "Epoch 68/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0304 - accuracy: 0.9955 - val_loss: 0.3551 - val_accuracy: 0.8910\n",
            "Epoch 69/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0344 - accuracy: 0.9926 - val_loss: 0.3560 - val_accuracy: 0.8924\n",
            "Epoch 70/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0268 - accuracy: 0.9954 - val_loss: 0.3518 - val_accuracy: 0.8924\n",
            "Epoch 71/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0276 - accuracy: 0.9943 - val_loss: 0.3497 - val_accuracy: 0.8924\n",
            "Epoch 72/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0238 - accuracy: 0.9973 - val_loss: 0.3502 - val_accuracy: 0.8937\n",
            "Epoch 73/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0232 - accuracy: 0.9962 - val_loss: 0.3542 - val_accuracy: 0.8924\n",
            "Epoch 74/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0203 - accuracy: 0.9975 - val_loss: 0.3567 - val_accuracy: 0.8910\n",
            "Epoch 75/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0209 - accuracy: 0.9963 - val_loss: 0.3539 - val_accuracy: 0.8937\n",
            "Epoch 76/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0207 - accuracy: 0.9979 - val_loss: 0.3537 - val_accuracy: 0.8992\n",
            "Epoch 77/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0208 - accuracy: 0.9969 - val_loss: 0.3571 - val_accuracy: 0.8978\n",
            "Epoch 78/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 0.0199 - accuracy: 0.9950 - val_loss: 0.3577 - val_accuracy: 0.8965\n",
            "Epoch 79/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0217 - accuracy: 0.9952 - val_loss: 0.3581 - val_accuracy: 0.8978\n",
            "Epoch 80/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 0.0215 - accuracy: 0.9959 - val_loss: 0.3611 - val_accuracy: 0.8992\n",
            "Epoch 81/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0155 - accuracy: 0.9980 - val_loss: 0.3612 - val_accuracy: 0.8978\n",
            "Epoch 82/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.3605 - val_accuracy: 0.8978\n",
            "Epoch 83/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0126 - accuracy: 0.9987 - val_loss: 0.3623 - val_accuracy: 0.8978\n",
            "Epoch 84/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0150 - accuracy: 0.9967 - val_loss: 0.3607 - val_accuracy: 0.9019\n",
            "Epoch 85/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0126 - accuracy: 0.9993 - val_loss: 0.3622 - val_accuracy: 0.8992\n",
            "Epoch 86/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 0.0124 - accuracy: 0.9976 - val_loss: 0.3654 - val_accuracy: 0.8978\n",
            "Epoch 87/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0100 - accuracy: 0.9992 - val_loss: 0.3681 - val_accuracy: 0.8965\n",
            "Epoch 88/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0124 - accuracy: 0.9978 - val_loss: 0.3657 - val_accuracy: 0.8992\n",
            "Epoch 89/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0135 - accuracy: 0.9975 - val_loss: 0.3669 - val_accuracy: 0.8978\n",
            "Epoch 90/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0119 - accuracy: 0.9976 - val_loss: 0.3654 - val_accuracy: 0.8978\n",
            "Epoch 91/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0130 - accuracy: 0.9980 - val_loss: 0.3668 - val_accuracy: 0.9005\n",
            "Epoch 92/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0089 - accuracy: 0.9992 - val_loss: 0.3686 - val_accuracy: 0.8992\n",
            "Epoch 93/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.3712 - val_accuracy: 0.8978\n",
            "Epoch 94/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0078 - accuracy: 0.9999 - val_loss: 0.3735 - val_accuracy: 0.9005\n",
            "Epoch 95/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 0.0153 - accuracy: 0.9956 - val_loss: 0.3713 - val_accuracy: 0.8951\n",
            "Epoch 96/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0095 - accuracy: 0.9995 - val_loss: 0.3735 - val_accuracy: 0.8965\n",
            "Epoch 97/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0094 - accuracy: 0.9986 - val_loss: 0.3716 - val_accuracy: 0.8965\n",
            "Epoch 98/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0093 - accuracy: 0.9991 - val_loss: 0.3731 - val_accuracy: 0.8965\n",
            "Epoch 99/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 0.3729 - val_accuracy: 0.8951\n",
            "Epoch 100/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0076 - accuracy: 0.9996 - val_loss: 0.3756 - val_accuracy: 0.8951\n",
            "Epoch 101/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0084 - accuracy: 0.9992 - val_loss: 0.3761 - val_accuracy: 0.8951\n",
            "Epoch 102/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.3765 - val_accuracy: 0.8951\n",
            "Epoch 103/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0084 - accuracy: 0.9994 - val_loss: 0.3757 - val_accuracy: 0.9019\n",
            "Epoch 104/250\n",
            "23/23 [==============================] - 6s 266ms/step - loss: 0.0081 - accuracy: 0.9993 - val_loss: 0.3766 - val_accuracy: 0.9033\n",
            "Epoch 105/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0078 - accuracy: 0.9983 - val_loss: 0.3761 - val_accuracy: 0.9033\n",
            "Epoch 106/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0072 - accuracy: 0.9992 - val_loss: 0.3773 - val_accuracy: 0.9060\n",
            "Epoch 107/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0100 - accuracy: 0.9976 - val_loss: 0.3788 - val_accuracy: 0.9033\n",
            "Epoch 108/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0053 - accuracy: 0.9999 - val_loss: 0.3780 - val_accuracy: 0.9019\n",
            "Epoch 109/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.3770 - val_accuracy: 0.9060\n",
            "Epoch 110/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0060 - accuracy: 0.9995 - val_loss: 0.3767 - val_accuracy: 0.9074\n",
            "Epoch 111/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 0.3799 - val_accuracy: 0.9060\n",
            "Epoch 112/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0058 - accuracy: 0.9991 - val_loss: 0.3778 - val_accuracy: 0.9060\n",
            "Epoch 113/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.3764 - val_accuracy: 0.9060\n",
            "Epoch 114/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.3789 - val_accuracy: 0.9060\n",
            "Epoch 115/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0061 - accuracy: 0.9993 - val_loss: 0.3869 - val_accuracy: 0.9046\n",
            "Epoch 116/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.3904 - val_accuracy: 0.9033\n",
            "Epoch 117/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0048 - accuracy: 0.9994 - val_loss: 0.3913 - val_accuracy: 0.9060\n",
            "Epoch 118/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0048 - accuracy: 0.9997 - val_loss: 0.3910 - val_accuracy: 0.9074\n",
            "Epoch 119/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 0.3888 - val_accuracy: 0.9087\n",
            "Epoch 120/250\n",
            "23/23 [==============================] - 6s 266ms/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 0.3832 - val_accuracy: 0.9087\n",
            "Epoch 121/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0042 - accuracy: 0.9998 - val_loss: 0.3844 - val_accuracy: 0.9087\n",
            "Epoch 122/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0047 - accuracy: 0.9996 - val_loss: 0.3895 - val_accuracy: 0.9005\n",
            "Epoch 123/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0043 - accuracy: 0.9998 - val_loss: 0.3886 - val_accuracy: 0.9033\n",
            "Epoch 124/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3885 - val_accuracy: 0.9060\n",
            "Epoch 125/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0034 - accuracy: 0.9998 - val_loss: 0.3956 - val_accuracy: 0.8978\n",
            "Epoch 126/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.3971 - val_accuracy: 0.9033\n",
            "Epoch 127/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3969 - val_accuracy: 0.9033\n",
            "Epoch 128/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0047 - accuracy: 0.9993 - val_loss: 0.3983 - val_accuracy: 0.9060\n",
            "Epoch 129/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 0.0038 - accuracy: 0.9997 - val_loss: 0.4033 - val_accuracy: 0.9019\n",
            "Epoch 130/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4056 - val_accuracy: 0.9033\n",
            "Epoch 131/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.4054 - val_accuracy: 0.9033\n",
            "Epoch 132/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4057 - val_accuracy: 0.9019\n",
            "Epoch 133/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.4067 - val_accuracy: 0.9019\n",
            "Epoch 134/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4066 - val_accuracy: 0.9033\n",
            "Epoch 135/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.4075 - val_accuracy: 0.9005\n",
            "Epoch 136/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 0.4077 - val_accuracy: 0.9019\n",
            "Epoch 137/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.4088 - val_accuracy: 0.8992\n",
            "Epoch 138/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4112 - val_accuracy: 0.8992\n",
            "Epoch 139/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4164 - val_accuracy: 0.8978\n",
            "Epoch 140/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.4144 - val_accuracy: 0.8978\n",
            "Epoch 141/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4126 - val_accuracy: 0.8992\n",
            "Epoch 142/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.4155 - val_accuracy: 0.8978\n",
            "Epoch 143/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.4150 - val_accuracy: 0.8992\n",
            "Epoch 144/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.4119 - val_accuracy: 0.9033\n",
            "Epoch 145/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4127 - val_accuracy: 0.9033\n",
            "Epoch 146/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4122 - val_accuracy: 0.9019\n",
            "Epoch 147/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4132 - val_accuracy: 0.9005\n",
            "Epoch 148/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4120 - val_accuracy: 0.9019\n",
            "Epoch 149/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4128 - val_accuracy: 0.9019\n",
            "Epoch 150/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0033 - accuracy: 0.9987 - val_loss: 0.4170 - val_accuracy: 0.8992\n",
            "Epoch 151/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4183 - val_accuracy: 0.8992\n",
            "Epoch 152/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4185 - val_accuracy: 0.9019\n",
            "Epoch 153/250\n",
            "23/23 [==============================] - 6s 266ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4189 - val_accuracy: 0.9019\n",
            "Epoch 154/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.4182 - val_accuracy: 0.9005\n",
            "Epoch 155/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4176 - val_accuracy: 0.8965\n",
            "Epoch 156/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.4175 - val_accuracy: 0.8992\n",
            "Epoch 157/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4167 - val_accuracy: 0.9005\n",
            "Epoch 158/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4165 - val_accuracy: 0.9019\n",
            "Epoch 159/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.4206 - val_accuracy: 0.9019\n",
            "Epoch 160/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4218 - val_accuracy: 0.9019\n",
            "Epoch 161/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.4273 - val_accuracy: 0.8978\n",
            "Epoch 162/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4272 - val_accuracy: 0.8992\n",
            "Epoch 163/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4242 - val_accuracy: 0.9005\n",
            "Epoch 164/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.4250 - val_accuracy: 0.9033\n",
            "Epoch 165/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.4306 - val_accuracy: 0.9005\n",
            "Epoch 166/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4284 - val_accuracy: 0.9005\n",
            "Epoch 167/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4266 - val_accuracy: 0.9046\n",
            "Epoch 168/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4223 - val_accuracy: 0.9046\n",
            "Epoch 169/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4188 - val_accuracy: 0.9060\n",
            "Epoch 170/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4199 - val_accuracy: 0.9033\n",
            "Epoch 171/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.9033\n",
            "Epoch 172/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4245 - val_accuracy: 0.9033\n",
            "Epoch 173/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4228 - val_accuracy: 0.9046\n",
            "Epoch 174/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4168 - val_accuracy: 0.9074\n",
            "Epoch 175/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4199 - val_accuracy: 0.9074\n",
            "Epoch 176/250\n",
            "23/23 [==============================] - 6s 266ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4248 - val_accuracy: 0.9060\n",
            "Epoch 177/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 8.3501e-04 - accuracy: 1.0000 - val_loss: 0.4247 - val_accuracy: 0.9087\n",
            "Epoch 178/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4248 - val_accuracy: 0.9060\n",
            "Epoch 179/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 9.2577e-04 - accuracy: 1.0000 - val_loss: 0.4246 - val_accuracy: 0.9060\n",
            "Epoch 180/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4228 - val_accuracy: 0.9046\n",
            "Epoch 181/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4272 - val_accuracy: 0.9046\n",
            "Epoch 182/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 9.1278e-04 - accuracy: 0.9999 - val_loss: 0.4270 - val_accuracy: 0.9074\n",
            "Epoch 183/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4275 - val_accuracy: 0.9060\n",
            "Epoch 184/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4290 - val_accuracy: 0.9060\n",
            "Epoch 185/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 9.7137e-04 - accuracy: 1.0000 - val_loss: 0.4304 - val_accuracy: 0.9087\n",
            "Epoch 186/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 9.5123e-04 - accuracy: 1.0000 - val_loss: 0.4296 - val_accuracy: 0.9033\n",
            "Epoch 187/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 9.4578e-04 - accuracy: 1.0000 - val_loss: 0.4321 - val_accuracy: 0.9060\n",
            "Epoch 188/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.4344 - val_accuracy: 0.9074\n",
            "Epoch 189/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 8.4134e-04 - accuracy: 1.0000 - val_loss: 0.4357 - val_accuracy: 0.9033\n",
            "Epoch 190/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 8.3256e-04 - accuracy: 1.0000 - val_loss: 0.4344 - val_accuracy: 0.9046\n",
            "Epoch 191/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4314 - val_accuracy: 0.9060\n",
            "Epoch 192/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 9.4192e-04 - accuracy: 1.0000 - val_loss: 0.4315 - val_accuracy: 0.9060\n",
            "Epoch 193/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 8.9449e-04 - accuracy: 1.0000 - val_loss: 0.4345 - val_accuracy: 0.9046\n",
            "Epoch 194/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.4351 - val_accuracy: 0.9005\n",
            "Epoch 195/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 6.6812e-04 - accuracy: 1.0000 - val_loss: 0.4366 - val_accuracy: 0.9005\n",
            "Epoch 196/250\n",
            "23/23 [==============================] - 6s 266ms/step - loss: 9.7139e-04 - accuracy: 1.0000 - val_loss: 0.4442 - val_accuracy: 0.8992\n",
            "Epoch 197/250\n",
            "23/23 [==============================] - 6s 266ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4453 - val_accuracy: 0.8978\n",
            "Epoch 198/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4427 - val_accuracy: 0.8978\n",
            "Epoch 199/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 8.8333e-04 - accuracy: 1.0000 - val_loss: 0.4389 - val_accuracy: 0.8992\n",
            "Epoch 200/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 7.9898e-04 - accuracy: 1.0000 - val_loss: 0.4371 - val_accuracy: 0.9019\n",
            "Epoch 201/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 8.7046e-04 - accuracy: 1.0000 - val_loss: 0.4373 - val_accuracy: 0.9019\n",
            "Epoch 202/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 9.2778e-04 - accuracy: 1.0000 - val_loss: 0.4392 - val_accuracy: 0.9019\n",
            "Epoch 203/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 5.5111e-04 - accuracy: 1.0000 - val_loss: 0.4393 - val_accuracy: 0.9019\n",
            "Epoch 204/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 6.3966e-04 - accuracy: 1.0000 - val_loss: 0.4416 - val_accuracy: 0.9019\n",
            "Epoch 205/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 7.1440e-04 - accuracy: 1.0000 - val_loss: 0.4431 - val_accuracy: 0.9019\n",
            "Epoch 206/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 7.4516e-04 - accuracy: 1.0000 - val_loss: 0.4431 - val_accuracy: 0.9019\n",
            "Epoch 207/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 5.5620e-04 - accuracy: 1.0000 - val_loss: 0.4447 - val_accuracy: 0.9033\n",
            "Epoch 208/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.4533 - val_accuracy: 0.8992\n",
            "Epoch 209/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.4659 - val_accuracy: 0.8978\n",
            "Epoch 210/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4657 - val_accuracy: 0.8951\n",
            "Epoch 211/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 6.2644e-04 - accuracy: 1.0000 - val_loss: 0.4655 - val_accuracy: 0.8937\n",
            "Epoch 212/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 6.5095e-04 - accuracy: 1.0000 - val_loss: 0.4610 - val_accuracy: 0.8965\n",
            "Epoch 213/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 6.0400e-04 - accuracy: 1.0000 - val_loss: 0.4577 - val_accuracy: 0.8965\n",
            "Epoch 214/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4568 - val_accuracy: 0.9005\n",
            "Epoch 215/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 6.5408e-04 - accuracy: 1.0000 - val_loss: 0.4458 - val_accuracy: 0.9019\n",
            "Epoch 216/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 4.7938e-04 - accuracy: 1.0000 - val_loss: 0.4478 - val_accuracy: 0.9005\n",
            "Epoch 217/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 4.0731e-04 - accuracy: 1.0000 - val_loss: 0.4504 - val_accuracy: 0.9005\n",
            "Epoch 218/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 8.4680e-04 - accuracy: 1.0000 - val_loss: 0.4457 - val_accuracy: 0.9005\n",
            "Epoch 219/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.4590 - val_accuracy: 0.8965\n",
            "Epoch 220/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 6.1082e-04 - accuracy: 1.0000 - val_loss: 0.4629 - val_accuracy: 0.8978\n",
            "Epoch 221/250\n",
            "23/23 [==============================] - 6s 266ms/step - loss: 6.5301e-04 - accuracy: 1.0000 - val_loss: 0.4575 - val_accuracy: 0.9033\n",
            "Epoch 222/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 9.1848e-04 - accuracy: 1.0000 - val_loss: 0.4536 - val_accuracy: 0.9005\n",
            "Epoch 223/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 6.5843e-04 - accuracy: 0.9998 - val_loss: 0.4598 - val_accuracy: 0.8992\n",
            "Epoch 224/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 0.0011 - accuracy: 0.9993 - val_loss: 0.4621 - val_accuracy: 0.9019\n",
            "Epoch 225/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 9.0539e-04 - accuracy: 0.9998 - val_loss: 0.4594 - val_accuracy: 0.9046\n",
            "Epoch 226/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4601 - val_accuracy: 0.9046\n",
            "Epoch 227/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 6.0406e-04 - accuracy: 0.9999 - val_loss: 0.4629 - val_accuracy: 0.9033\n",
            "Epoch 228/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 9.2619e-04 - accuracy: 0.9995 - val_loss: 0.4653 - val_accuracy: 0.9019\n",
            "Epoch 229/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 7.0320e-04 - accuracy: 1.0000 - val_loss: 0.4601 - val_accuracy: 0.9005\n",
            "Epoch 230/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 5.1068e-04 - accuracy: 1.0000 - val_loss: 0.4617 - val_accuracy: 0.9005\n",
            "Epoch 231/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 8.8085e-04 - accuracy: 1.0000 - val_loss: 0.4704 - val_accuracy: 0.9005\n",
            "Epoch 232/250\n",
            "23/23 [==============================] - 6s 270ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4696 - val_accuracy: 0.8978\n",
            "Epoch 233/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 5.9111e-04 - accuracy: 1.0000 - val_loss: 0.4609 - val_accuracy: 0.8965\n",
            "Epoch 234/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 4.6030e-04 - accuracy: 1.0000 - val_loss: 0.4655 - val_accuracy: 0.8978\n",
            "Epoch 235/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 3.8001e-04 - accuracy: 1.0000 - val_loss: 0.4675 - val_accuracy: 0.8978\n",
            "Epoch 236/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 4.2746e-04 - accuracy: 1.0000 - val_loss: 0.4683 - val_accuracy: 0.8992\n",
            "Epoch 237/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 3.3065e-04 - accuracy: 1.0000 - val_loss: 0.4689 - val_accuracy: 0.8992\n",
            "Epoch 238/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 5.3461e-04 - accuracy: 1.0000 - val_loss: 0.4677 - val_accuracy: 0.9005\n",
            "Epoch 239/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 4.3281e-04 - accuracy: 1.0000 - val_loss: 0.4662 - val_accuracy: 0.9005\n",
            "Epoch 240/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 4.4725e-04 - accuracy: 1.0000 - val_loss: 0.4688 - val_accuracy: 0.9005\n",
            "Epoch 241/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 7.2419e-04 - accuracy: 1.0000 - val_loss: 0.4689 - val_accuracy: 0.8992\n",
            "Epoch 242/250\n",
            "23/23 [==============================] - 6s 269ms/step - loss: 3.2109e-04 - accuracy: 1.0000 - val_loss: 0.4652 - val_accuracy: 0.8978\n",
            "Epoch 243/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 2.9635e-04 - accuracy: 1.0000 - val_loss: 0.4663 - val_accuracy: 0.9005\n",
            "Epoch 244/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 3.1290e-04 - accuracy: 1.0000 - val_loss: 0.4670 - val_accuracy: 0.9005\n",
            "Epoch 245/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 5.8820e-04 - accuracy: 1.0000 - val_loss: 0.4682 - val_accuracy: 0.9019\n",
            "Epoch 246/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 5.1962e-04 - accuracy: 1.0000 - val_loss: 0.4665 - val_accuracy: 0.9019\n",
            "Epoch 247/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 3.8890e-04 - accuracy: 1.0000 - val_loss: 0.4651 - val_accuracy: 0.9005\n",
            "Epoch 248/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 3.7109e-04 - accuracy: 1.0000 - val_loss: 0.4619 - val_accuracy: 0.9005\n",
            "Epoch 249/250\n",
            "23/23 [==============================] - 6s 268ms/step - loss: 3.4907e-04 - accuracy: 1.0000 - val_loss: 0.4626 - val_accuracy: 0.9019\n",
            "Epoch 250/250\n",
            "23/23 [==============================] - 6s 267ms/step - loss: 5.9410e-04 - accuracy: 1.0000 - val_loss: 0.4647 - val_accuracy: 0.9019\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kj7B5xTAsFaK",
        "outputId": "4d0ee60d-d41b-4e16-de29-fa33f54605ba"
      },
      "source": [
        "# gmp\n",
        "\n",
        "# Create the base model \n",
        "base_model = tf.keras.applications.InceptionV3(input_shape=(160,160,3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.summary()\n",
        "\n",
        "# process data\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
        "])\n",
        "\n",
        "# flattening\n",
        "global_max = tf.keras.layers.GlobalMaxPool2D()\n",
        "\n",
        "# final layer\n",
        "prediction_layer = tf.keras.layers.Dense(5)\n",
        "\n",
        "# construct a new network\n",
        "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = base_model(x)\n",
        "x = global_max(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "print(len(base_model.trainable_variables))\n",
        "print(len(model.trainable_variables))\n",
        "\n",
        "base_learning_rate = 0.0001\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate/10),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(train_dataset,\n",
        "                         epochs=250,\n",
        "                         validation_data=validation_dataset)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(history_fine.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_fine.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(history_fine.history['loss'], label='Training Loss')\n",
        "plt.plot(history_fine.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 79, 79, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 79, 79, 32)   96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 79, 79, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 77, 77, 32)   9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 77, 77, 32)   96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 77, 77, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 77, 77, 64)   18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 77, 77, 64)   192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 77, 77, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 38, 38, 64)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 38, 38, 80)   5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 38, 38, 80)   240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 38, 38, 80)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 36, 36, 192)  138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 36, 36, 192)  576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 36, 36, 192)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 17, 17, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 17, 17, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 17, 17, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 17, 17, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 17, 17, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 17, 17, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 17, 17, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 17, 17, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 17, 17, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 17, 17, 96)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 17, 17, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 17, 17, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 17, 17, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 17, 17, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 17, 17, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 17, 17, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 17, 17, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 17, 17, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 17, 17, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 17, 17, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 17, 17, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 17, 17, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 17, 17, 32)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 17, 17, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 17, 17, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 17, 17, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 17, 17, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 17, 17, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 17, 17, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 17, 17, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 17, 17, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 17, 17, 48)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 17, 17, 96)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 17, 17, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 17, 17, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 17, 17, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 17, 17, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 17, 17, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 17, 17, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 17, 17, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 17, 17, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 17, 17, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 17, 17, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 17, 17, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 17, 17, 96)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 17, 17, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 17, 17, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 17, 17, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 17, 17, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 17, 17, 64)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 17, 17, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 17, 17, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 17, 17, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 17, 17, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 17, 17, 48)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 17, 17, 96)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 17, 17, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 17, 17, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 17, 17, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 17, 17, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 17, 17, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 17, 17, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 17, 17, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 17, 17, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 17, 17, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 17, 17, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 17, 17, 64)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 17, 17, 96)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 17, 17, 64)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 17, 17, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 17, 17, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 17, 17, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 17, 17, 64)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 17, 17, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 17, 17, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 17, 17, 96)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 384)    995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 96)     82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 384)    1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 96)     288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 384)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 96)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 288)    0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 8, 8, 768)    0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 128)    384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 128)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 128)    114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 128)    384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 128)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 128)    114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 128)    384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 128)    384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 128)    0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 128)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 128)    114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 128)    114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 128)    384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 128)    384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 128)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 128)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 8, 8, 768)    0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 192)    147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 192)    172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 192)    172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 192)    147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 192)    576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 192)    576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 192)    576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 192)    576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 192)    0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 192)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 192)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 192)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 8, 8, 768)    0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 8, 8, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 8, 8, 160)    480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 8, 8, 160)    0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 8, 8, 160)    179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 8, 8, 160)    480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 8, 8, 160)    0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 8, 8, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 8, 8, 160)    179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 8, 8, 160)    480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 8, 8, 160)    480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 8, 160)    0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 8, 8, 160)    0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 8, 8, 160)    179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 8, 8, 160)    179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 8, 8, 160)    480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 8, 8, 160)    480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 8, 8, 160)    0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 8, 160)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 8, 8, 768)    0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 192)    147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 8, 8, 192)    215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 8, 8, 192)    215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 8, 8, 192)    147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 8, 8, 192)    576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 8, 8, 192)    576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 8, 8, 192)    576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 8, 8, 192)    576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 8, 8, 192)    0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 8, 8, 192)    0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 8, 8, 192)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 8, 8, 192)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 8, 8, 768)    0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 8, 8, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 160)    480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 160)    0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 8, 8, 160)    179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 8, 8, 160)    480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 8, 8, 160)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 8, 8, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 160)    179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 8, 8, 160)    480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 8, 8, 160)    480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 8, 8, 160)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 8, 8, 160)    0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 8, 8, 160)    179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 8, 8, 160)    179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 8, 8, 160)    480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 8, 8, 160)    480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 8, 8, 160)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 8, 8, 160)    0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 8, 8, 768)    0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 8, 8, 192)    147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 8, 8, 192)    215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 8, 8, 192)    215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 8, 8, 192)    147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 8, 8, 192)    576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 8, 8, 192)    576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 8, 8, 192)    576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 8, 8, 192)    576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 8, 8, 192)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 8, 8, 192)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 8, 8, 192)    0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 8, 8, 192)    0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 8, 8, 768)    0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 8, 8, 192)    576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 8, 8, 192)    0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 8, 8, 192)    258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 8, 8, 192)    576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 8, 8, 192)    0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 8, 8, 192)    258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 8, 8, 192)    576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 8, 8, 192)    576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 8, 8, 192)    0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 8, 8, 192)    0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 8, 8, 192)    258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 8, 8, 192)    258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 8, 8, 192)    576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 8, 8, 192)    576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 8, 8, 192)    0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 8, 8, 192)    0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 8, 8, 768)    0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 8, 8, 192)    258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 8, 8, 192)    258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 8, 8, 192)    147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 8, 8, 192)    576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 8, 8, 192)    576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 8, 8, 192)    576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 8, 8, 192)    576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 8, 8, 192)    0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 8, 8, 192)    0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 8, 8, 192)    0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 8, 8, 192)    0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 8, 8, 768)    0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 8, 8, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 8, 8, 192)    576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 8, 8, 192)    0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 8, 8, 192)    258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 8, 8, 192)    576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 8, 8, 192)    0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 8, 8, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 8, 8, 192)    258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 8, 8, 192)    576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 8, 8, 192)    576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 8, 8, 192)    0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 8, 8, 192)    0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 3, 3, 320)    552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 3, 3, 192)    331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 3, 3, 320)    960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 3, 3, 192)    576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 3, 3, 320)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 3, 3, 192)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 3, 3, 448)    1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 3, 3, 448)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 3, 3, 384)    1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 3, 3, 384)    1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 3, 3, 384)    1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 3, 3, 384)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 3, 3, 384)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 3, 3, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 3, 3, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 3, 3, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 3, 3, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 3, 3, 384)    1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 3, 3, 384)    1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 3, 3, 384)    1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 3, 3, 384)    1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 3, 3, 192)    245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 3, 3, 320)    960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 3, 3, 384)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 3, 3, 384)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 3, 3, 384)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 3, 3, 384)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 3, 3, 192)    576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 3, 3, 320)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3, 3, 768)    0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 3, 3, 192)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 3, 3, 448)    1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 3, 3, 448)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 3, 3, 384)    1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 3, 3, 384)    1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 3, 3, 384)    1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 3, 3, 384)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 3, 3, 384)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 3, 3, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 3, 3, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 3, 3, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 3, 3, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 3, 3, 384)    1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 3, 3, 384)    1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 3, 3, 384)    1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 3, 3, 384)    1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 3, 3, 192)    393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 3, 3, 320)    960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 3, 3, 384)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 3, 3, 384)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 3, 3, 384)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 3, 3, 384)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 3, 3, 192)    576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 3, 3, 320)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3, 3, 768)    0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 3, 3, 192)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 21,768,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n",
            "188\n",
            "190\n",
            "Epoch 1/250\n",
            "23/23 [==============================] - 64s 895ms/step - loss: 2.5278 - accuracy: 0.1974 - val_loss: 2.1751 - val_accuracy: 0.3079\n",
            "Epoch 2/250\n",
            "23/23 [==============================] - 11s 467ms/step - loss: 1.6951 - accuracy: 0.3753 - val_loss: 1.4553 - val_accuracy: 0.5095\n",
            "Epoch 3/250\n",
            "23/23 [==============================] - 11s 473ms/step - loss: 1.3500 - accuracy: 0.4813 - val_loss: 1.1297 - val_accuracy: 0.6144\n",
            "Epoch 4/250\n",
            "23/23 [==============================] - 11s 478ms/step - loss: 1.1506 - accuracy: 0.5672 - val_loss: 0.9417 - val_accuracy: 0.6662\n",
            "Epoch 5/250\n",
            "23/23 [==============================] - 11s 484ms/step - loss: 1.0072 - accuracy: 0.6087 - val_loss: 0.8374 - val_accuracy: 0.7030\n",
            "Epoch 6/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.8631 - accuracy: 0.6758 - val_loss: 0.7732 - val_accuracy: 0.7343\n",
            "Epoch 7/250\n",
            "23/23 [==============================] - 11s 501ms/step - loss: 0.8312 - accuracy: 0.7010 - val_loss: 0.7224 - val_accuracy: 0.7439\n",
            "Epoch 8/250\n",
            "23/23 [==============================] - 11s 493ms/step - loss: 0.7461 - accuracy: 0.7271 - val_loss: 0.6850 - val_accuracy: 0.7575\n",
            "Epoch 9/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.7074 - accuracy: 0.7470 - val_loss: 0.6506 - val_accuracy: 0.7657\n",
            "Epoch 10/250\n",
            "23/23 [==============================] - 11s 485ms/step - loss: 0.6467 - accuracy: 0.7619 - val_loss: 0.6260 - val_accuracy: 0.7752\n",
            "Epoch 11/250\n",
            "23/23 [==============================] - 11s 484ms/step - loss: 0.6426 - accuracy: 0.7610 - val_loss: 0.6062 - val_accuracy: 0.7861\n",
            "Epoch 12/250\n",
            "23/23 [==============================] - 11s 485ms/step - loss: 0.5833 - accuracy: 0.7828 - val_loss: 0.5903 - val_accuracy: 0.7943\n",
            "Epoch 13/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.5708 - accuracy: 0.7955 - val_loss: 0.5747 - val_accuracy: 0.8025\n",
            "Epoch 14/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.5231 - accuracy: 0.8112 - val_loss: 0.5606 - val_accuracy: 0.8038\n",
            "Epoch 15/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.4664 - accuracy: 0.8289 - val_loss: 0.5560 - val_accuracy: 0.8052\n",
            "Epoch 16/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.4533 - accuracy: 0.8344 - val_loss: 0.5469 - val_accuracy: 0.8093\n",
            "Epoch 17/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.4212 - accuracy: 0.8553 - val_loss: 0.5351 - val_accuracy: 0.8174\n",
            "Epoch 18/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.4046 - accuracy: 0.8562 - val_loss: 0.5176 - val_accuracy: 0.8188\n",
            "Epoch 19/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.3901 - accuracy: 0.8603 - val_loss: 0.5128 - val_accuracy: 0.8311\n",
            "Epoch 20/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.3561 - accuracy: 0.8774 - val_loss: 0.5102 - val_accuracy: 0.8324\n",
            "Epoch 21/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.3560 - accuracy: 0.8720 - val_loss: 0.5054 - val_accuracy: 0.8392\n",
            "Epoch 22/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.3502 - accuracy: 0.8830 - val_loss: 0.4992 - val_accuracy: 0.8379\n",
            "Epoch 23/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.3406 - accuracy: 0.8813 - val_loss: 0.4976 - val_accuracy: 0.8392\n",
            "Epoch 24/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.3137 - accuracy: 0.8920 - val_loss: 0.4954 - val_accuracy: 0.8420\n",
            "Epoch 25/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.3218 - accuracy: 0.8922 - val_loss: 0.4864 - val_accuracy: 0.8392\n",
            "Epoch 26/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.2703 - accuracy: 0.9005 - val_loss: 0.4802 - val_accuracy: 0.8406\n",
            "Epoch 27/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.2706 - accuracy: 0.9078 - val_loss: 0.4729 - val_accuracy: 0.8406\n",
            "Epoch 28/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.2637 - accuracy: 0.9050 - val_loss: 0.4686 - val_accuracy: 0.8420\n",
            "Epoch 29/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.2663 - accuracy: 0.9039 - val_loss: 0.4633 - val_accuracy: 0.8447\n",
            "Epoch 30/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.2474 - accuracy: 0.9099 - val_loss: 0.4567 - val_accuracy: 0.8474\n",
            "Epoch 31/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.2408 - accuracy: 0.9205 - val_loss: 0.4526 - val_accuracy: 0.8488\n",
            "Epoch 32/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.2220 - accuracy: 0.9260 - val_loss: 0.4532 - val_accuracy: 0.8447\n",
            "Epoch 33/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.2153 - accuracy: 0.9184 - val_loss: 0.4482 - val_accuracy: 0.8474\n",
            "Epoch 34/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.2075 - accuracy: 0.9314 - val_loss: 0.4456 - val_accuracy: 0.8542\n",
            "Epoch 35/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.1895 - accuracy: 0.9311 - val_loss: 0.4413 - val_accuracy: 0.8529\n",
            "Epoch 36/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.1839 - accuracy: 0.9417 - val_loss: 0.4442 - val_accuracy: 0.8529\n",
            "Epoch 37/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.1788 - accuracy: 0.9424 - val_loss: 0.4448 - val_accuracy: 0.8542\n",
            "Epoch 38/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.1697 - accuracy: 0.9464 - val_loss: 0.4449 - val_accuracy: 0.8542\n",
            "Epoch 39/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.1682 - accuracy: 0.9470 - val_loss: 0.4403 - val_accuracy: 0.8569\n",
            "Epoch 40/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.1470 - accuracy: 0.9496 - val_loss: 0.4408 - val_accuracy: 0.8556\n",
            "Epoch 41/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.1421 - accuracy: 0.9535 - val_loss: 0.4405 - val_accuracy: 0.8569\n",
            "Epoch 42/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.1408 - accuracy: 0.9486 - val_loss: 0.4333 - val_accuracy: 0.8610\n",
            "Epoch 43/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.1371 - accuracy: 0.9537 - val_loss: 0.4282 - val_accuracy: 0.8678\n",
            "Epoch 44/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.1238 - accuracy: 0.9578 - val_loss: 0.4333 - val_accuracy: 0.8665\n",
            "Epoch 45/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.1165 - accuracy: 0.9653 - val_loss: 0.4391 - val_accuracy: 0.8651\n",
            "Epoch 46/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.1202 - accuracy: 0.9636 - val_loss: 0.4406 - val_accuracy: 0.8624\n",
            "Epoch 47/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.1119 - accuracy: 0.9641 - val_loss: 0.4423 - val_accuracy: 0.8692\n",
            "Epoch 48/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.1211 - accuracy: 0.9595 - val_loss: 0.4360 - val_accuracy: 0.8692\n",
            "Epoch 49/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0994 - accuracy: 0.9666 - val_loss: 0.4398 - val_accuracy: 0.8733\n",
            "Epoch 50/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0983 - accuracy: 0.9695 - val_loss: 0.4327 - val_accuracy: 0.8760\n",
            "Epoch 51/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.1007 - accuracy: 0.9694 - val_loss: 0.4309 - val_accuracy: 0.8760\n",
            "Epoch 52/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0855 - accuracy: 0.9735 - val_loss: 0.4337 - val_accuracy: 0.8733\n",
            "Epoch 53/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0886 - accuracy: 0.9764 - val_loss: 0.4377 - val_accuracy: 0.8760\n",
            "Epoch 54/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0931 - accuracy: 0.9704 - val_loss: 0.4321 - val_accuracy: 0.8760\n",
            "Epoch 55/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0739 - accuracy: 0.9765 - val_loss: 0.4365 - val_accuracy: 0.8774\n",
            "Epoch 56/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0782 - accuracy: 0.9787 - val_loss: 0.4386 - val_accuracy: 0.8733\n",
            "Epoch 57/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0786 - accuracy: 0.9733 - val_loss: 0.4462 - val_accuracy: 0.8733\n",
            "Epoch 58/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0770 - accuracy: 0.9741 - val_loss: 0.4460 - val_accuracy: 0.8733\n",
            "Epoch 59/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0730 - accuracy: 0.9738 - val_loss: 0.4421 - val_accuracy: 0.8733\n",
            "Epoch 60/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0624 - accuracy: 0.9835 - val_loss: 0.4386 - val_accuracy: 0.8774\n",
            "Epoch 61/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0643 - accuracy: 0.9783 - val_loss: 0.4394 - val_accuracy: 0.8815\n",
            "Epoch 62/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0519 - accuracy: 0.9884 - val_loss: 0.4374 - val_accuracy: 0.8801\n",
            "Epoch 63/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0624 - accuracy: 0.9801 - val_loss: 0.4396 - val_accuracy: 0.8801\n",
            "Epoch 64/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0521 - accuracy: 0.9856 - val_loss: 0.4446 - val_accuracy: 0.8828\n",
            "Epoch 65/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0574 - accuracy: 0.9813 - val_loss: 0.4428 - val_accuracy: 0.8828\n",
            "Epoch 66/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0485 - accuracy: 0.9871 - val_loss: 0.4445 - val_accuracy: 0.8856\n",
            "Epoch 67/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0483 - accuracy: 0.9834 - val_loss: 0.4519 - val_accuracy: 0.8828\n",
            "Epoch 68/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0449 - accuracy: 0.9895 - val_loss: 0.4530 - val_accuracy: 0.8815\n",
            "Epoch 69/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0375 - accuracy: 0.9906 - val_loss: 0.4550 - val_accuracy: 0.8815\n",
            "Epoch 70/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0459 - accuracy: 0.9870 - val_loss: 0.4585 - val_accuracy: 0.8774\n",
            "Epoch 71/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0367 - accuracy: 0.9908 - val_loss: 0.4549 - val_accuracy: 0.8774\n",
            "Epoch 72/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0381 - accuracy: 0.9895 - val_loss: 0.4541 - val_accuracy: 0.8801\n",
            "Epoch 73/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0348 - accuracy: 0.9938 - val_loss: 0.4597 - val_accuracy: 0.8787\n",
            "Epoch 74/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0416 - accuracy: 0.9902 - val_loss: 0.4623 - val_accuracy: 0.8801\n",
            "Epoch 75/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0305 - accuracy: 0.9933 - val_loss: 0.4573 - val_accuracy: 0.8842\n",
            "Epoch 76/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0348 - accuracy: 0.9916 - val_loss: 0.4531 - val_accuracy: 0.8842\n",
            "Epoch 77/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0264 - accuracy: 0.9950 - val_loss: 0.4609 - val_accuracy: 0.8801\n",
            "Epoch 78/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0345 - accuracy: 0.9910 - val_loss: 0.4623 - val_accuracy: 0.8801\n",
            "Epoch 79/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0264 - accuracy: 0.9948 - val_loss: 0.4609 - val_accuracy: 0.8801\n",
            "Epoch 80/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0262 - accuracy: 0.9946 - val_loss: 0.4694 - val_accuracy: 0.8801\n",
            "Epoch 81/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0224 - accuracy: 0.9972 - val_loss: 0.4693 - val_accuracy: 0.8801\n",
            "Epoch 82/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0234 - accuracy: 0.9934 - val_loss: 0.4760 - val_accuracy: 0.8801\n",
            "Epoch 83/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0235 - accuracy: 0.9949 - val_loss: 0.4746 - val_accuracy: 0.8801\n",
            "Epoch 84/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0213 - accuracy: 0.9938 - val_loss: 0.4748 - val_accuracy: 0.8842\n",
            "Epoch 85/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0256 - accuracy: 0.9939 - val_loss: 0.4765 - val_accuracy: 0.8842\n",
            "Epoch 86/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0254 - accuracy: 0.9944 - val_loss: 0.4764 - val_accuracy: 0.8828\n",
            "Epoch 87/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0264 - accuracy: 0.9930 - val_loss: 0.4820 - val_accuracy: 0.8815\n",
            "Epoch 88/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0242 - accuracy: 0.9951 - val_loss: 0.4818 - val_accuracy: 0.8815\n",
            "Epoch 89/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0178 - accuracy: 0.9973 - val_loss: 0.4840 - val_accuracy: 0.8801\n",
            "Epoch 90/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0143 - accuracy: 0.9982 - val_loss: 0.4839 - val_accuracy: 0.8801\n",
            "Epoch 91/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0201 - accuracy: 0.9936 - val_loss: 0.4873 - val_accuracy: 0.8828\n",
            "Epoch 92/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0161 - accuracy: 0.9952 - val_loss: 0.4893 - val_accuracy: 0.8828\n",
            "Epoch 93/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0237 - accuracy: 0.9940 - val_loss: 0.4891 - val_accuracy: 0.8869\n",
            "Epoch 94/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0192 - accuracy: 0.9963 - val_loss: 0.4892 - val_accuracy: 0.8828\n",
            "Epoch 95/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0171 - accuracy: 0.9951 - val_loss: 0.4899 - val_accuracy: 0.8787\n",
            "Epoch 96/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0238 - accuracy: 0.9935 - val_loss: 0.4868 - val_accuracy: 0.8828\n",
            "Epoch 97/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0170 - accuracy: 0.9972 - val_loss: 0.4888 - val_accuracy: 0.8856\n",
            "Epoch 98/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0162 - accuracy: 0.9961 - val_loss: 0.4906 - val_accuracy: 0.8842\n",
            "Epoch 99/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0203 - accuracy: 0.9926 - val_loss: 0.4954 - val_accuracy: 0.8869\n",
            "Epoch 100/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0173 - accuracy: 0.9961 - val_loss: 0.4939 - val_accuracy: 0.8856\n",
            "Epoch 101/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0129 - accuracy: 0.9972 - val_loss: 0.4941 - val_accuracy: 0.8856\n",
            "Epoch 102/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0113 - accuracy: 0.9982 - val_loss: 0.4997 - val_accuracy: 0.8856\n",
            "Epoch 103/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0150 - accuracy: 0.9974 - val_loss: 0.4991 - val_accuracy: 0.8856\n",
            "Epoch 104/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0186 - accuracy: 0.9946 - val_loss: 0.5008 - val_accuracy: 0.8896\n",
            "Epoch 105/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0098 - accuracy: 0.9981 - val_loss: 0.5032 - val_accuracy: 0.8869\n",
            "Epoch 106/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0145 - accuracy: 0.9971 - val_loss: 0.5011 - val_accuracy: 0.8924\n",
            "Epoch 107/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0105 - accuracy: 0.9997 - val_loss: 0.5008 - val_accuracy: 0.8896\n",
            "Epoch 108/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0113 - accuracy: 0.9986 - val_loss: 0.5056 - val_accuracy: 0.8883\n",
            "Epoch 109/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0086 - accuracy: 0.9996 - val_loss: 0.5100 - val_accuracy: 0.8883\n",
            "Epoch 110/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0092 - accuracy: 0.9993 - val_loss: 0.5113 - val_accuracy: 0.8883\n",
            "Epoch 111/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0084 - accuracy: 0.9986 - val_loss: 0.5161 - val_accuracy: 0.8856\n",
            "Epoch 112/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0136 - accuracy: 0.9953 - val_loss: 0.5174 - val_accuracy: 0.8842\n",
            "Epoch 113/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0087 - accuracy: 0.9994 - val_loss: 0.5166 - val_accuracy: 0.8856\n",
            "Epoch 114/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0097 - accuracy: 0.9988 - val_loss: 0.5191 - val_accuracy: 0.8842\n",
            "Epoch 115/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0118 - accuracy: 0.9959 - val_loss: 0.5201 - val_accuracy: 0.8842\n",
            "Epoch 116/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 0.5264 - val_accuracy: 0.8828\n",
            "Epoch 117/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.5283 - val_accuracy: 0.8842\n",
            "Epoch 118/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0067 - accuracy: 0.9991 - val_loss: 0.5322 - val_accuracy: 0.8828\n",
            "Epoch 119/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.5289 - val_accuracy: 0.8815\n",
            "Epoch 120/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0156 - accuracy: 0.9958 - val_loss: 0.5194 - val_accuracy: 0.8842\n",
            "Epoch 121/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0077 - accuracy: 0.9972 - val_loss: 0.5184 - val_accuracy: 0.8842\n",
            "Epoch 122/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0049 - accuracy: 0.9993 - val_loss: 0.5185 - val_accuracy: 0.8856\n",
            "Epoch 123/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0095 - accuracy: 0.9960 - val_loss: 0.5232 - val_accuracy: 0.8842\n",
            "Epoch 124/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.5247 - val_accuracy: 0.8842\n",
            "Epoch 125/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0077 - accuracy: 0.9992 - val_loss: 0.5248 - val_accuracy: 0.8828\n",
            "Epoch 126/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0091 - accuracy: 0.9960 - val_loss: 0.5269 - val_accuracy: 0.8842\n",
            "Epoch 127/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.5269 - val_accuracy: 0.8856\n",
            "Epoch 128/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0049 - accuracy: 0.9992 - val_loss: 0.5276 - val_accuracy: 0.8869\n",
            "Epoch 129/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.5340 - val_accuracy: 0.8856\n",
            "Epoch 130/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0048 - accuracy: 0.9996 - val_loss: 0.5362 - val_accuracy: 0.8856\n",
            "Epoch 131/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0057 - accuracy: 0.9993 - val_loss: 0.5424 - val_accuracy: 0.8828\n",
            "Epoch 132/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.5443 - val_accuracy: 0.8828\n",
            "Epoch 133/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.5456 - val_accuracy: 0.8856\n",
            "Epoch 134/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.5471 - val_accuracy: 0.8842\n",
            "Epoch 135/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0046 - accuracy: 0.9993 - val_loss: 0.5464 - val_accuracy: 0.8842\n",
            "Epoch 136/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.5438 - val_accuracy: 0.8856\n",
            "Epoch 137/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.5473 - val_accuracy: 0.8856\n",
            "Epoch 138/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 0.5515 - val_accuracy: 0.8869\n",
            "Epoch 139/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.5498 - val_accuracy: 0.8856\n",
            "Epoch 140/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.5480 - val_accuracy: 0.8856\n",
            "Epoch 141/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0068 - accuracy: 0.9990 - val_loss: 0.5526 - val_accuracy: 0.8869\n",
            "Epoch 142/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.5529 - val_accuracy: 0.8842\n",
            "Epoch 143/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.5521 - val_accuracy: 0.8869\n",
            "Epoch 144/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.5466 - val_accuracy: 0.8896\n",
            "Epoch 145/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0043 - accuracy: 0.9993 - val_loss: 0.5355 - val_accuracy: 0.8910\n",
            "Epoch 146/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0054 - accuracy: 0.9991 - val_loss: 0.5414 - val_accuracy: 0.8883\n",
            "Epoch 147/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.5407 - val_accuracy: 0.8869\n",
            "Epoch 148/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.5432 - val_accuracy: 0.8883\n",
            "Epoch 149/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.5468 - val_accuracy: 0.8856\n",
            "Epoch 150/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5522 - val_accuracy: 0.8856\n",
            "Epoch 151/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.5459 - val_accuracy: 0.8883\n",
            "Epoch 152/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0057 - accuracy: 0.9990 - val_loss: 0.5476 - val_accuracy: 0.8883\n",
            "Epoch 153/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.5489 - val_accuracy: 0.8910\n",
            "Epoch 154/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.5591 - val_accuracy: 0.8896\n",
            "Epoch 155/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0062 - accuracy: 0.9986 - val_loss: 0.5659 - val_accuracy: 0.8869\n",
            "Epoch 156/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5568 - val_accuracy: 0.8869\n",
            "Epoch 157/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.5582 - val_accuracy: 0.8883\n",
            "Epoch 158/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.5490 - val_accuracy: 0.8896\n",
            "Epoch 159/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0057 - accuracy: 0.9988 - val_loss: 0.5483 - val_accuracy: 0.8896\n",
            "Epoch 160/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.5490 - val_accuracy: 0.8883\n",
            "Epoch 161/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.5535 - val_accuracy: 0.8883\n",
            "Epoch 162/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.5553 - val_accuracy: 0.8869\n",
            "Epoch 163/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5496 - val_accuracy: 0.8896\n",
            "Epoch 164/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.5466 - val_accuracy: 0.8869\n",
            "Epoch 165/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5527 - val_accuracy: 0.8869\n",
            "Epoch 166/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.5541 - val_accuracy: 0.8869\n",
            "Epoch 167/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0043 - accuracy: 0.9979 - val_loss: 0.5538 - val_accuracy: 0.8856\n",
            "Epoch 168/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5507 - val_accuracy: 0.8842\n",
            "Epoch 169/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.5555 - val_accuracy: 0.8815\n",
            "Epoch 170/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5550 - val_accuracy: 0.8828\n",
            "Epoch 171/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.5581 - val_accuracy: 0.8842\n",
            "Epoch 172/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.5635 - val_accuracy: 0.8842\n",
            "Epoch 173/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.5610 - val_accuracy: 0.8828\n",
            "Epoch 174/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5610 - val_accuracy: 0.8828\n",
            "Epoch 175/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.5588 - val_accuracy: 0.8815\n",
            "Epoch 176/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.5580 - val_accuracy: 0.8801\n",
            "Epoch 177/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.5537 - val_accuracy: 0.8815\n",
            "Epoch 178/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.5579 - val_accuracy: 0.8815\n",
            "Epoch 179/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0034 - accuracy: 0.9982 - val_loss: 0.5594 - val_accuracy: 0.8801\n",
            "Epoch 180/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.5635 - val_accuracy: 0.8856\n",
            "Epoch 181/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.5669 - val_accuracy: 0.8842\n",
            "Epoch 182/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5691 - val_accuracy: 0.8842\n",
            "Epoch 183/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.5727 - val_accuracy: 0.8856\n",
            "Epoch 184/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.5795 - val_accuracy: 0.8856\n",
            "Epoch 185/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.5804 - val_accuracy: 0.8828\n",
            "Epoch 186/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5780 - val_accuracy: 0.8842\n",
            "Epoch 187/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.5741 - val_accuracy: 0.8856\n",
            "Epoch 188/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.5722 - val_accuracy: 0.8842\n",
            "Epoch 189/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5725 - val_accuracy: 0.8842\n",
            "Epoch 190/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.5719 - val_accuracy: 0.8828\n",
            "Epoch 191/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.5718 - val_accuracy: 0.8828\n",
            "Epoch 192/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0039 - accuracy: 0.9983 - val_loss: 0.5906 - val_accuracy: 0.8801\n",
            "Epoch 193/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5896 - val_accuracy: 0.8828\n",
            "Epoch 194/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5889 - val_accuracy: 0.8815\n",
            "Epoch 195/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.5973 - val_accuracy: 0.8787\n",
            "Epoch 196/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.6057 - val_accuracy: 0.8760\n",
            "Epoch 197/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 0.8747\n",
            "Epoch 198/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 0.8747\n",
            "Epoch 199/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.6104 - val_accuracy: 0.8747\n",
            "Epoch 200/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 7.3419e-04 - accuracy: 1.0000 - val_loss: 0.6120 - val_accuracy: 0.8774\n",
            "Epoch 201/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.6099 - val_accuracy: 0.8787\n",
            "Epoch 202/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6089 - val_accuracy: 0.8747\n",
            "Epoch 203/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0018 - accuracy: 0.9990 - val_loss: 0.6079 - val_accuracy: 0.8747\n",
            "Epoch 204/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 9.6408e-04 - accuracy: 1.0000 - val_loss: 0.6084 - val_accuracy: 0.8760\n",
            "Epoch 205/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.6091 - val_accuracy: 0.8747\n",
            "Epoch 206/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0028 - accuracy: 0.9984 - val_loss: 0.6027 - val_accuracy: 0.8747\n",
            "Epoch 207/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6034 - val_accuracy: 0.8774\n",
            "Epoch 208/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.6021 - val_accuracy: 0.8760\n",
            "Epoch 209/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.5954 - val_accuracy: 0.8787\n",
            "Epoch 210/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 9.1461e-04 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 0.8774\n",
            "Epoch 211/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 0.8747\n",
            "Epoch 212/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 0.8747\n",
            "Epoch 213/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0027 - accuracy: 0.9986 - val_loss: 0.6106 - val_accuracy: 0.8733\n",
            "Epoch 214/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 7.4762e-04 - accuracy: 1.0000 - val_loss: 0.6134 - val_accuracy: 0.8787\n",
            "Epoch 215/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 8.7464e-04 - accuracy: 1.0000 - val_loss: 0.6100 - val_accuracy: 0.8774\n",
            "Epoch 216/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 0.8760\n",
            "Epoch 217/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.6019 - val_accuracy: 0.8760\n",
            "Epoch 218/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6049 - val_accuracy: 0.8787\n",
            "Epoch 219/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 7.9001e-04 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 0.8787\n",
            "Epoch 220/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 7.2999e-04 - accuracy: 1.0000 - val_loss: 0.6095 - val_accuracy: 0.8787\n",
            "Epoch 221/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 7.5670e-04 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 0.8787\n",
            "Epoch 222/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 5.1641e-04 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 0.8801\n",
            "Epoch 223/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 5.2553e-04 - accuracy: 1.0000 - val_loss: 0.6076 - val_accuracy: 0.8801\n",
            "Epoch 224/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 7.1897e-04 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 0.8801\n",
            "Epoch 225/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.6142 - val_accuracy: 0.8815\n",
            "Epoch 226/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 0.8774\n",
            "Epoch 227/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 8.3564e-04 - accuracy: 1.0000 - val_loss: 0.6174 - val_accuracy: 0.8760\n",
            "Epoch 228/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 7.2209e-04 - accuracy: 1.0000 - val_loss: 0.6166 - val_accuracy: 0.8774\n",
            "Epoch 229/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 7.0852e-04 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 0.8760\n",
            "Epoch 230/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 7.2414e-04 - accuracy: 1.0000 - val_loss: 0.6147 - val_accuracy: 0.8774\n",
            "Epoch 231/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 7.1476e-04 - accuracy: 1.0000 - val_loss: 0.6201 - val_accuracy: 0.8774\n",
            "Epoch 232/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 4.6496e-04 - accuracy: 1.0000 - val_loss: 0.6229 - val_accuracy: 0.8774\n",
            "Epoch 233/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 0.6322 - val_accuracy: 0.8760\n",
            "Epoch 234/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 6.0878e-04 - accuracy: 1.0000 - val_loss: 0.6410 - val_accuracy: 0.8747\n",
            "Epoch 235/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 6.4703e-04 - accuracy: 1.0000 - val_loss: 0.6391 - val_accuracy: 0.8733\n",
            "Epoch 236/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 5.3874e-04 - accuracy: 1.0000 - val_loss: 0.6331 - val_accuracy: 0.8760\n",
            "Epoch 237/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 8.3676e-04 - accuracy: 1.0000 - val_loss: 0.6319 - val_accuracy: 0.8787\n",
            "Epoch 238/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 8.9623e-04 - accuracy: 1.0000 - val_loss: 0.6275 - val_accuracy: 0.8787\n",
            "Epoch 239/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 5.9043e-04 - accuracy: 1.0000 - val_loss: 0.6284 - val_accuracy: 0.8774\n",
            "Epoch 240/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.6259 - val_accuracy: 0.8787\n",
            "Epoch 241/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0018 - accuracy: 0.9991 - val_loss: 0.6206 - val_accuracy: 0.8774\n",
            "Epoch 242/250\n",
            "23/23 [==============================] - 11s 491ms/step - loss: 4.9466e-04 - accuracy: 1.0000 - val_loss: 0.6211 - val_accuracy: 0.8787\n",
            "Epoch 243/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 7.8525e-04 - accuracy: 0.9998 - val_loss: 0.6261 - val_accuracy: 0.8801\n",
            "Epoch 244/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 6.5837e-04 - accuracy: 0.9999 - val_loss: 0.6271 - val_accuracy: 0.8842\n",
            "Epoch 245/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 6.0166e-04 - accuracy: 1.0000 - val_loss: 0.6249 - val_accuracy: 0.8842\n",
            "Epoch 246/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6237 - val_accuracy: 0.8869\n",
            "Epoch 247/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 4.8548e-04 - accuracy: 1.0000 - val_loss: 0.6249 - val_accuracy: 0.8869\n",
            "Epoch 248/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.6230 - val_accuracy: 0.8828\n",
            "Epoch 249/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 6.4343e-04 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 0.8815\n",
            "Epoch 250/250\n",
            "23/23 [==============================] - 11s 491ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6276 - val_accuracy: 0.8815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qxsJPiJEsFaK",
        "outputId": "72cb4dd6-dc8b-4dc4-8ea7-3b78f814997d"
      },
      "source": [
        "# gap\n",
        "\n",
        "# Create the base model \n",
        "base_model = tf.keras.applications.InceptionV3(input_shape=(160,160,3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.summary()\n",
        "\n",
        "# process data\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
        "])\n",
        "\n",
        "# flattening\n",
        "global_average = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "# final layer\n",
        "prediction_layer = tf.keras.layers.Dense(5)\n",
        "\n",
        "# construct a new network\n",
        "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = base_model(x)\n",
        "x = global_average(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "print(len(base_model.trainable_variables))\n",
        "print(len(model.trainable_variables))\n",
        "\n",
        "base_learning_rate = 0.0001\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate/10),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(train_dataset,\n",
        "                         epochs=250,\n",
        "                         validation_data=validation_dataset)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(history_fine.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_fine.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(history_fine.history['loss'], label='Training Loss')\n",
        "plt.plot(history_fine.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 79, 79, 32)   864         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 79, 79, 32)   96          conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 79, 79, 32)   0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 77, 77, 32)   9216        activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 77, 77, 32)   96          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 77, 77, 32)   0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 77, 77, 64)   18432       activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 77, 77, 64)   192         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 77, 77, 64)   0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 38, 38, 64)   0           activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 38, 38, 80)   5120        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 38, 38, 80)   240         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 38, 38, 80)   0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 36, 36, 192)  138240      activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 36, 36, 192)  576         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 36, 36, 192)  0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 17, 17, 192)  0           activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 17, 17, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 17, 17, 64)   192         conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 17, 17, 64)   0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 17, 17, 48)   9216        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 17, 17, 96)   55296       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 17, 17, 48)   144         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 17, 17, 96)   288         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 17, 17, 48)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 17, 17, 96)   0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 17, 17, 192)  0           max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 17, 17, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 17, 17, 64)   76800       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 17, 17, 96)   82944       activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 17, 17, 32)   6144        average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 17, 17, 64)   192         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 17, 17, 64)   192         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 17, 17, 96)   288         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 17, 17, 32)   96          conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 17, 17, 64)   0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 17, 17, 64)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 17, 17, 96)   0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 17, 17, 32)   0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 17, 17, 256)  0           activation_99[0][0]              \n",
            "                                                                 activation_101[0][0]             \n",
            "                                                                 activation_104[0][0]             \n",
            "                                                                 activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 17, 17, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 17, 17, 64)   192         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 17, 17, 64)   0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 17, 17, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 17, 17, 96)   55296       activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 17, 17, 48)   144         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 17, 17, 96)   288         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 17, 17, 48)   0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 17, 17, 96)   0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_10 (AveragePo (None, 17, 17, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 17, 17, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 17, 17, 64)   76800       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 17, 17, 96)   82944       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 17, 17, 64)   16384       average_pooling2d_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 17, 17, 64)   192         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 17, 17, 64)   192         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 17, 17, 96)   288         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 17, 17, 64)   192         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 17, 17, 64)   0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 17, 17, 64)   0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 17, 17, 96)   0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 17, 17, 64)   0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 17, 17, 288)  0           activation_106[0][0]             \n",
            "                                                                 activation_108[0][0]             \n",
            "                                                                 activation_111[0][0]             \n",
            "                                                                 activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 17, 17, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 17, 17, 64)   192         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 17, 17, 64)   0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 17, 17, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 17, 17, 96)   55296       activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 17, 17, 48)   144         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 17, 17, 96)   288         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 17, 17, 48)   0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 17, 17, 96)   0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_11 (AveragePo (None, 17, 17, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 17, 17, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 17, 17, 64)   76800       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 17, 17, 96)   82944       activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 17, 17, 64)   18432       average_pooling2d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 17, 17, 64)   192         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 17, 17, 64)   192         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 17, 17, 96)   288         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 17, 17, 64)   192         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 17, 17, 64)   0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 17, 17, 64)   0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 17, 17, 96)   0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 17, 17, 64)   0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 17, 17, 288)  0           activation_113[0][0]             \n",
            "                                                                 activation_115[0][0]             \n",
            "                                                                 activation_118[0][0]             \n",
            "                                                                 activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 17, 17, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 17, 17, 64)   192         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 17, 17, 64)   0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 17, 17, 96)   55296       activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 17, 17, 96)   288         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 17, 17, 96)   0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 8, 8, 384)    995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 8, 8, 96)     82944       activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 8, 8, 384)    1152        conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 8, 8, 96)     288         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 8, 8, 384)    0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 8, 8, 96)     0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 8, 8, 288)    0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 8, 8, 768)    0           activation_120[0][0]             \n",
            "                                                                 activation_123[0][0]             \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 8, 8, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 8, 8, 128)    384         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 8, 8, 128)    0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 8, 8, 128)    114688      activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 8, 8, 128)    384         conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 8, 8, 128)    0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 8, 8, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 8, 8, 128)    114688      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 8, 8, 128)    384         conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 8, 8, 128)    384         conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 8, 8, 128)    0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 8, 8, 128)    0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 8, 8, 128)    114688      activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 8, 8, 128)    114688      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 8, 8, 128)    384         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 8, 8, 128)    384         conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 8, 8, 128)    0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 8, 8, 128)    0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_12 (AveragePo (None, 8, 8, 768)    0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 8, 8, 192)    147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 8, 8, 192)    172032      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 8, 8, 192)    172032      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 8, 8, 192)    147456      average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 8, 8, 192)    576         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 8, 8, 192)    576         conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 8, 8, 192)    576         conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 8, 8, 192)    576         conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 8, 8, 192)    0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 8, 8, 192)    0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 8, 8, 192)    0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 8, 8, 192)    0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 8, 8, 768)    0           activation_124[0][0]             \n",
            "                                                                 activation_127[0][0]             \n",
            "                                                                 activation_132[0][0]             \n",
            "                                                                 activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 8, 8, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 8, 8, 160)    480         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 8, 8, 160)    0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 8, 8, 160)    179200      activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 8, 8, 160)    480         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 8, 8, 160)    0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 8, 8, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 8, 8, 160)    179200      activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 8, 8, 160)    480         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 8, 8, 160)    480         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 8, 8, 160)    0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 8, 8, 160)    0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 8, 8, 160)    179200      activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 8, 8, 160)    179200      activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 8, 8, 160)    480         conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 8, 8, 160)    480         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 8, 8, 160)    0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 8, 8, 160)    0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_13 (AveragePo (None, 8, 8, 768)    0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 8, 8, 192)    147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 8, 8, 192)    215040      activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 8, 8, 192)    215040      activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 8, 8, 192)    147456      average_pooling2d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 8, 8, 192)    576         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 8, 8, 192)    576         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 8, 8, 192)    576         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 8, 8, 192)    576         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 8, 8, 192)    0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 8, 8, 192)    0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 8, 8, 192)    0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 8, 8, 192)    0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 8, 8, 768)    0           activation_134[0][0]             \n",
            "                                                                 activation_137[0][0]             \n",
            "                                                                 activation_142[0][0]             \n",
            "                                                                 activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 8, 8, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 8, 8, 160)    480         conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 8, 8, 160)    0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 8, 8, 160)    179200      activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 8, 8, 160)    480         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 8, 8, 160)    0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 8, 8, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 8, 8, 160)    179200      activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 8, 8, 160)    480         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 8, 8, 160)    480         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 8, 8, 160)    0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 8, 8, 160)    0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 8, 8, 160)    179200      activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 8, 8, 160)    179200      activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 8, 8, 160)    480         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 8, 8, 160)    480         conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 8, 8, 160)    0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 8, 8, 160)    0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_14 (AveragePo (None, 8, 8, 768)    0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 8, 8, 192)    147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 8, 8, 192)    215040      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 8, 8, 192)    215040      activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 8, 8, 192)    147456      average_pooling2d_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 8, 8, 192)    576         conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 8, 8, 192)    576         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 8, 8, 192)    576         conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 8, 8, 192)    576         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 8, 8, 192)    0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 8, 8, 192)    0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 8, 8, 192)    0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 8, 8, 192)    0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 8, 8, 768)    0           activation_144[0][0]             \n",
            "                                                                 activation_147[0][0]             \n",
            "                                                                 activation_152[0][0]             \n",
            "                                                                 activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 8, 8, 192)    576         conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 8, 8, 192)    0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 8, 8, 192)    258048      activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 8, 8, 192)    576         conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 8, 8, 192)    0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 8, 8, 192)    258048      activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 8, 8, 192)    576         conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 8, 8, 192)    576         conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 8, 8, 192)    0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 8, 8, 192)    0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 8, 8, 192)    258048      activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 8, 8, 192)    258048      activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 8, 8, 192)    576         conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 8, 8, 192)    576         conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 8, 8, 192)    0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 8, 8, 192)    0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_15 (AveragePo (None, 8, 8, 768)    0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 8, 8, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 8, 8, 192)    258048      activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 8, 8, 192)    258048      activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 8, 8, 192)    147456      average_pooling2d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 8, 8, 192)    576         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 8, 8, 192)    576         conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 8, 8, 192)    576         conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 8, 8, 192)    576         conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 8, 8, 192)    0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 8, 8, 192)    0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 8, 8, 192)    0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 8, 8, 192)    0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 8, 8, 768)    0           activation_154[0][0]             \n",
            "                                                                 activation_157[0][0]             \n",
            "                                                                 activation_162[0][0]             \n",
            "                                                                 activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 8, 8, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 8, 8, 192)    576         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 8, 8, 192)    0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 8, 8, 192)    258048      activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 8, 8, 192)    576         conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 8, 8, 192)    0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 8, 8, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 8, 8, 192)    258048      activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 8, 8, 192)    576         conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 8, 8, 192)    576         conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 8, 8, 192)    0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 8, 8, 192)    0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 3, 3, 320)    552960      activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 3, 3, 192)    331776      activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 3, 3, 320)    960         conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 3, 3, 192)    576         conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 3, 3, 320)    0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 3, 3, 192)    0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_165[0][0]             \n",
            "                                                                 activation_169[0][0]             \n",
            "                                                                 max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 3, 3, 448)    1344        conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 3, 3, 448)    0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 3, 3, 384)    1548288     activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 3, 3, 384)    1152        conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 3, 3, 384)    1152        conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 3, 3, 384)    0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 3, 3, 384)    0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 3, 3, 384)    442368      activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 3, 3, 384)    442368      activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 3, 3, 384)    442368      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 3, 3, 384)    442368      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_16 (AveragePo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 3, 3, 384)    1152        conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 3, 3, 384)    1152        conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 3, 3, 384)    1152        conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 3, 3, 384)    1152        conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 3, 3, 192)    245760      average_pooling2d_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 3, 3, 320)    960         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 3, 3, 384)    0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 3, 3, 384)    0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 3, 3, 384)    0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 3, 3, 384)    0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 3, 3, 192)    576         conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 3, 3, 320)    0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_172[0][0]             \n",
            "                                                                 activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 3, 3, 768)    0           activation_176[0][0]             \n",
            "                                                                 activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 3, 3, 192)    0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_170[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_2[0][0]              \n",
            "                                                                 activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 3, 3, 448)    1344        conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 3, 3, 448)    0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 3, 3, 384)    1548288     activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 3, 3, 384)    1152        conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 3, 3, 384)    1152        conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 3, 3, 384)    0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 3, 3, 384)    0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 3, 3, 384)    442368      activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 3, 3, 384)    442368      activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 3, 3, 384)    442368      activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 3, 3, 384)    442368      activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_17 (AveragePo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 3, 3, 384)    1152        conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 3, 3, 384)    1152        conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 3, 3, 384)    1152        conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 3, 3, 384)    1152        conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 3, 3, 192)    393216      average_pooling2d_17[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 3, 3, 320)    960         conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 3, 3, 384)    0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 3, 3, 384)    0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 3, 3, 384)    0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 3, 3, 384)    0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 3, 3, 192)    576         conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 3, 3, 320)    0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_181[0][0]             \n",
            "                                                                 activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 3, 3, 768)    0           activation_185[0][0]             \n",
            "                                                                 activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 3, 3, 192)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_179[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_3[0][0]              \n",
            "                                                                 activation_187[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 21,768,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n",
            "188\n",
            "190\n",
            "Epoch 1/250\n",
            "23/23 [==============================] - 18s 533ms/step - loss: 1.7852 - accuracy: 0.2264 - val_loss: 1.4674 - val_accuracy: 0.4387\n",
            "Epoch 2/250\n",
            "23/23 [==============================] - 11s 491ms/step - loss: 1.4067 - accuracy: 0.4116 - val_loss: 1.0085 - val_accuracy: 0.6335\n",
            "Epoch 3/250\n",
            "23/23 [==============================] - 11s 494ms/step - loss: 1.1898 - accuracy: 0.5523 - val_loss: 0.8416 - val_accuracy: 0.7098\n",
            "Epoch 4/250\n",
            "23/23 [==============================] - 11s 493ms/step - loss: 1.0180 - accuracy: 0.6448 - val_loss: 0.7383 - val_accuracy: 0.7384\n",
            "Epoch 5/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.8774 - accuracy: 0.7176 - val_loss: 0.6750 - val_accuracy: 0.7629\n",
            "Epoch 6/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.7897 - accuracy: 0.7327 - val_loss: 0.6224 - val_accuracy: 0.7793\n",
            "Epoch 7/250\n",
            "23/23 [==============================] - 11s 485ms/step - loss: 0.7279 - accuracy: 0.7534 - val_loss: 0.5811 - val_accuracy: 0.8025\n",
            "Epoch 8/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.6388 - accuracy: 0.7805 - val_loss: 0.5402 - val_accuracy: 0.8093\n",
            "Epoch 9/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.5894 - accuracy: 0.8095 - val_loss: 0.5166 - val_accuracy: 0.8161\n",
            "Epoch 10/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.5444 - accuracy: 0.8236 - val_loss: 0.4935 - val_accuracy: 0.8256\n",
            "Epoch 11/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.5028 - accuracy: 0.8344 - val_loss: 0.4771 - val_accuracy: 0.8202\n",
            "Epoch 12/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.4645 - accuracy: 0.8482 - val_loss: 0.4611 - val_accuracy: 0.8351\n",
            "Epoch 13/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.4396 - accuracy: 0.8578 - val_loss: 0.4501 - val_accuracy: 0.8420\n",
            "Epoch 14/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.3988 - accuracy: 0.8741 - val_loss: 0.4349 - val_accuracy: 0.8488\n",
            "Epoch 15/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.3722 - accuracy: 0.8915 - val_loss: 0.4257 - val_accuracy: 0.8556\n",
            "Epoch 16/250\n",
            "23/23 [==============================] - 11s 491ms/step - loss: 0.3649 - accuracy: 0.8775 - val_loss: 0.4106 - val_accuracy: 0.8638\n",
            "Epoch 17/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.3390 - accuracy: 0.8927 - val_loss: 0.4002 - val_accuracy: 0.8733\n",
            "Epoch 18/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.3182 - accuracy: 0.8983 - val_loss: 0.3911 - val_accuracy: 0.8706\n",
            "Epoch 19/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.2952 - accuracy: 0.9114 - val_loss: 0.3813 - val_accuracy: 0.8747\n",
            "Epoch 20/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.2795 - accuracy: 0.9126 - val_loss: 0.3774 - val_accuracy: 0.8719\n",
            "Epoch 21/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.2619 - accuracy: 0.9151 - val_loss: 0.3702 - val_accuracy: 0.8733\n",
            "Epoch 22/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.2585 - accuracy: 0.9163 - val_loss: 0.3653 - val_accuracy: 0.8747\n",
            "Epoch 23/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.2407 - accuracy: 0.9220 - val_loss: 0.3592 - val_accuracy: 0.8801\n",
            "Epoch 24/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.2303 - accuracy: 0.9289 - val_loss: 0.3522 - val_accuracy: 0.8815\n",
            "Epoch 25/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.1996 - accuracy: 0.9414 - val_loss: 0.3483 - val_accuracy: 0.8828\n",
            "Epoch 26/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.2068 - accuracy: 0.9415 - val_loss: 0.3475 - val_accuracy: 0.8842\n",
            "Epoch 27/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.1690 - accuracy: 0.9526 - val_loss: 0.3433 - val_accuracy: 0.8896\n",
            "Epoch 28/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.1748 - accuracy: 0.9474 - val_loss: 0.3378 - val_accuracy: 0.8965\n",
            "Epoch 29/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.1632 - accuracy: 0.9516 - val_loss: 0.3326 - val_accuracy: 0.8978\n",
            "Epoch 30/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.1592 - accuracy: 0.9506 - val_loss: 0.3330 - val_accuracy: 0.8951\n",
            "Epoch 31/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.1446 - accuracy: 0.9585 - val_loss: 0.3318 - val_accuracy: 0.8896\n",
            "Epoch 32/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.1336 - accuracy: 0.9631 - val_loss: 0.3301 - val_accuracy: 0.8910\n",
            "Epoch 33/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.1298 - accuracy: 0.9636 - val_loss: 0.3260 - val_accuracy: 0.8937\n",
            "Epoch 34/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.1146 - accuracy: 0.9734 - val_loss: 0.3236 - val_accuracy: 0.8951\n",
            "Epoch 35/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.1145 - accuracy: 0.9694 - val_loss: 0.3214 - val_accuracy: 0.8978\n",
            "Epoch 36/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.1015 - accuracy: 0.9720 - val_loss: 0.3232 - val_accuracy: 0.8978\n",
            "Epoch 37/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.1015 - accuracy: 0.9754 - val_loss: 0.3244 - val_accuracy: 0.8965\n",
            "Epoch 38/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0924 - accuracy: 0.9773 - val_loss: 0.3276 - val_accuracy: 0.8965\n",
            "Epoch 39/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0791 - accuracy: 0.9840 - val_loss: 0.3293 - val_accuracy: 0.9005\n",
            "Epoch 40/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0796 - accuracy: 0.9820 - val_loss: 0.3282 - val_accuracy: 0.8992\n",
            "Epoch 41/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0756 - accuracy: 0.9816 - val_loss: 0.3280 - val_accuracy: 0.8978\n",
            "Epoch 42/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0691 - accuracy: 0.9859 - val_loss: 0.3288 - val_accuracy: 0.9005\n",
            "Epoch 43/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0674 - accuracy: 0.9832 - val_loss: 0.3295 - val_accuracy: 0.9005\n",
            "Epoch 44/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0680 - accuracy: 0.9812 - val_loss: 0.3264 - val_accuracy: 0.9019\n",
            "Epoch 45/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0555 - accuracy: 0.9889 - val_loss: 0.3250 - val_accuracy: 0.9005\n",
            "Epoch 46/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0666 - accuracy: 0.9839 - val_loss: 0.3243 - val_accuracy: 0.8978\n",
            "Epoch 47/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0614 - accuracy: 0.9892 - val_loss: 0.3275 - val_accuracy: 0.8992\n",
            "Epoch 48/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0550 - accuracy: 0.9878 - val_loss: 0.3277 - val_accuracy: 0.8992\n",
            "Epoch 49/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0477 - accuracy: 0.9883 - val_loss: 0.3259 - val_accuracy: 0.9005\n",
            "Epoch 50/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0466 - accuracy: 0.9897 - val_loss: 0.3252 - val_accuracy: 0.8992\n",
            "Epoch 51/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0428 - accuracy: 0.9930 - val_loss: 0.3290 - val_accuracy: 0.8992\n",
            "Epoch 52/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0343 - accuracy: 0.9941 - val_loss: 0.3297 - val_accuracy: 0.8978\n",
            "Epoch 53/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0371 - accuracy: 0.9937 - val_loss: 0.3320 - val_accuracy: 0.8992\n",
            "Epoch 54/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0331 - accuracy: 0.9963 - val_loss: 0.3326 - val_accuracy: 0.9019\n",
            "Epoch 55/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0280 - accuracy: 0.9963 - val_loss: 0.3338 - val_accuracy: 0.9005\n",
            "Epoch 56/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0340 - accuracy: 0.9936 - val_loss: 0.3330 - val_accuracy: 0.8992\n",
            "Epoch 57/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0328 - accuracy: 0.9946 - val_loss: 0.3323 - val_accuracy: 0.9005\n",
            "Epoch 58/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0303 - accuracy: 0.9949 - val_loss: 0.3342 - val_accuracy: 0.9019\n",
            "Epoch 59/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0266 - accuracy: 0.9976 - val_loss: 0.3366 - val_accuracy: 0.8992\n",
            "Epoch 60/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0262 - accuracy: 0.9971 - val_loss: 0.3374 - val_accuracy: 0.8978\n",
            "Epoch 61/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0234 - accuracy: 0.9969 - val_loss: 0.3370 - val_accuracy: 0.8978\n",
            "Epoch 62/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0215 - accuracy: 0.9987 - val_loss: 0.3345 - val_accuracy: 0.8965\n",
            "Epoch 63/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0235 - accuracy: 0.9960 - val_loss: 0.3367 - val_accuracy: 0.8978\n",
            "Epoch 64/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0216 - accuracy: 0.9952 - val_loss: 0.3401 - val_accuracy: 0.8992\n",
            "Epoch 65/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0189 - accuracy: 0.9969 - val_loss: 0.3421 - val_accuracy: 0.8992\n",
            "Epoch 66/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0257 - accuracy: 0.9953 - val_loss: 0.3429 - val_accuracy: 0.9005\n",
            "Epoch 67/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0201 - accuracy: 0.9969 - val_loss: 0.3405 - val_accuracy: 0.8965\n",
            "Epoch 68/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0149 - accuracy: 0.9993 - val_loss: 0.3441 - val_accuracy: 0.8965\n",
            "Epoch 69/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0146 - accuracy: 0.9984 - val_loss: 0.3472 - val_accuracy: 0.8978\n",
            "Epoch 70/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0147 - accuracy: 0.9993 - val_loss: 0.3454 - val_accuracy: 0.8978\n",
            "Epoch 71/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0162 - accuracy: 0.9986 - val_loss: 0.3482 - val_accuracy: 0.8951\n",
            "Epoch 72/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0119 - accuracy: 0.9996 - val_loss: 0.3518 - val_accuracy: 0.8978\n",
            "Epoch 73/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0116 - accuracy: 0.9979 - val_loss: 0.3517 - val_accuracy: 0.9005\n",
            "Epoch 74/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0138 - accuracy: 0.9989 - val_loss: 0.3534 - val_accuracy: 0.9005\n",
            "Epoch 75/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0116 - accuracy: 0.9987 - val_loss: 0.3543 - val_accuracy: 0.8992\n",
            "Epoch 76/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0127 - accuracy: 0.9979 - val_loss: 0.3546 - val_accuracy: 0.8965\n",
            "Epoch 77/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.3569 - val_accuracy: 0.8978\n",
            "Epoch 78/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0110 - accuracy: 0.9998 - val_loss: 0.3589 - val_accuracy: 0.8992\n",
            "Epoch 79/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.3619 - val_accuracy: 0.8992\n",
            "Epoch 80/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0099 - accuracy: 0.9981 - val_loss: 0.3634 - val_accuracy: 0.9019\n",
            "Epoch 81/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0098 - accuracy: 0.9983 - val_loss: 0.3632 - val_accuracy: 0.9019\n",
            "Epoch 82/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0105 - accuracy: 0.9976 - val_loss: 0.3609 - val_accuracy: 0.9033\n",
            "Epoch 83/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0101 - accuracy: 0.9987 - val_loss: 0.3617 - val_accuracy: 0.9005\n",
            "Epoch 84/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0089 - accuracy: 0.9993 - val_loss: 0.3666 - val_accuracy: 0.9019\n",
            "Epoch 85/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0078 - accuracy: 0.9997 - val_loss: 0.3693 - val_accuracy: 0.9019\n",
            "Epoch 86/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0083 - accuracy: 0.9995 - val_loss: 0.3697 - val_accuracy: 0.8992\n",
            "Epoch 87/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0081 - accuracy: 0.9993 - val_loss: 0.3682 - val_accuracy: 0.9033\n",
            "Epoch 88/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 0.3694 - val_accuracy: 0.9033\n",
            "Epoch 89/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.3711 - val_accuracy: 0.9019\n",
            "Epoch 90/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0074 - accuracy: 0.9997 - val_loss: 0.3698 - val_accuracy: 0.9005\n",
            "Epoch 91/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0066 - accuracy: 0.9995 - val_loss: 0.3720 - val_accuracy: 0.8992\n",
            "Epoch 92/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0064 - accuracy: 0.9990 - val_loss: 0.3749 - val_accuracy: 0.8992\n",
            "Epoch 93/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.3803 - val_accuracy: 0.8978\n",
            "Epoch 94/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0059 - accuracy: 0.9992 - val_loss: 0.3807 - val_accuracy: 0.8992\n",
            "Epoch 95/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 0.3818 - val_accuracy: 0.9005\n",
            "Epoch 96/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 0.3778 - val_accuracy: 0.9033\n",
            "Epoch 97/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.3775 - val_accuracy: 0.9046\n",
            "Epoch 98/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0048 - accuracy: 0.9999 - val_loss: 0.3780 - val_accuracy: 0.9060\n",
            "Epoch 99/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0054 - accuracy: 0.9994 - val_loss: 0.3805 - val_accuracy: 0.9046\n",
            "Epoch 100/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0064 - accuracy: 0.9996 - val_loss: 0.3851 - val_accuracy: 0.9046\n",
            "Epoch 101/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0047 - accuracy: 0.9997 - val_loss: 0.3887 - val_accuracy: 0.9046\n",
            "Epoch 102/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.3877 - val_accuracy: 0.9033\n",
            "Epoch 103/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.3859 - val_accuracy: 0.9046\n",
            "Epoch 104/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.3852 - val_accuracy: 0.9046\n",
            "Epoch 105/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 0.3839 - val_accuracy: 0.9060\n",
            "Epoch 106/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.3848 - val_accuracy: 0.9060\n",
            "Epoch 107/250\n",
            "23/23 [==============================] - 11s 491ms/step - loss: 0.0050 - accuracy: 0.9994 - val_loss: 0.3871 - val_accuracy: 0.9033\n",
            "Epoch 108/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.3908 - val_accuracy: 0.9019\n",
            "Epoch 109/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.3920 - val_accuracy: 0.9019\n",
            "Epoch 110/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0049 - accuracy: 0.9995 - val_loss: 0.3892 - val_accuracy: 0.9046\n",
            "Epoch 111/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3880 - val_accuracy: 0.9046\n",
            "Epoch 112/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3882 - val_accuracy: 0.9046\n",
            "Epoch 113/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 0.3888 - val_accuracy: 0.9046\n",
            "Epoch 114/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3885 - val_accuracy: 0.9046\n",
            "Epoch 115/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3926 - val_accuracy: 0.9060\n",
            "Epoch 116/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0034 - accuracy: 0.9998 - val_loss: 0.3963 - val_accuracy: 0.9060\n",
            "Epoch 117/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3967 - val_accuracy: 0.9074\n",
            "Epoch 118/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.3977 - val_accuracy: 0.9060\n",
            "Epoch 119/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0043 - accuracy: 0.9999 - val_loss: 0.3979 - val_accuracy: 0.9060\n",
            "Epoch 120/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3971 - val_accuracy: 0.9087\n",
            "Epoch 121/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.3954 - val_accuracy: 0.9101\n",
            "Epoch 122/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3969 - val_accuracy: 0.9074\n",
            "Epoch 123/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0041 - accuracy: 0.9997 - val_loss: 0.3958 - val_accuracy: 0.9074\n",
            "Epoch 124/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3993 - val_accuracy: 0.9074\n",
            "Epoch 125/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.4003 - val_accuracy: 0.9046\n",
            "Epoch 126/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.4004 - val_accuracy: 0.9046\n",
            "Epoch 127/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4014 - val_accuracy: 0.9019\n",
            "Epoch 128/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.4050 - val_accuracy: 0.9019\n",
            "Epoch 129/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4093 - val_accuracy: 0.9019\n",
            "Epoch 130/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4087 - val_accuracy: 0.9005\n",
            "Epoch 131/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4114 - val_accuracy: 0.9005\n",
            "Epoch 132/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4114 - val_accuracy: 0.9019\n",
            "Epoch 133/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4129 - val_accuracy: 0.9033\n",
            "Epoch 134/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4136 - val_accuracy: 0.9005\n",
            "Epoch 135/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4158 - val_accuracy: 0.9019\n",
            "Epoch 136/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4151 - val_accuracy: 0.9033\n",
            "Epoch 137/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4140 - val_accuracy: 0.9019\n",
            "Epoch 138/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4145 - val_accuracy: 0.9019\n",
            "Epoch 139/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4120 - val_accuracy: 0.9033\n",
            "Epoch 140/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.4136 - val_accuracy: 0.9046\n",
            "Epoch 141/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4124 - val_accuracy: 0.9046\n",
            "Epoch 142/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.4143 - val_accuracy: 0.9033\n",
            "Epoch 143/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4116 - val_accuracy: 0.9060\n",
            "Epoch 144/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.4117 - val_accuracy: 0.9060\n",
            "Epoch 145/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4139 - val_accuracy: 0.9060\n",
            "Epoch 146/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4144 - val_accuracy: 0.9060\n",
            "Epoch 147/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.4131 - val_accuracy: 0.9060\n",
            "Epoch 148/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.4108 - val_accuracy: 0.9033\n",
            "Epoch 149/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4115 - val_accuracy: 0.9019\n",
            "Epoch 150/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4139 - val_accuracy: 0.9005\n",
            "Epoch 151/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4159 - val_accuracy: 0.9005\n",
            "Epoch 152/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.4187 - val_accuracy: 0.9074\n",
            "Epoch 153/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4187 - val_accuracy: 0.9060\n",
            "Epoch 154/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4162 - val_accuracy: 0.9060\n",
            "Epoch 155/250\n",
            "23/23 [==============================] - 11s 484ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4172 - val_accuracy: 0.9046\n",
            "Epoch 156/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4205 - val_accuracy: 0.9033\n",
            "Epoch 157/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4244 - val_accuracy: 0.9019\n",
            "Epoch 158/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 9.2169e-04 - accuracy: 1.0000 - val_loss: 0.4242 - val_accuracy: 0.9046\n",
            "Epoch 159/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.4258 - val_accuracy: 0.9033\n",
            "Epoch 160/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 9.1770e-04 - accuracy: 1.0000 - val_loss: 0.4262 - val_accuracy: 0.9033\n",
            "Epoch 161/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4289 - val_accuracy: 0.9046\n",
            "Epoch 162/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4301 - val_accuracy: 0.9060\n",
            "Epoch 163/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 9.7043e-04 - accuracy: 1.0000 - val_loss: 0.4318 - val_accuracy: 0.9033\n",
            "Epoch 164/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 9.3427e-04 - accuracy: 1.0000 - val_loss: 0.4303 - val_accuracy: 0.9033\n",
            "Epoch 165/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 9.5948e-04 - accuracy: 1.0000 - val_loss: 0.4309 - val_accuracy: 0.9046\n",
            "Epoch 166/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 9.7518e-04 - accuracy: 1.0000 - val_loss: 0.4316 - val_accuracy: 0.9046\n",
            "Epoch 167/250\n",
            "23/23 [==============================] - 11s 486ms/step - loss: 8.8491e-04 - accuracy: 1.0000 - val_loss: 0.4336 - val_accuracy: 0.9046\n",
            "Epoch 168/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 9.8078e-04 - accuracy: 1.0000 - val_loss: 0.4321 - val_accuracy: 0.9046\n",
            "Epoch 169/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 8.4583e-04 - accuracy: 1.0000 - val_loss: 0.4336 - val_accuracy: 0.9019\n",
            "Epoch 170/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 9.6405e-04 - accuracy: 1.0000 - val_loss: 0.4313 - val_accuracy: 0.9046\n",
            "Epoch 171/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4305 - val_accuracy: 0.9046\n",
            "Epoch 172/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4290 - val_accuracy: 0.9046\n",
            "Epoch 173/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 9.2556e-04 - accuracy: 1.0000 - val_loss: 0.4310 - val_accuracy: 0.9046\n",
            "Epoch 174/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 8.4665e-04 - accuracy: 1.0000 - val_loss: 0.4313 - val_accuracy: 0.9060\n",
            "Epoch 175/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4393 - val_accuracy: 0.9033\n",
            "Epoch 176/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 9.1648e-04 - accuracy: 1.0000 - val_loss: 0.4431 - val_accuracy: 0.9019\n",
            "Epoch 177/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4432 - val_accuracy: 0.9019\n",
            "Epoch 178/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 8.0509e-04 - accuracy: 1.0000 - val_loss: 0.4405 - val_accuracy: 0.9033\n",
            "Epoch 179/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 9.9651e-04 - accuracy: 1.0000 - val_loss: 0.4420 - val_accuracy: 0.9033\n",
            "Epoch 180/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 8.1209e-04 - accuracy: 1.0000 - val_loss: 0.4439 - val_accuracy: 0.9033\n",
            "Epoch 181/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 6.8340e-04 - accuracy: 1.0000 - val_loss: 0.4435 - val_accuracy: 0.9046\n",
            "Epoch 182/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 6.8961e-04 - accuracy: 1.0000 - val_loss: 0.4443 - val_accuracy: 0.9046\n",
            "Epoch 183/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 6.4862e-04 - accuracy: 1.0000 - val_loss: 0.4434 - val_accuracy: 0.9060\n",
            "Epoch 184/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 7.8411e-04 - accuracy: 1.0000 - val_loss: 0.4412 - val_accuracy: 0.9033\n",
            "Epoch 185/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 7.4081e-04 - accuracy: 1.0000 - val_loss: 0.4416 - val_accuracy: 0.9033\n",
            "Epoch 186/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 8.0854e-04 - accuracy: 1.0000 - val_loss: 0.4457 - val_accuracy: 0.9019\n",
            "Epoch 187/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 8.5093e-04 - accuracy: 1.0000 - val_loss: 0.4469 - val_accuracy: 0.9019\n",
            "Epoch 188/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4565 - val_accuracy: 0.9019\n",
            "Epoch 189/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 6.5823e-04 - accuracy: 1.0000 - val_loss: 0.4593 - val_accuracy: 0.9005\n",
            "Epoch 190/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4558 - val_accuracy: 0.9019\n",
            "Epoch 191/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 6.9443e-04 - accuracy: 1.0000 - val_loss: 0.4532 - val_accuracy: 0.9019\n",
            "Epoch 192/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 6.7058e-04 - accuracy: 1.0000 - val_loss: 0.4494 - val_accuracy: 0.9019\n",
            "Epoch 193/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.4424 - val_accuracy: 0.9046\n",
            "Epoch 194/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4406 - val_accuracy: 0.9060\n",
            "Epoch 195/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0012 - accuracy: 0.9993 - val_loss: 0.4474 - val_accuracy: 0.9033\n",
            "Epoch 196/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 5.6597e-04 - accuracy: 1.0000 - val_loss: 0.4467 - val_accuracy: 0.9033\n",
            "Epoch 197/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 7.0851e-04 - accuracy: 1.0000 - val_loss: 0.4436 - val_accuracy: 0.9033\n",
            "Epoch 198/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 5.7747e-04 - accuracy: 1.0000 - val_loss: 0.4451 - val_accuracy: 0.9074\n",
            "Epoch 199/250\n",
            "23/23 [==============================] - 11s 491ms/step - loss: 7.6179e-04 - accuracy: 1.0000 - val_loss: 0.4506 - val_accuracy: 0.9087\n",
            "Epoch 200/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 6.6304e-04 - accuracy: 1.0000 - val_loss: 0.4502 - val_accuracy: 0.9087\n",
            "Epoch 201/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 0.0020 - accuracy: 0.9988 - val_loss: 0.4503 - val_accuracy: 0.9046\n",
            "Epoch 202/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 5.4140e-04 - accuracy: 1.0000 - val_loss: 0.4485 - val_accuracy: 0.9046\n",
            "Epoch 203/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 6.0340e-04 - accuracy: 1.0000 - val_loss: 0.4487 - val_accuracy: 0.9046\n",
            "Epoch 204/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 7.5583e-04 - accuracy: 1.0000 - val_loss: 0.4443 - val_accuracy: 0.9101\n",
            "Epoch 205/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 5.1949e-04 - accuracy: 1.0000 - val_loss: 0.4444 - val_accuracy: 0.9087\n",
            "Epoch 206/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 4.2566e-04 - accuracy: 1.0000 - val_loss: 0.4444 - val_accuracy: 0.9087\n",
            "Epoch 207/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 5.5363e-04 - accuracy: 1.0000 - val_loss: 0.4459 - val_accuracy: 0.9087\n",
            "Epoch 208/250\n",
            "23/23 [==============================] - 11s 491ms/step - loss: 4.5709e-04 - accuracy: 1.0000 - val_loss: 0.4463 - val_accuracy: 0.9101\n",
            "Epoch 209/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 5.1457e-04 - accuracy: 1.0000 - val_loss: 0.4465 - val_accuracy: 0.9087\n",
            "Epoch 210/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 7.8409e-04 - accuracy: 0.9998 - val_loss: 0.4453 - val_accuracy: 0.9033\n",
            "Epoch 211/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 4.4367e-04 - accuracy: 1.0000 - val_loss: 0.4458 - val_accuracy: 0.9046\n",
            "Epoch 212/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 4.9675e-04 - accuracy: 1.0000 - val_loss: 0.4470 - val_accuracy: 0.9033\n",
            "Epoch 213/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 6.5050e-04 - accuracy: 1.0000 - val_loss: 0.4431 - val_accuracy: 0.9074\n",
            "Epoch 214/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 4.3694e-04 - accuracy: 1.0000 - val_loss: 0.4450 - val_accuracy: 0.9046\n",
            "Epoch 215/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 4.7678e-04 - accuracy: 1.0000 - val_loss: 0.4458 - val_accuracy: 0.9060\n",
            "Epoch 216/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 4.0716e-04 - accuracy: 1.0000 - val_loss: 0.4470 - val_accuracy: 0.9074\n",
            "Epoch 217/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 4.8838e-04 - accuracy: 1.0000 - val_loss: 0.4503 - val_accuracy: 0.9087\n",
            "Epoch 218/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 4.2855e-04 - accuracy: 1.0000 - val_loss: 0.4526 - val_accuracy: 0.9074\n",
            "Epoch 219/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 3.4281e-04 - accuracy: 1.0000 - val_loss: 0.4543 - val_accuracy: 0.9074\n",
            "Epoch 220/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 3.1089e-04 - accuracy: 1.0000 - val_loss: 0.4559 - val_accuracy: 0.9074\n",
            "Epoch 221/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 3.5127e-04 - accuracy: 1.0000 - val_loss: 0.4568 - val_accuracy: 0.9060\n",
            "Epoch 222/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 3.8546e-04 - accuracy: 1.0000 - val_loss: 0.4549 - val_accuracy: 0.9060\n",
            "Epoch 223/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 2.9069e-04 - accuracy: 1.0000 - val_loss: 0.4536 - val_accuracy: 0.9060\n",
            "Epoch 224/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 3.7016e-04 - accuracy: 1.0000 - val_loss: 0.4537 - val_accuracy: 0.9060\n",
            "Epoch 225/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 4.6127e-04 - accuracy: 1.0000 - val_loss: 0.4540 - val_accuracy: 0.9046\n",
            "Epoch 226/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 3.4891e-04 - accuracy: 1.0000 - val_loss: 0.4545 - val_accuracy: 0.9060\n",
            "Epoch 227/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 6.0383e-04 - accuracy: 0.9997 - val_loss: 0.4561 - val_accuracy: 0.9074\n",
            "Epoch 228/250\n",
            "23/23 [==============================] - 11s 491ms/step - loss: 3.8454e-04 - accuracy: 1.0000 - val_loss: 0.4592 - val_accuracy: 0.9087\n",
            "Epoch 229/250\n",
            "23/23 [==============================] - 11s 492ms/step - loss: 3.8913e-04 - accuracy: 0.9999 - val_loss: 0.4633 - val_accuracy: 0.9074\n",
            "Epoch 230/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 3.5412e-04 - accuracy: 1.0000 - val_loss: 0.4663 - val_accuracy: 0.9060\n",
            "Epoch 231/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 8.3298e-04 - accuracy: 1.0000 - val_loss: 0.4642 - val_accuracy: 0.9074\n",
            "Epoch 232/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 3.2606e-04 - accuracy: 1.0000 - val_loss: 0.4616 - val_accuracy: 0.9087\n",
            "Epoch 233/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 3.1145e-04 - accuracy: 1.0000 - val_loss: 0.4588 - val_accuracy: 0.9074\n",
            "Epoch 234/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 5.2888e-04 - accuracy: 0.9999 - val_loss: 0.4658 - val_accuracy: 0.9019\n",
            "Epoch 235/250\n",
            "23/23 [==============================] - 11s 492ms/step - loss: 3.5829e-04 - accuracy: 1.0000 - val_loss: 0.4772 - val_accuracy: 0.9019\n",
            "Epoch 236/250\n",
            "23/23 [==============================] - 11s 491ms/step - loss: 2.8549e-04 - accuracy: 1.0000 - val_loss: 0.4731 - val_accuracy: 0.9033\n",
            "Epoch 237/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 3.1996e-04 - accuracy: 1.0000 - val_loss: 0.4672 - val_accuracy: 0.9033\n",
            "Epoch 238/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 7.0798e-04 - accuracy: 1.0000 - val_loss: 0.4615 - val_accuracy: 0.8992\n",
            "Epoch 239/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 3.7095e-04 - accuracy: 1.0000 - val_loss: 0.4608 - val_accuracy: 0.9005\n",
            "Epoch 240/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 3.1434e-04 - accuracy: 1.0000 - val_loss: 0.4615 - val_accuracy: 0.9005\n",
            "Epoch 241/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 3.6638e-04 - accuracy: 1.0000 - val_loss: 0.4631 - val_accuracy: 0.9019\n",
            "Epoch 242/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 4.4089e-04 - accuracy: 1.0000 - val_loss: 0.4638 - val_accuracy: 0.9019\n",
            "Epoch 243/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 2.4924e-04 - accuracy: 1.0000 - val_loss: 0.4652 - val_accuracy: 0.9019\n",
            "Epoch 244/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 2.7561e-04 - accuracy: 1.0000 - val_loss: 0.4651 - val_accuracy: 0.9033\n",
            "Epoch 245/250\n",
            "23/23 [==============================] - 11s 490ms/step - loss: 5.6215e-04 - accuracy: 1.0000 - val_loss: 0.4578 - val_accuracy: 0.9046\n",
            "Epoch 246/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 4.4602e-04 - accuracy: 1.0000 - val_loss: 0.4584 - val_accuracy: 0.9060\n",
            "Epoch 247/250\n",
            "23/23 [==============================] - 11s 488ms/step - loss: 2.3896e-04 - accuracy: 1.0000 - val_loss: 0.4609 - val_accuracy: 0.9074\n",
            "Epoch 248/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 5.7462e-04 - accuracy: 1.0000 - val_loss: 0.4586 - val_accuracy: 0.9060\n",
            "Epoch 249/250\n",
            "23/23 [==============================] - 11s 489ms/step - loss: 4.1574e-04 - accuracy: 1.0000 - val_loss: 0.4560 - val_accuracy: 0.9060\n",
            "Epoch 250/250\n",
            "23/23 [==============================] - 11s 487ms/step - loss: 2.6923e-04 - accuracy: 1.0000 - val_loss: 0.4558 - val_accuracy: 0.9060\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn6ReHHFsFaM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mwmFuwFsFaM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}